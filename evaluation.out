nohup: ignoring input
2024-12-28 14:03:15,153 - INFO - Logging setup completed successfully
2024-12-28 14:03:15,153 - INFO - Log file created at: /home/brian/Notebooks/ddv2-ws/logs/experiment_20241228_140314.log
2024-12-28 14:03:15,154 - INFO - Starting experiment run
2024-12-28 14:03:15,154 - INFO - Test mode: False
2024-12-28 14:03:15,154 - INFO - 
Processing dataset: GTSRB
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:03:15,739 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:03:15,740 - INFO - Dataset type: image
2024-12-28 14:03:15,740 - INFO - Sample size: 39209
2024-12-28 14:03:15,741 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:03:15,741 - INFO - 
Progress: 1.0% - Evaluating GTSRB with SVM (standard mode, iteration 1/1)
2024-12-28 14:03:15,993 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:03:15,994 - INFO - Dataset type: image
2024-12-28 14:03:15,994 - INFO - Sample size: 39209
2024-12-28 14:03:15,994 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:03:15,994 - INFO - Loading datasets...
2024-12-28 14:03:35,421 - INFO - Dataset loading completed in 19.43s
2024-12-28 14:03:35,422 - INFO - Extracting validation features...
2024-12-28 14:03:35,422 - INFO - Extracting features from 4435 samples...

Archiving previous files...
Archiving results from: /home/brian/Notebooks/ddv2-ws/results
Results archived
Archiving logs from: /home/brian/Notebooks/ddv2-ws/logs
Logs archived
Cleanup completed
Archiving finished - starting evaluation

Logging to: /home/brian/Notebooks/ddv2-ws/logs/experiment_20241228_140314.log
Initializing configuration manager...
Getting configurations...

Evaluation Plan:
- Datasets: ['GTSRB', 'GTSRB', 'GTSRB', 'GTSRB', 'ImageNette', 'ImageNette', 'ImageNette', 'ImageNette', 'CIFAR100', 'CIFAR100', 'CIFAR100', 'CIFAR100']
- Classifiers: ['SVM', 'LogisticRegression', 'RandomForest', 'KNeighbors']
- Sample sizes: Full
- Modes: ['standard', 'dynadetect']
- Iterations: 1

Starting evaluation...
2024-12-28 14:03:36,541 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:03:36,545 - INFO - Validation feature extraction completed in 1.12s
2024-12-28 14:03:36,545 - INFO - Extracting training features...
2024-12-28 14:03:36,545 - INFO - Extracting features from 19755 samples...
2024-12-28 14:03:39,212 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:03:39,235 - INFO - Training feature extraction completed in 2.69s
2024-12-28 14:03:39,236 - INFO - Creating model for classifier: SVM
2024-12-28 14:03:39,237 - INFO - Using device: cuda
2024-12-28 14:03:39,237 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 14:03:39,237 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:03:39,237 - INFO - Training set processing completed in 0.00s
2024-12-28 14:03:39,238 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:03:39,240 - INFO - Memory usage at start_fit: CPU 1034.4 MB, GPU 87.1 MB
2024-12-28 14:03:39,240 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:03:39,255 - INFO - Number of unique classes: 43
2024-12-28 14:03:39,414 - INFO - Fitted scaler and transformed data
2024-12-28 14:03:39,414 - INFO - Scaling time: 0.16s
2024-12-28 14:03:40,926 - INFO - Epoch 1/500, Train Loss: 6.8475, Val Loss: 3.1650
2024-12-28 14:03:42,085 - INFO - Epoch 2/500, Train Loss: 2.4995, Val Loss: 2.2674
2024-12-28 14:03:43,280 - INFO - Epoch 3/500, Train Loss: 1.8066, Val Loss: 1.9024
2024-12-28 14:03:44,409 - INFO - Epoch 4/500, Train Loss: 1.4608, Val Loss: 1.6632
2024-12-28 14:03:45,526 - INFO - Epoch 5/500, Train Loss: 1.2407, Val Loss: 1.5018
2024-12-28 14:03:46,677 - INFO - Epoch 6/500, Train Loss: 1.0993, Val Loss: 1.3884
2024-12-28 14:03:47,919 - INFO - Epoch 7/500, Train Loss: 0.9864, Val Loss: 1.3124
2024-12-28 14:03:49,118 - INFO - Epoch 8/500, Train Loss: 0.9065, Val Loss: 1.2494
2024-12-28 14:03:50,155 - INFO - Epoch 9/500, Train Loss: 0.8391, Val Loss: 1.2099
2024-12-28 14:03:51,264 - INFO - Epoch 10/500, Train Loss: 0.7826, Val Loss: 1.1137
2024-12-28 14:03:52,422 - INFO - Epoch 11/500, Train Loss: 0.7438, Val Loss: 1.0902
2024-12-28 14:03:53,574 - INFO - Epoch 12/500, Train Loss: 0.7033, Val Loss: 1.0600
2024-12-28 14:03:54,723 - INFO - Epoch 13/500, Train Loss: 0.6693, Val Loss: 1.0562
2024-12-28 14:03:55,847 - INFO - Epoch 14/500, Train Loss: 0.6420, Val Loss: 1.0061
2024-12-28 14:03:57,028 - INFO - Epoch 15/500, Train Loss: 0.6225, Val Loss: 1.0015
2024-12-28 14:03:58,261 - INFO - Epoch 16/500, Train Loss: 0.6014, Val Loss: 0.9813
2024-12-28 14:03:59,453 - INFO - Epoch 17/500, Train Loss: 0.5849, Val Loss: 0.9677
2024-12-28 14:04:00,454 - INFO - Epoch 18/500, Train Loss: 0.5689, Val Loss: 0.9474
2024-12-28 14:04:01,578 - INFO - Epoch 19/500, Train Loss: 0.5548, Val Loss: 0.9418
2024-12-28 14:04:02,776 - INFO - Epoch 20/500, Train Loss: 0.5450, Val Loss: 0.9331
2024-12-28 14:04:03,997 - INFO - Epoch 21/500, Train Loss: 0.5352, Val Loss: 0.9486
2024-12-28 14:04:05,164 - INFO - Epoch 22/500, Train Loss: 0.5252, Val Loss: 0.8976
2024-12-28 14:04:06,240 - INFO - Epoch 23/500, Train Loss: 0.5136, Val Loss: 0.9158
2024-12-28 14:04:07,318 - INFO - Epoch 24/500, Train Loss: 0.5097, Val Loss: 0.9116
2024-12-28 14:04:08,454 - INFO - Epoch 25/500, Train Loss: 0.5047, Val Loss: 0.9044
2024-12-28 14:04:09,463 - INFO - Epoch 26/500, Train Loss: 0.4962, Val Loss: 0.9199
2024-12-28 14:04:10,573 - INFO - Epoch 27/500, Train Loss: 0.4910, Val Loss: 0.8930
2024-12-28 14:04:11,600 - INFO - Epoch 28/500, Train Loss: 0.4912, Val Loss: 0.8829
2024-12-28 14:04:12,648 - INFO - Epoch 29/500, Train Loss: 0.4868, Val Loss: 0.8733
2024-12-28 14:04:13,703 - INFO - Epoch 30/500, Train Loss: 0.4775, Val Loss: 0.8631
2024-12-28 14:04:14,750 - INFO - Epoch 31/500, Train Loss: 0.4739, Val Loss: 0.8972
2024-12-28 14:04:15,787 - INFO - Epoch 32/500, Train Loss: 0.4736, Val Loss: 0.8973
2024-12-28 14:04:16,814 - INFO - Epoch 33/500, Train Loss: 0.4693, Val Loss: 0.8653
2024-12-28 14:04:17,829 - INFO - Epoch 34/500, Train Loss: 0.4662, Val Loss: 0.8783
2024-12-28 14:04:18,871 - INFO - Epoch 35/500, Train Loss: 0.4649, Val Loss: 0.8519
2024-12-28 14:04:19,825 - INFO - Epoch 36/500, Train Loss: 0.4625, Val Loss: 0.8963
2024-12-28 14:04:20,855 - INFO - Epoch 37/500, Train Loss: 0.4604, Val Loss: 0.8940
2024-12-28 14:04:21,920 - INFO - Epoch 38/500, Train Loss: 0.4613, Val Loss: 0.8486
2024-12-28 14:04:22,964 - INFO - Epoch 39/500, Train Loss: 0.4564, Val Loss: 0.8539
2024-12-28 14:04:23,995 - INFO - Epoch 40/500, Train Loss: 0.4535, Val Loss: 0.8793
2024-12-28 14:04:25,054 - INFO - Epoch 41/500, Train Loss: 0.4548, Val Loss: 0.8803
2024-12-28 14:04:26,175 - INFO - Epoch 42/500, Train Loss: 0.4466, Val Loss: 0.8771
2024-12-28 14:04:27,375 - INFO - Epoch 43/500, Train Loss: 0.4467, Val Loss: 0.8487
2024-12-28 14:04:27,376 - INFO - Early stopping triggered at epoch 43
2024-12-28 14:04:27,376 - INFO - Training completed in 48.14s
2024-12-28 14:04:27,376 - INFO - Final memory usage: CPU 1392.5 MB, GPU 103.8 MB
2024-12-28 14:04:27,377 - INFO - Model training completed in 48.14s
2024-12-28 14:04:27,445 - INFO - Prediction completed in 0.07s
2024-12-28 14:04:27,458 - INFO - Poison rate 0.0 completed in 48.22s
2024-12-28 14:04:27,458 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:04:27,463 - INFO - Total number of labels flipped: 197
2024-12-28 14:04:27,463 - INFO - Label flipping completed in 0.01s
2024-12-28 14:04:27,463 - INFO - Training set processing completed in 0.00s
2024-12-28 14:04:27,463 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:04:27,464 - INFO - Memory usage at start_fit: CPU 1365.9 MB, GPU 103.7 MB
2024-12-28 14:04:27,464 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:04:27,469 - INFO - Number of unique classes: 43
2024-12-28 14:04:27,631 - INFO - Fitted scaler and transformed data
2024-12-28 14:04:27,631 - INFO - Scaling time: 0.16s
2024-12-28 14:04:28,748 - INFO - Epoch 1/500, Train Loss: 7.6233, Val Loss: 3.6917
2024-12-28 14:04:29,785 - INFO - Epoch 2/500, Train Loss: 3.0414, Val Loss: 2.8539
2024-12-28 14:04:30,828 - INFO - Epoch 3/500, Train Loss: 2.2595, Val Loss: 2.4766
2024-12-28 14:04:31,890 - INFO - Epoch 4/500, Train Loss: 1.8539, Val Loss: 2.2794
2024-12-28 14:04:32,902 - INFO - Epoch 5/500, Train Loss: 1.5789, Val Loss: 2.1768
2024-12-28 14:04:33,870 - INFO - Epoch 6/500, Train Loss: 1.3984, Val Loss: 2.0575
2024-12-28 14:04:34,868 - INFO - Epoch 7/500, Train Loss: 1.2677, Val Loss: 1.9309
2024-12-28 14:04:35,934 - INFO - Epoch 8/500, Train Loss: 1.1588, Val Loss: 1.8752
2024-12-28 14:04:37,122 - INFO - Epoch 9/500, Train Loss: 1.0734, Val Loss: 1.7908
2024-12-28 14:04:38,301 - INFO - Epoch 10/500, Train Loss: 1.0049, Val Loss: 1.7425
2024-12-28 14:04:39,468 - INFO - Epoch 11/500, Train Loss: 0.9491, Val Loss: 1.7458
2024-12-28 14:04:40,646 - INFO - Epoch 12/500, Train Loss: 0.8998, Val Loss: 1.7030
2024-12-28 14:04:41,756 - INFO - Epoch 13/500, Train Loss: 0.8617, Val Loss: 1.6792
2024-12-28 14:04:42,856 - INFO - Epoch 14/500, Train Loss: 0.8240, Val Loss: 1.6400
2024-12-28 14:04:43,801 - INFO - Epoch 15/500, Train Loss: 0.7910, Val Loss: 1.5853
2024-12-28 14:04:44,834 - INFO - Epoch 16/500, Train Loss: 0.7651, Val Loss: 1.5835
2024-12-28 14:04:45,853 - INFO - Epoch 17/500, Train Loss: 0.7481, Val Loss: 1.5655
2024-12-28 14:04:46,920 - INFO - Epoch 18/500, Train Loss: 0.7265, Val Loss: 1.5332
2024-12-28 14:04:47,926 - INFO - Epoch 19/500, Train Loss: 0.7136, Val Loss: 1.5620
2024-12-28 14:04:49,067 - INFO - Epoch 20/500, Train Loss: 0.6906, Val Loss: 1.5409
2024-12-28 14:04:50,265 - INFO - Epoch 21/500, Train Loss: 0.6755, Val Loss: 1.5479
2024-12-28 14:04:51,398 - INFO - Epoch 22/500, Train Loss: 0.6667, Val Loss: 1.5296
2024-12-28 14:04:52,513 - INFO - Epoch 23/500, Train Loss: 0.6592, Val Loss: 1.5054
2024-12-28 14:04:53,676 - INFO - Epoch 24/500, Train Loss: 0.6417, Val Loss: 1.5024
2024-12-28 14:04:54,860 - INFO - Epoch 25/500, Train Loss: 0.6373, Val Loss: 1.4958
2024-12-28 14:04:56,070 - INFO - Epoch 26/500, Train Loss: 0.6260, Val Loss: 1.5225
2024-12-28 14:04:57,255 - INFO - Epoch 27/500, Train Loss: 0.6250, Val Loss: 1.4896
2024-12-28 14:04:58,441 - INFO - Epoch 28/500, Train Loss: 0.6186, Val Loss: 1.4849
2024-12-28 14:04:59,642 - INFO - Epoch 29/500, Train Loss: 0.6139, Val Loss: 1.4834
2024-12-28 14:05:00,776 - INFO - Epoch 30/500, Train Loss: 0.6061, Val Loss: 1.5243
2024-12-28 14:05:01,693 - INFO - Epoch 31/500, Train Loss: 0.6017, Val Loss: 1.5457
2024-12-28 14:05:02,763 - INFO - Epoch 32/500, Train Loss: 0.6021, Val Loss: 1.5083
2024-12-28 14:05:03,865 - INFO - Epoch 33/500, Train Loss: 0.5979, Val Loss: 1.4675
2024-12-28 14:05:04,971 - INFO - Epoch 34/500, Train Loss: 0.5893, Val Loss: 1.4500
2024-12-28 14:05:05,995 - INFO - Epoch 35/500, Train Loss: 0.5838, Val Loss: 1.4935
2024-12-28 14:05:07,143 - INFO - Epoch 36/500, Train Loss: 0.5901, Val Loss: 1.4641
2024-12-28 14:05:08,250 - INFO - Epoch 37/500, Train Loss: 0.5781, Val Loss: 1.4703
2024-12-28 14:05:09,285 - INFO - Epoch 38/500, Train Loss: 0.5784, Val Loss: 1.4809
2024-12-28 14:05:10,315 - INFO - Epoch 39/500, Train Loss: 0.5709, Val Loss: 1.4438
2024-12-28 14:05:11,389 - INFO - Epoch 40/500, Train Loss: 0.5707, Val Loss: 1.4927
2024-12-28 14:05:12,557 - INFO - Epoch 41/500, Train Loss: 0.5660, Val Loss: 1.4443
2024-12-28 14:05:13,761 - INFO - Epoch 42/500, Train Loss: 0.5740, Val Loss: 1.5169
2024-12-28 14:05:14,799 - INFO - Epoch 43/500, Train Loss: 0.5672, Val Loss: 1.4577
2024-12-28 14:05:15,875 - INFO - Epoch 44/500, Train Loss: 0.5686, Val Loss: 1.4522
2024-12-28 14:05:15,875 - INFO - Early stopping triggered at epoch 44
2024-12-28 14:05:15,875 - INFO - Training completed in 48.41s
2024-12-28 14:05:15,875 - INFO - Final memory usage: CPU 1408.4 MB, GPU 103.8 MB
2024-12-28 14:05:15,876 - INFO - Model training completed in 48.41s
2024-12-28 14:05:15,939 - INFO - Prediction completed in 0.06s
2024-12-28 14:05:15,949 - INFO - Poison rate 0.01 completed in 48.49s
2024-12-28 14:05:15,950 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:05:15,961 - INFO - Total number of labels flipped: 592
2024-12-28 14:05:15,961 - INFO - Label flipping completed in 0.01s
2024-12-28 14:05:15,961 - INFO - Training set processing completed in 0.00s
2024-12-28 14:05:15,961 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:05:15,962 - INFO - Memory usage at start_fit: CPU 1373.3 MB, GPU 103.7 MB
2024-12-28 14:05:15,962 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:05:15,967 - INFO - Number of unique classes: 43
2024-12-28 14:05:16,122 - INFO - Fitted scaler and transformed data
2024-12-28 14:05:16,122 - INFO - Scaling time: 0.15s
2024-12-28 14:05:17,395 - INFO - Epoch 1/500, Train Loss: 8.5624, Val Loss: 5.0759
2024-12-28 14:05:18,549 - INFO - Epoch 2/500, Train Loss: 3.9854, Val Loss: 4.3065
2024-12-28 14:05:19,595 - INFO - Epoch 3/500, Train Loss: 3.0908, Val Loss: 3.9173
2024-12-28 14:05:20,641 - INFO - Epoch 4/500, Train Loss: 2.5776, Val Loss: 3.7291
2024-12-28 14:05:21,666 - INFO - Epoch 5/500, Train Loss: 2.2264, Val Loss: 3.4896
2024-12-28 14:05:22,746 - INFO - Epoch 6/500, Train Loss: 1.9804, Val Loss: 3.3677
2024-12-28 14:05:23,925 - INFO - Epoch 7/500, Train Loss: 1.7956, Val Loss: 3.3272
2024-12-28 14:05:25,091 - INFO - Epoch 8/500, Train Loss: 1.6498, Val Loss: 3.2638
2024-12-28 14:05:26,249 - INFO - Epoch 9/500, Train Loss: 1.5366, Val Loss: 3.2171
2024-12-28 14:05:27,412 - INFO - Epoch 10/500, Train Loss: 1.4335, Val Loss: 3.1439
2024-12-28 14:05:28,520 - INFO - Epoch 11/500, Train Loss: 1.3652, Val Loss: 3.1322
2024-12-28 14:05:29,564 - INFO - Epoch 12/500, Train Loss: 1.2984, Val Loss: 3.0108
2024-12-28 14:05:30,661 - INFO - Epoch 13/500, Train Loss: 1.2378, Val Loss: 3.0461
2024-12-28 14:05:31,806 - INFO - Epoch 14/500, Train Loss: 1.1920, Val Loss: 3.0101
2024-12-28 14:05:32,997 - INFO - Epoch 15/500, Train Loss: 1.1431, Val Loss: 2.9756
2024-12-28 14:05:34,109 - INFO - Epoch 16/500, Train Loss: 1.1066, Val Loss: 3.0103
2024-12-28 14:05:35,209 - INFO - Epoch 17/500, Train Loss: 1.0773, Val Loss: 2.9611
2024-12-28 14:05:36,272 - INFO - Epoch 18/500, Train Loss: 1.0493, Val Loss: 2.9050
2024-12-28 14:05:37,318 - INFO - Epoch 19/500, Train Loss: 1.0255, Val Loss: 2.9658
2024-12-28 14:05:38,450 - INFO - Epoch 20/500, Train Loss: 0.9953, Val Loss: 2.9742
2024-12-28 14:05:39,513 - INFO - Epoch 21/500, Train Loss: 0.9727, Val Loss: 2.9881
2024-12-28 14:05:40,491 - INFO - Epoch 22/500, Train Loss: 0.9579, Val Loss: 2.9164
2024-12-28 14:05:41,525 - INFO - Epoch 23/500, Train Loss: 0.9446, Val Loss: 2.8918
2024-12-28 14:05:42,573 - INFO - Epoch 24/500, Train Loss: 0.9339, Val Loss: 2.8604
2024-12-28 14:05:43,710 - INFO - Epoch 25/500, Train Loss: 0.9140, Val Loss: 2.8570
2024-12-28 14:05:44,754 - INFO - Epoch 26/500, Train Loss: 0.8972, Val Loss: 2.9158
2024-12-28 14:05:45,876 - INFO - Epoch 27/500, Train Loss: 0.8927, Val Loss: 2.8535
2024-12-28 14:05:46,867 - INFO - Epoch 28/500, Train Loss: 0.8929, Val Loss: 2.8469
2024-12-28 14:05:47,918 - INFO - Epoch 29/500, Train Loss: 0.8757, Val Loss: 2.8256
2024-12-28 14:05:48,985 - INFO - Epoch 30/500, Train Loss: 0.8650, Val Loss: 2.8632
2024-12-28 14:05:49,987 - INFO - Epoch 31/500, Train Loss: 0.8621, Val Loss: 2.8364
2024-12-28 14:05:51,120 - INFO - Epoch 32/500, Train Loss: 0.8545, Val Loss: 2.7726
2024-12-28 14:05:52,275 - INFO - Epoch 33/500, Train Loss: 0.8492, Val Loss: 2.7947
2024-12-28 14:05:53,442 - INFO - Epoch 34/500, Train Loss: 0.8383, Val Loss: 2.8414
2024-12-28 14:05:54,647 - INFO - Epoch 35/500, Train Loss: 0.8342, Val Loss: 2.8630
2024-12-28 14:05:55,814 - INFO - Epoch 36/500, Train Loss: 0.8218, Val Loss: 2.8049
2024-12-28 14:05:56,985 - INFO - Epoch 37/500, Train Loss: 0.8219, Val Loss: 2.7816
2024-12-28 14:05:56,985 - INFO - Early stopping triggered at epoch 37
2024-12-28 14:05:56,985 - INFO - Training completed in 41.02s
2024-12-28 14:05:56,986 - INFO - Final memory usage: CPU 1412.9 MB, GPU 103.8 MB
2024-12-28 14:05:56,986 - INFO - Model training completed in 41.02s
2024-12-28 14:05:57,050 - INFO - Prediction completed in 0.06s
2024-12-28 14:05:57,080 - INFO - Poison rate 0.03 completed in 41.13s
2024-12-28 14:05:57,080 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:05:57,106 - INFO - Total number of labels flipped: 987
2024-12-28 14:05:57,106 - INFO - Label flipping completed in 0.03s
2024-12-28 14:05:57,106 - INFO - Training set processing completed in 0.00s
2024-12-28 14:05:57,106 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:05:57,107 - INFO - Memory usage at start_fit: CPU 1377.9 MB, GPU 103.7 MB
2024-12-28 14:05:57,107 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:05:57,113 - INFO - Number of unique classes: 43
2024-12-28 14:05:57,264 - INFO - Fitted scaler and transformed data
2024-12-28 14:05:57,264 - INFO - Scaling time: 0.15s
2024-12-28 14:05:58,447 - INFO - Epoch 1/500, Train Loss: 9.6919, Val Loss: 6.4063
2024-12-28 14:05:59,589 - INFO - Epoch 2/500, Train Loss: 4.8928, Val Loss: 5.4999
2024-12-28 14:06:00,686 - INFO - Epoch 3/500, Train Loss: 3.8512, Val Loss: 5.0237
2024-12-28 14:06:01,889 - INFO - Epoch 4/500, Train Loss: 3.2352, Val Loss: 4.7835
2024-12-28 14:06:02,908 - INFO - Epoch 5/500, Train Loss: 2.8119, Val Loss: 4.6880
2024-12-28 14:06:04,008 - INFO - Epoch 6/500, Train Loss: 2.5095, Val Loss: 4.6249
2024-12-28 14:06:05,018 - INFO - Epoch 7/500, Train Loss: 2.2796, Val Loss: 4.6144
2024-12-28 14:06:06,089 - INFO - Epoch 8/500, Train Loss: 2.1028, Val Loss: 4.4532
2024-12-28 14:06:07,168 - INFO - Epoch 9/500, Train Loss: 1.9692, Val Loss: 4.4373
2024-12-28 14:06:08,195 - INFO - Epoch 10/500, Train Loss: 1.8498, Val Loss: 4.4621
2024-12-28 14:06:09,292 - INFO - Epoch 11/500, Train Loss: 1.7484, Val Loss: 4.4275
2024-12-28 14:06:10,397 - INFO - Epoch 12/500, Train Loss: 1.6618, Val Loss: 4.4656
2024-12-28 14:06:11,516 - INFO - Epoch 13/500, Train Loss: 1.5924, Val Loss: 4.3713
2024-12-28 14:06:12,644 - INFO - Epoch 14/500, Train Loss: 1.5386, Val Loss: 4.3824
2024-12-28 14:06:13,911 - INFO - Epoch 15/500, Train Loss: 1.4840, Val Loss: 4.3841
2024-12-28 14:06:15,223 - INFO - Epoch 16/500, Train Loss: 1.4337, Val Loss: 4.3734
2024-12-28 14:06:16,439 - INFO - Epoch 17/500, Train Loss: 1.3885, Val Loss: 4.3058
2024-12-28 14:06:17,536 - INFO - Epoch 18/500, Train Loss: 1.3532, Val Loss: 4.3105
2024-12-28 14:06:18,646 - INFO - Epoch 19/500, Train Loss: 1.3326, Val Loss: 4.3260
2024-12-28 14:06:19,884 - INFO - Epoch 20/500, Train Loss: 1.2828, Val Loss: 4.3601
2024-12-28 14:06:21,101 - INFO - Epoch 21/500, Train Loss: 1.2696, Val Loss: 4.3107
2024-12-28 14:06:22,314 - INFO - Epoch 22/500, Train Loss: 1.2307, Val Loss: 4.3354
2024-12-28 14:06:22,314 - INFO - Early stopping triggered at epoch 22
2024-12-28 14:06:22,314 - INFO - Training completed in 25.21s
2024-12-28 14:06:22,315 - INFO - Final memory usage: CPU 1417.4 MB, GPU 103.8 MB
2024-12-28 14:06:22,315 - INFO - Model training completed in 25.21s
2024-12-28 14:06:22,378 - INFO - Prediction completed in 0.06s
2024-12-28 14:06:22,389 - INFO - Poison rate 0.05 completed in 25.31s
2024-12-28 14:06:22,389 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:06:22,414 - INFO - Total number of labels flipped: 1382
2024-12-28 14:06:22,414 - INFO - Label flipping completed in 0.02s
2024-12-28 14:06:22,414 - INFO - Training set processing completed in 0.00s
2024-12-28 14:06:22,414 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:06:22,415 - INFO - Memory usage at start_fit: CPU 1378.9 MB, GPU 103.7 MB
2024-12-28 14:06:22,416 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:06:22,422 - INFO - Number of unique classes: 43
2024-12-28 14:06:22,586 - INFO - Fitted scaler and transformed data
2024-12-28 14:06:22,586 - INFO - Scaling time: 0.16s
2024-12-28 14:06:23,739 - INFO - Epoch 1/500, Train Loss: 10.7896, Val Loss: 7.4868
2024-12-28 14:06:24,838 - INFO - Epoch 2/500, Train Loss: 5.9420, Val Loss: 6.6616
2024-12-28 14:06:25,981 - INFO - Epoch 3/500, Train Loss: 4.7831, Val Loss: 6.1941
2024-12-28 14:06:27,133 - INFO - Epoch 4/500, Train Loss: 4.0580, Val Loss: 6.0619
2024-12-28 14:06:28,227 - INFO - Epoch 5/500, Train Loss: 3.6091, Val Loss: 5.8925
2024-12-28 14:06:29,349 - INFO - Epoch 6/500, Train Loss: 3.2523, Val Loss: 5.9371
2024-12-28 14:06:30,480 - INFO - Epoch 7/500, Train Loss: 3.0073, Val Loss: 5.7170
2024-12-28 14:06:31,566 - INFO - Epoch 8/500, Train Loss: 2.7840, Val Loss: 5.7408
2024-12-28 14:06:32,670 - INFO - Epoch 9/500, Train Loss: 2.6059, Val Loss: 5.7232
2024-12-28 14:06:33,793 - INFO - Epoch 10/500, Train Loss: 2.4504, Val Loss: 5.6443
2024-12-28 14:06:34,953 - INFO - Epoch 11/500, Train Loss: 2.3346, Val Loss: 5.5776
2024-12-28 14:06:36,082 - INFO - Epoch 12/500, Train Loss: 2.2333, Val Loss: 5.5983
2024-12-28 14:06:37,218 - INFO - Epoch 13/500, Train Loss: 2.1409, Val Loss: 5.5516
2024-12-28 14:06:38,322 - INFO - Epoch 14/500, Train Loss: 2.0584, Val Loss: 5.5366
2024-12-28 14:06:39,512 - INFO - Epoch 15/500, Train Loss: 2.0005, Val Loss: 5.5492
2024-12-28 14:06:40,591 - INFO - Epoch 16/500, Train Loss: 1.9248, Val Loss: 5.5599
2024-12-28 14:06:41,672 - INFO - Epoch 17/500, Train Loss: 1.8751, Val Loss: 5.5879
2024-12-28 14:06:42,707 - INFO - Epoch 18/500, Train Loss: 1.8224, Val Loss: 5.5296
2024-12-28 14:06:43,745 - INFO - Epoch 19/500, Train Loss: 1.7790, Val Loss: 5.5188
2024-12-28 14:06:44,845 - INFO - Epoch 20/500, Train Loss: 1.7436, Val Loss: 5.4891
2024-12-28 14:06:45,889 - INFO - Epoch 21/500, Train Loss: 1.7014, Val Loss: 5.4195
2024-12-28 14:06:46,912 - INFO - Epoch 22/500, Train Loss: 1.6828, Val Loss: 5.5027
2024-12-28 14:06:47,969 - INFO - Epoch 23/500, Train Loss: 1.6388, Val Loss: 5.4324
2024-12-28 14:06:49,084 - INFO - Epoch 24/500, Train Loss: 1.6098, Val Loss: 5.4911
2024-12-28 14:06:50,157 - INFO - Epoch 25/500, Train Loss: 1.5873, Val Loss: 5.4987
2024-12-28 14:06:51,250 - INFO - Epoch 26/500, Train Loss: 1.5727, Val Loss: 5.5500
2024-12-28 14:06:51,251 - INFO - Early stopping triggered at epoch 26
2024-12-28 14:06:51,251 - INFO - Training completed in 28.84s
2024-12-28 14:06:51,251 - INFO - Final memory usage: CPU 1417.5 MB, GPU 103.8 MB
2024-12-28 14:06:51,252 - INFO - Model training completed in 28.84s
2024-12-28 14:06:51,324 - INFO - Prediction completed in 0.07s
2024-12-28 14:06:51,335 - INFO - Poison rate 0.07 completed in 28.95s
2024-12-28 14:06:51,335 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:06:51,375 - INFO - Total number of labels flipped: 1975
2024-12-28 14:06:51,375 - INFO - Label flipping completed in 0.04s
2024-12-28 14:06:51,375 - INFO - Training set processing completed in 0.00s
2024-12-28 14:06:51,375 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:06:51,376 - INFO - Memory usage at start_fit: CPU 1378.9 MB, GPU 103.7 MB
2024-12-28 14:06:51,376 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:06:51,382 - INFO - Number of unique classes: 43
2024-12-28 14:06:51,554 - INFO - Fitted scaler and transformed data
2024-12-28 14:06:51,554 - INFO - Scaling time: 0.17s
2024-12-28 14:06:52,764 - INFO - Epoch 1/500, Train Loss: 12.2521, Val Loss: 8.6237
2024-12-28 14:06:53,811 - INFO - Epoch 2/500, Train Loss: 7.2732, Val Loss: 7.6879
2024-12-28 14:06:54,999 - INFO - Epoch 3/500, Train Loss: 5.9801, Val Loss: 7.3452
2024-12-28 14:06:56,231 - INFO - Epoch 4/500, Train Loss: 5.1839, Val Loss: 7.1707
2024-12-28 14:06:57,487 - INFO - Epoch 5/500, Train Loss: 4.6431, Val Loss: 7.1928
2024-12-28 14:06:58,603 - INFO - Epoch 6/500, Train Loss: 4.2321, Val Loss: 6.9856
2024-12-28 14:06:59,783 - INFO - Epoch 7/500, Train Loss: 3.9183, Val Loss: 7.0431
2024-12-28 14:07:00,928 - INFO - Epoch 8/500, Train Loss: 3.6408, Val Loss: 7.0448
2024-12-28 14:07:01,973 - INFO - Epoch 9/500, Train Loss: 3.4427, Val Loss: 7.0197
2024-12-28 14:07:02,956 - INFO - Epoch 10/500, Train Loss: 3.2550, Val Loss: 7.1189
2024-12-28 14:07:04,029 - INFO - Epoch 11/500, Train Loss: 3.1277, Val Loss: 6.8512
2024-12-28 14:07:05,246 - INFO - Epoch 12/500, Train Loss: 2.9840, Val Loss: 6.8824
2024-12-28 14:07:06,418 - INFO - Epoch 13/500, Train Loss: 2.8901, Val Loss: 6.9804
2024-12-28 14:07:07,514 - INFO - Epoch 14/500, Train Loss: 2.7956, Val Loss: 6.9802
2024-12-28 14:07:08,599 - INFO - Epoch 15/500, Train Loss: 2.6980, Val Loss: 6.9293
2024-12-28 14:07:09,658 - INFO - Epoch 16/500, Train Loss: 2.6247, Val Loss: 6.9535
2024-12-28 14:07:09,658 - INFO - Early stopping triggered at epoch 16
2024-12-28 14:07:09,658 - INFO - Training completed in 18.28s
2024-12-28 14:07:09,659 - INFO - Final memory usage: CPU 1417.9 MB, GPU 103.8 MB
2024-12-28 14:07:09,659 - INFO - Model training completed in 18.28s
2024-12-28 14:07:09,723 - INFO - Prediction completed in 0.06s
2024-12-28 14:07:09,733 - INFO - Poison rate 0.1 completed in 18.40s
2024-12-28 14:07:09,733 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:07:09,803 - INFO - Total number of labels flipped: 3951
2024-12-28 14:07:09,804 - INFO - Label flipping completed in 0.07s
2024-12-28 14:07:09,804 - INFO - Training set processing completed in 0.00s
2024-12-28 14:07:09,804 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:07:09,805 - INFO - Memory usage at start_fit: CPU 1379.3 MB, GPU 103.7 MB
2024-12-28 14:07:09,805 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:07:09,811 - INFO - Number of unique classes: 43
2024-12-28 14:07:09,952 - INFO - Fitted scaler and transformed data
2024-12-28 14:07:09,952 - INFO - Scaling time: 0.14s
2024-12-28 14:07:11,067 - INFO - Epoch 1/500, Train Loss: 16.5268, Val Loss: 14.6003
2024-12-28 14:07:12,227 - INFO - Epoch 2/500, Train Loss: 11.3991, Val Loss: 13.8971
2024-12-28 14:07:13,410 - INFO - Epoch 3/500, Train Loss: 9.8693, Val Loss: 13.6283
2024-12-28 14:07:14,585 - INFO - Epoch 4/500, Train Loss: 8.8730, Val Loss: 13.5271
2024-12-28 14:07:15,756 - INFO - Epoch 5/500, Train Loss: 8.1740, Val Loss: 13.4665
2024-12-28 14:07:16,935 - INFO - Epoch 6/500, Train Loss: 7.6234, Val Loss: 13.4236
2024-12-28 14:07:17,988 - INFO - Epoch 7/500, Train Loss: 7.2309, Val Loss: 13.6357
2024-12-28 14:07:19,196 - INFO - Epoch 8/500, Train Loss: 6.9051, Val Loss: 13.4496
2024-12-28 14:07:20,407 - INFO - Epoch 9/500, Train Loss: 6.5743, Val Loss: 13.6316
2024-12-28 14:07:21,525 - INFO - Epoch 10/500, Train Loss: 6.3331, Val Loss: 13.5358
2024-12-28 14:07:22,682 - INFO - Epoch 11/500, Train Loss: 6.1487, Val Loss: 13.5927
2024-12-28 14:07:22,682 - INFO - Early stopping triggered at epoch 11
2024-12-28 14:07:22,682 - INFO - Training completed in 12.88s
2024-12-28 14:07:22,683 - INFO - Final memory usage: CPU 1417.9 MB, GPU 103.8 MB
2024-12-28 14:07:22,683 - INFO - Model training completed in 12.88s
2024-12-28 14:07:22,763 - INFO - Prediction completed in 0.08s
2024-12-28 14:07:22,793 - INFO - Poison rate 0.2 completed in 13.06s
2024-12-28 14:07:22,794 - INFO - Total results to save: 7
2024-12-28 14:07:22,797 - INFO - Saved 7 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:07:22,797 - INFO - Total evaluation time: 246.80s
2024-12-28 14:07:22,804 - INFO - 
Progress: 2.1% - Evaluating GTSRB with SVM (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:07:22,999 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:07:22,999 - INFO - Dataset type: image
2024-12-28 14:07:23,000 - INFO - Sample size: 39209
2024-12-28 14:07:23,000 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:07:23,001 - INFO - Loading datasets...
2024-12-28 14:07:41,830 - INFO - Dataset loading completed in 18.83s
2024-12-28 14:07:41,830 - INFO - Extracting validation features...
2024-12-28 14:07:41,830 - INFO - Extracting features from 4435 samples...
2024-12-28 14:07:42,562 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:07:42,571 - INFO - Validation feature extraction completed in 0.74s
2024-12-28 14:07:42,572 - INFO - Extracting training features...
2024-12-28 14:07:42,572 - INFO - Extracting features from 19755 samples...
2024-12-28 14:07:45,229 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:07:45,244 - INFO - Training feature extraction completed in 2.67s
2024-12-28 14:07:45,244 - INFO - Creating model for classifier: SVM
2024-12-28 14:07:45,244 - INFO - Using device: cuda
2024-12-28 14:07:45,244 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 14:07:45,244 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:07:45,245 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:07:45,245 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:07:46,586 - INFO - Feature scaling completed in 1.34s
2024-12-28 14:07:46,586 - INFO - Starting feature selection (k=50)
2024-12-28 14:07:46,814 - INFO - Feature selection completed in 0.23s. Output shape: (19755, 50)
2024-12-28 14:07:46,815 - INFO - Starting anomaly detection
2024-12-28 14:07:52,368 - INFO - Anomaly detection completed in 5.55s
2024-12-28 14:07:52,368 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:07:52,369 - INFO - Total fit_transform time: 7.12s
2024-12-28 14:07:52,369 - INFO - Training set processing completed in 7.12s
2024-12-28 14:07:52,369 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:07:52,370 - INFO - Memory usage at start_fit: CPU 1668.6 MB, GPU 103.1 MB
2024-12-28 14:07:52,371 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:07:52,381 - INFO - Number of unique classes: 43
2024-12-28 14:07:52,531 - INFO - Fitted scaler and transformed data
2024-12-28 14:07:52,531 - INFO - Scaling time: 0.15s
2024-12-28 14:07:53,375 - INFO - Epoch 1/500, Train Loss: 6.5431, Val Loss: 3.1775
2024-12-28 14:07:54,449 - INFO - Epoch 2/500, Train Loss: 2.3830, Val Loss: 2.2586
2024-12-28 14:07:55,565 - INFO - Epoch 3/500, Train Loss: 1.7276, Val Loss: 1.8502
2024-12-28 14:07:56,582 - INFO - Epoch 4/500, Train Loss: 1.3987, Val Loss: 1.6297
2024-12-28 14:07:57,665 - INFO - Epoch 5/500, Train Loss: 1.1942, Val Loss: 1.4493
2024-12-28 14:07:58,691 - INFO - Epoch 6/500, Train Loss: 1.0518, Val Loss: 1.3704
2024-12-28 14:07:59,761 - INFO - Epoch 7/500, Train Loss: 0.9450, Val Loss: 1.3144
2024-12-28 14:08:00,905 - INFO - Epoch 8/500, Train Loss: 0.8673, Val Loss: 1.2325
2024-12-28 14:08:01,974 - INFO - Epoch 9/500, Train Loss: 0.8032, Val Loss: 1.1580
2024-12-28 14:08:03,008 - INFO - Epoch 10/500, Train Loss: 0.7517, Val Loss: 1.1506
2024-12-28 14:08:04,024 - INFO - Epoch 11/500, Train Loss: 0.7104, Val Loss: 1.1089
2024-12-28 14:08:05,144 - INFO - Epoch 12/500, Train Loss: 0.6750, Val Loss: 1.0945
2024-12-28 14:08:06,316 - INFO - Epoch 13/500, Train Loss: 0.6506, Val Loss: 1.0554
2024-12-28 14:08:07,550 - INFO - Epoch 14/500, Train Loss: 0.6190, Val Loss: 1.0435
2024-12-28 14:08:08,675 - INFO - Epoch 15/500, Train Loss: 0.5950, Val Loss: 1.0056
2024-12-28 14:08:09,841 - INFO - Epoch 16/500, Train Loss: 0.5759, Val Loss: 1.0080
2024-12-28 14:08:11,016 - INFO - Epoch 17/500, Train Loss: 0.5626, Val Loss: 1.0160
2024-12-28 14:08:12,196 - INFO - Epoch 18/500, Train Loss: 0.5450, Val Loss: 0.9852
2024-12-28 14:08:13,401 - INFO - Epoch 19/500, Train Loss: 0.5322, Val Loss: 0.9901
2024-12-28 14:08:14,573 - INFO - Epoch 20/500, Train Loss: 0.5240, Val Loss: 0.9646
2024-12-28 14:08:15,749 - INFO - Epoch 21/500, Train Loss: 0.5092, Val Loss: 0.9765
2024-12-28 14:08:16,786 - INFO - Epoch 22/500, Train Loss: 0.5027, Val Loss: 0.9660
2024-12-28 14:08:17,892 - INFO - Epoch 23/500, Train Loss: 0.4993, Val Loss: 0.9617
2024-12-28 14:08:18,975 - INFO - Epoch 24/500, Train Loss: 0.4934, Val Loss: 0.9732
2024-12-28 14:08:20,158 - INFO - Epoch 25/500, Train Loss: 0.4837, Val Loss: 0.9370
2024-12-28 14:08:21,268 - INFO - Epoch 26/500, Train Loss: 0.4762, Val Loss: 0.9779
2024-12-28 14:08:22,465 - INFO - Epoch 27/500, Train Loss: 0.4746, Val Loss: 0.9515
2024-12-28 14:08:23,554 - INFO - Epoch 28/500, Train Loss: 0.4704, Val Loss: 0.9194
2024-12-28 14:08:24,722 - INFO - Epoch 29/500, Train Loss: 0.4677, Val Loss: 0.9153
2024-12-28 14:08:25,834 - INFO - Epoch 30/500, Train Loss: 0.4597, Val Loss: 0.9294
2024-12-28 14:08:26,923 - INFO - Epoch 31/500, Train Loss: 0.4560, Val Loss: 0.9138
2024-12-28 14:08:27,993 - INFO - Epoch 32/500, Train Loss: 0.4491, Val Loss: 0.9123
2024-12-28 14:08:29,123 - INFO - Epoch 33/500, Train Loss: 0.4524, Val Loss: 0.9190
2024-12-28 14:08:30,261 - INFO - Epoch 34/500, Train Loss: 0.4470, Val Loss: 0.9275
2024-12-28 14:08:31,396 - INFO - Epoch 35/500, Train Loss: 0.4526, Val Loss: 0.9192
2024-12-28 14:08:32,508 - INFO - Epoch 36/500, Train Loss: 0.4433, Val Loss: 0.9460
2024-12-28 14:08:33,495 - INFO - Epoch 37/500, Train Loss: 0.4449, Val Loss: 0.9444
2024-12-28 14:08:33,495 - INFO - Early stopping triggered at epoch 37
2024-12-28 14:08:33,495 - INFO - Training completed in 41.13s
2024-12-28 14:08:33,496 - INFO - Final memory usage: CPU 1707.8 MB, GPU 103.6 MB
2024-12-28 14:08:33,496 - INFO - Model training completed in 41.13s
2024-12-28 14:08:33,558 - INFO - Prediction completed in 0.06s
2024-12-28 14:08:33,570 - INFO - Poison rate 0.0 completed in 48.33s
2024-12-28 14:08:33,570 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:08:33,575 - INFO - Total number of labels flipped: 197
2024-12-28 14:08:33,575 - INFO - Label flipping completed in 0.01s
2024-12-28 14:08:33,576 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:08:33,576 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:08:34,930 - INFO - Feature scaling completed in 1.35s
2024-12-28 14:08:34,930 - INFO - Starting feature selection (k=50)
2024-12-28 14:08:34,959 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:08:34,959 - INFO - Starting anomaly detection
2024-12-28 14:08:40,039 - INFO - Anomaly detection completed in 5.08s
2024-12-28 14:08:40,039 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:08:40,039 - INFO - Total fit_transform time: 6.46s
2024-12-28 14:08:40,039 - INFO - Training set processing completed in 6.46s
2024-12-28 14:08:40,039 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:08:40,040 - INFO - Memory usage at start_fit: CPU 1688.5 MB, GPU 103.5 MB
2024-12-28 14:08:40,040 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:08:40,048 - INFO - Number of unique classes: 43
2024-12-28 14:08:40,196 - INFO - Fitted scaler and transformed data
2024-12-28 14:08:40,196 - INFO - Scaling time: 0.15s
2024-12-28 14:08:41,309 - INFO - Epoch 1/500, Train Loss: 7.1115, Val Loss: 3.8711
2024-12-28 14:08:42,473 - INFO - Epoch 2/500, Train Loss: 2.8590, Val Loss: 2.9265
2024-12-28 14:08:43,602 - INFO - Epoch 3/500, Train Loss: 2.1279, Val Loss: 2.4777
2024-12-28 14:08:44,586 - INFO - Epoch 4/500, Train Loss: 1.7314, Val Loss: 2.2210
2024-12-28 14:08:45,651 - INFO - Epoch 5/500, Train Loss: 1.4828, Val Loss: 2.0507
2024-12-28 14:08:46,825 - INFO - Epoch 6/500, Train Loss: 1.3017, Val Loss: 1.9738
2024-12-28 14:08:47,918 - INFO - Epoch 7/500, Train Loss: 1.1741, Val Loss: 1.9038
2024-12-28 14:08:49,056 - INFO - Epoch 8/500, Train Loss: 1.0774, Val Loss: 1.8359
2024-12-28 14:08:50,207 - INFO - Epoch 9/500, Train Loss: 0.9936, Val Loss: 1.7796
2024-12-28 14:08:51,429 - INFO - Epoch 10/500, Train Loss: 0.9243, Val Loss: 1.6850
2024-12-28 14:08:52,542 - INFO - Epoch 11/500, Train Loss: 0.8750, Val Loss: 1.6997
2024-12-28 14:08:53,664 - INFO - Epoch 12/500, Train Loss: 0.8295, Val Loss: 1.6825
2024-12-28 14:08:54,765 - INFO - Epoch 13/500, Train Loss: 0.7938, Val Loss: 1.6719
2024-12-28 14:08:55,829 - INFO - Epoch 14/500, Train Loss: 0.7607, Val Loss: 1.6312
2024-12-28 14:08:56,961 - INFO - Epoch 15/500, Train Loss: 0.7309, Val Loss: 1.6172
2024-12-28 14:08:58,029 - INFO - Epoch 16/500, Train Loss: 0.7115, Val Loss: 1.6166
2024-12-28 14:08:58,994 - INFO - Epoch 17/500, Train Loss: 0.6850, Val Loss: 1.5766
2024-12-28 14:08:59,974 - INFO - Epoch 18/500, Train Loss: 0.6653, Val Loss: 1.5637
2024-12-28 14:09:01,081 - INFO - Epoch 19/500, Train Loss: 0.6484, Val Loss: 1.5881
2024-12-28 14:09:02,284 - INFO - Epoch 20/500, Train Loss: 0.6438, Val Loss: 1.5521
2024-12-28 14:09:03,337 - INFO - Epoch 21/500, Train Loss: 0.6304, Val Loss: 1.5546
2024-12-28 14:09:04,394 - INFO - Epoch 22/500, Train Loss: 0.6218, Val Loss: 1.5275
2024-12-28 14:09:05,455 - INFO - Epoch 23/500, Train Loss: 0.6062, Val Loss: 1.5511
2024-12-28 14:09:06,580 - INFO - Epoch 24/500, Train Loss: 0.5937, Val Loss: 1.5579
2024-12-28 14:09:07,798 - INFO - Epoch 25/500, Train Loss: 0.5884, Val Loss: 1.5172
2024-12-28 14:09:08,988 - INFO - Epoch 26/500, Train Loss: 0.5854, Val Loss: 1.5241
2024-12-28 14:09:10,078 - INFO - Epoch 27/500, Train Loss: 0.5787, Val Loss: 1.5548
2024-12-28 14:09:11,175 - INFO - Epoch 28/500, Train Loss: 0.5704, Val Loss: 1.5197
2024-12-28 14:09:12,203 - INFO - Epoch 29/500, Train Loss: 0.5666, Val Loss: 1.5321
2024-12-28 14:09:13,301 - INFO - Epoch 30/500, Train Loss: 0.5584, Val Loss: 1.5414
2024-12-28 14:09:13,301 - INFO - Early stopping triggered at epoch 30
2024-12-28 14:09:13,301 - INFO - Training completed in 33.26s
2024-12-28 14:09:13,301 - INFO - Final memory usage: CPU 1727.2 MB, GPU 103.6 MB
2024-12-28 14:09:13,302 - INFO - Model training completed in 33.26s
2024-12-28 14:09:13,390 - INFO - Prediction completed in 0.09s
2024-12-28 14:09:13,401 - INFO - Poison rate 0.01 completed in 39.83s
2024-12-28 14:09:13,401 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:09:13,413 - INFO - Total number of labels flipped: 592
2024-12-28 14:09:13,413 - INFO - Label flipping completed in 0.01s
2024-12-28 14:09:13,413 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:09:13,413 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:09:14,599 - INFO - Feature scaling completed in 1.19s
2024-12-28 14:09:14,599 - INFO - Starting feature selection (k=50)
2024-12-28 14:09:14,627 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:09:14,627 - INFO - Starting anomaly detection
2024-12-28 14:09:22,413 - INFO - Anomaly detection completed in 7.79s
2024-12-28 14:09:22,413 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:09:22,413 - INFO - Total fit_transform time: 9.00s
2024-12-28 14:09:22,413 - INFO - Training set processing completed in 9.00s
2024-12-28 14:09:22,413 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:09:22,415 - INFO - Memory usage at start_fit: CPU 1688.7 MB, GPU 103.5 MB
2024-12-28 14:09:22,415 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:09:22,421 - INFO - Number of unique classes: 43
2024-12-28 14:09:22,561 - INFO - Fitted scaler and transformed data
2024-12-28 14:09:22,561 - INFO - Scaling time: 0.14s
2024-12-28 14:09:23,521 - INFO - Epoch 1/500, Train Loss: 8.1143, Val Loss: 5.4674
2024-12-28 14:09:24,591 - INFO - Epoch 2/500, Train Loss: 3.7726, Val Loss: 4.5277
2024-12-28 14:09:25,646 - INFO - Epoch 3/500, Train Loss: 2.8905, Val Loss: 4.0376
2024-12-28 14:09:26,679 - INFO - Epoch 4/500, Train Loss: 2.3789, Val Loss: 3.7999
2024-12-28 14:09:27,761 - INFO - Epoch 5/500, Train Loss: 2.0678, Val Loss: 3.5873
2024-12-28 14:09:28,965 - INFO - Epoch 6/500, Train Loss: 1.8337, Val Loss: 3.5010
2024-12-28 14:09:30,051 - INFO - Epoch 7/500, Train Loss: 1.6543, Val Loss: 3.4368
2024-12-28 14:09:31,051 - INFO - Epoch 8/500, Train Loss: 1.5215, Val Loss: 3.3831
2024-12-28 14:09:31,974 - INFO - Epoch 9/500, Train Loss: 1.4074, Val Loss: 3.2738
2024-12-28 14:09:33,121 - INFO - Epoch 10/500, Train Loss: 1.3154, Val Loss: 3.2854
2024-12-28 14:09:34,161 - INFO - Epoch 11/500, Train Loss: 1.2443, Val Loss: 3.2641
2024-12-28 14:09:35,240 - INFO - Epoch 12/500, Train Loss: 1.1857, Val Loss: 3.2290
2024-12-28 14:09:36,400 - INFO - Epoch 13/500, Train Loss: 1.1327, Val Loss: 3.1427
2024-12-28 14:09:37,577 - INFO - Epoch 14/500, Train Loss: 1.0901, Val Loss: 3.1559
2024-12-28 14:09:38,738 - INFO - Epoch 15/500, Train Loss: 1.0486, Val Loss: 3.1714
2024-12-28 14:09:39,853 - INFO - Epoch 16/500, Train Loss: 1.0131, Val Loss: 3.1795
2024-12-28 14:09:40,859 - INFO - Epoch 17/500, Train Loss: 0.9824, Val Loss: 3.1041
2024-12-28 14:09:41,957 - INFO - Epoch 18/500, Train Loss: 0.9608, Val Loss: 3.0906
2024-12-28 14:09:43,045 - INFO - Epoch 19/500, Train Loss: 0.9348, Val Loss: 3.0561
2024-12-28 14:09:44,141 - INFO - Epoch 20/500, Train Loss: 0.9148, Val Loss: 3.0950
2024-12-28 14:09:45,178 - INFO - Epoch 21/500, Train Loss: 0.8939, Val Loss: 3.1512
2024-12-28 14:09:46,350 - INFO - Epoch 22/500, Train Loss: 0.8833, Val Loss: 3.0620
2024-12-28 14:09:47,503 - INFO - Epoch 23/500, Train Loss: 0.8707, Val Loss: 3.0781
2024-12-28 14:09:48,604 - INFO - Epoch 24/500, Train Loss: 0.8532, Val Loss: 3.0552
2024-12-28 14:09:48,604 - INFO - Early stopping triggered at epoch 24
2024-12-28 14:09:48,604 - INFO - Training completed in 26.19s
2024-12-28 14:09:48,604 - INFO - Final memory usage: CPU 1727.3 MB, GPU 103.6 MB
2024-12-28 14:09:48,605 - INFO - Model training completed in 26.19s
2024-12-28 14:09:48,691 - INFO - Prediction completed in 0.09s
2024-12-28 14:09:48,702 - INFO - Poison rate 0.03 completed in 35.30s
2024-12-28 14:09:48,702 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:09:48,721 - INFO - Total number of labels flipped: 987
2024-12-28 14:09:48,721 - INFO - Label flipping completed in 0.02s
2024-12-28 14:09:48,722 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:09:48,722 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:09:49,969 - INFO - Feature scaling completed in 1.25s
2024-12-28 14:09:49,969 - INFO - Starting feature selection (k=50)
2024-12-28 14:09:49,996 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:09:49,996 - INFO - Starting anomaly detection
2024-12-28 14:09:58,181 - INFO - Anomaly detection completed in 8.18s
2024-12-28 14:09:58,182 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:09:58,182 - INFO - Total fit_transform time: 9.46s
2024-12-28 14:09:58,182 - INFO - Training set processing completed in 9.46s
2024-12-28 14:09:58,182 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:09:58,183 - INFO - Memory usage at start_fit: CPU 1688.8 MB, GPU 103.5 MB
2024-12-28 14:09:58,184 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:09:58,190 - INFO - Number of unique classes: 43
2024-12-28 14:09:58,346 - INFO - Fitted scaler and transformed data
2024-12-28 14:09:58,346 - INFO - Scaling time: 0.16s
2024-12-28 14:09:59,431 - INFO - Epoch 1/500, Train Loss: 8.8585, Val Loss: 6.2987
2024-12-28 14:10:00,614 - INFO - Epoch 2/500, Train Loss: 4.5913, Val Loss: 5.4086
2024-12-28 14:10:01,740 - INFO - Epoch 3/500, Train Loss: 3.6076, Val Loss: 4.9263
2024-12-28 14:10:02,909 - INFO - Epoch 4/500, Train Loss: 3.0334, Val Loss: 4.6664
2024-12-28 14:10:04,058 - INFO - Epoch 5/500, Train Loss: 2.6405, Val Loss: 4.5612
2024-12-28 14:10:05,225 - INFO - Epoch 6/500, Train Loss: 2.3674, Val Loss: 4.4036
2024-12-28 14:10:06,359 - INFO - Epoch 7/500, Train Loss: 2.1533, Val Loss: 4.2968
2024-12-28 14:10:07,537 - INFO - Epoch 8/500, Train Loss: 1.9840, Val Loss: 4.2947
2024-12-28 14:10:08,670 - INFO - Epoch 9/500, Train Loss: 1.8455, Val Loss: 4.2709
2024-12-28 14:10:09,721 - INFO - Epoch 10/500, Train Loss: 1.7448, Val Loss: 4.2080
2024-12-28 14:10:10,906 - INFO - Epoch 11/500, Train Loss: 1.6496, Val Loss: 4.1948
2024-12-28 14:10:12,048 - INFO - Epoch 12/500, Train Loss: 1.5813, Val Loss: 4.2496
2024-12-28 14:10:13,154 - INFO - Epoch 13/500, Train Loss: 1.5123, Val Loss: 4.1122
2024-12-28 14:10:14,318 - INFO - Epoch 14/500, Train Loss: 1.4508, Val Loss: 4.1485
2024-12-28 14:10:15,486 - INFO - Epoch 15/500, Train Loss: 1.3982, Val Loss: 4.0883
2024-12-28 14:10:16,613 - INFO - Epoch 16/500, Train Loss: 1.3503, Val Loss: 4.0570
2024-12-28 14:10:17,735 - INFO - Epoch 17/500, Train Loss: 1.3192, Val Loss: 4.0491
2024-12-28 14:10:18,853 - INFO - Epoch 18/500, Train Loss: 1.2852, Val Loss: 4.1166
2024-12-28 14:10:20,001 - INFO - Epoch 19/500, Train Loss: 1.2553, Val Loss: 4.0650
2024-12-28 14:10:21,126 - INFO - Epoch 20/500, Train Loss: 1.2351, Val Loss: 4.1135
2024-12-28 14:10:22,131 - INFO - Epoch 21/500, Train Loss: 1.2045, Val Loss: 4.1320
2024-12-28 14:10:23,223 - INFO - Epoch 22/500, Train Loss: 1.1814, Val Loss: 4.0536
2024-12-28 14:10:23,223 - INFO - Early stopping triggered at epoch 22
2024-12-28 14:10:23,223 - INFO - Training completed in 25.04s
2024-12-28 14:10:23,224 - INFO - Final memory usage: CPU 1727.5 MB, GPU 103.6 MB
2024-12-28 14:10:23,224 - INFO - Model training completed in 25.04s
2024-12-28 14:10:23,319 - INFO - Prediction completed in 0.09s
2024-12-28 14:10:23,331 - INFO - Poison rate 0.05 completed in 34.63s
2024-12-28 14:10:23,331 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:10:23,373 - INFO - Total number of labels flipped: 1382
2024-12-28 14:10:23,373 - INFO - Label flipping completed in 0.04s
2024-12-28 14:10:23,373 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:10:23,373 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:10:24,630 - INFO - Feature scaling completed in 1.26s
2024-12-28 14:10:24,630 - INFO - Starting feature selection (k=50)
2024-12-28 14:10:24,657 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:10:24,658 - INFO - Starting anomaly detection
2024-12-28 14:10:32,643 - INFO - Anomaly detection completed in 7.99s
2024-12-28 14:10:32,643 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:10:32,644 - INFO - Total fit_transform time: 9.27s
2024-12-28 14:10:32,644 - INFO - Training set processing completed in 9.27s
2024-12-28 14:10:32,644 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:10:32,645 - INFO - Memory usage at start_fit: CPU 1688.9 MB, GPU 103.5 MB
2024-12-28 14:10:32,645 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:10:32,652 - INFO - Number of unique classes: 43
2024-12-28 14:10:32,792 - INFO - Fitted scaler and transformed data
2024-12-28 14:10:32,792 - INFO - Scaling time: 0.14s
2024-12-28 14:10:33,772 - INFO - Epoch 1/500, Train Loss: 10.1032, Val Loss: 7.3475
2024-12-28 14:10:34,821 - INFO - Epoch 2/500, Train Loss: 5.4647, Val Loss: 6.3389
2024-12-28 14:10:35,858 - INFO - Epoch 3/500, Train Loss: 4.3952, Val Loss: 6.0861
2024-12-28 14:10:36,827 - INFO - Epoch 4/500, Train Loss: 3.7682, Val Loss: 5.8552
2024-12-28 14:10:38,034 - INFO - Epoch 5/500, Train Loss: 3.3349, Val Loss: 5.7836
2024-12-28 14:10:39,088 - INFO - Epoch 6/500, Train Loss: 3.0255, Val Loss: 5.6920
2024-12-28 14:10:40,163 - INFO - Epoch 7/500, Train Loss: 2.7763, Val Loss: 5.6562
2024-12-28 14:10:41,236 - INFO - Epoch 8/500, Train Loss: 2.5749, Val Loss: 5.5460
2024-12-28 14:10:42,277 - INFO - Epoch 9/500, Train Loss: 2.4206, Val Loss: 5.5995
2024-12-28 14:10:43,378 - INFO - Epoch 10/500, Train Loss: 2.2954, Val Loss: 5.5910
2024-12-28 14:10:44,409 - INFO - Epoch 11/500, Train Loss: 2.1698, Val Loss: 5.5249
2024-12-28 14:10:45,464 - INFO - Epoch 12/500, Train Loss: 2.0637, Val Loss: 5.6189
2024-12-28 14:10:46,523 - INFO - Epoch 13/500, Train Loss: 1.9918, Val Loss: 5.4903
2024-12-28 14:10:47,602 - INFO - Epoch 14/500, Train Loss: 1.9154, Val Loss: 5.4597
2024-12-28 14:10:48,731 - INFO - Epoch 15/500, Train Loss: 1.8552, Val Loss: 5.4745
2024-12-28 14:10:49,960 - INFO - Epoch 16/500, Train Loss: 1.7892, Val Loss: 5.4602
2024-12-28 14:10:51,190 - INFO - Epoch 17/500, Train Loss: 1.7413, Val Loss: 5.4982
2024-12-28 14:10:52,358 - INFO - Epoch 18/500, Train Loss: 1.7029, Val Loss: 5.5054
2024-12-28 14:10:53,569 - INFO - Epoch 19/500, Train Loss: 1.6643, Val Loss: 5.4381
2024-12-28 14:10:54,816 - INFO - Epoch 20/500, Train Loss: 1.6168, Val Loss: 5.4269
2024-12-28 14:10:56,009 - INFO - Epoch 21/500, Train Loss: 1.5921, Val Loss: 5.5017
2024-12-28 14:10:57,124 - INFO - Epoch 22/500, Train Loss: 1.5624, Val Loss: 5.4808
2024-12-28 14:10:58,271 - INFO - Epoch 23/500, Train Loss: 1.5363, Val Loss: 5.4082
2024-12-28 14:10:59,449 - INFO - Epoch 24/500, Train Loss: 1.5033, Val Loss: 5.4449
2024-12-28 14:11:00,612 - INFO - Epoch 25/500, Train Loss: 1.4769, Val Loss: 5.5122
2024-12-28 14:11:01,621 - INFO - Epoch 26/500, Train Loss: 1.4557, Val Loss: 5.4397
2024-12-28 14:11:02,692 - INFO - Epoch 27/500, Train Loss: 1.4455, Val Loss: 5.4910
2024-12-28 14:11:03,856 - INFO - Epoch 28/500, Train Loss: 1.4305, Val Loss: 5.3956
2024-12-28 14:11:05,021 - INFO - Epoch 29/500, Train Loss: 1.3995, Val Loss: 5.5018
2024-12-28 14:11:06,113 - INFO - Epoch 30/500, Train Loss: 1.3864, Val Loss: 5.5044
2024-12-28 14:11:07,222 - INFO - Epoch 31/500, Train Loss: 1.3877, Val Loss: 5.4220
2024-12-28 14:11:08,287 - INFO - Epoch 32/500, Train Loss: 1.3553, Val Loss: 5.3348
2024-12-28 14:11:09,303 - INFO - Epoch 33/500, Train Loss: 1.3519, Val Loss: 5.4414
2024-12-28 14:11:10,359 - INFO - Epoch 34/500, Train Loss: 1.3382, Val Loss: 5.4596
2024-12-28 14:11:11,467 - INFO - Epoch 35/500, Train Loss: 1.3401, Val Loss: 5.4751
2024-12-28 14:11:12,527 - INFO - Epoch 36/500, Train Loss: 1.3173, Val Loss: 5.4874
2024-12-28 14:11:13,662 - INFO - Epoch 37/500, Train Loss: 1.3230, Val Loss: 5.4383
2024-12-28 14:11:13,662 - INFO - Early stopping triggered at epoch 37
2024-12-28 14:11:13,665 - INFO - Training completed in 41.02s
2024-12-28 14:11:13,665 - INFO - Final memory usage: CPU 1727.6 MB, GPU 103.6 MB
2024-12-28 14:11:13,666 - INFO - Model training completed in 41.02s
2024-12-28 14:11:13,728 - INFO - Prediction completed in 0.06s
2024-12-28 14:11:13,739 - INFO - Poison rate 0.07 completed in 50.41s
2024-12-28 14:11:13,739 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:11:13,775 - INFO - Total number of labels flipped: 1975
2024-12-28 14:11:13,776 - INFO - Label flipping completed in 0.04s
2024-12-28 14:11:13,776 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:11:13,776 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:11:15,080 - INFO - Feature scaling completed in 1.30s
2024-12-28 14:11:15,080 - INFO - Starting feature selection (k=50)
2024-12-28 14:11:15,108 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:11:15,108 - INFO - Starting anomaly detection
2024-12-28 14:11:23,072 - INFO - Anomaly detection completed in 7.96s
2024-12-28 14:11:23,072 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:11:23,072 - INFO - Total fit_transform time: 9.30s
2024-12-28 14:11:23,072 - INFO - Training set processing completed in 9.30s
2024-12-28 14:11:23,072 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:11:23,074 - INFO - Memory usage at start_fit: CPU 1689.0 MB, GPU 103.5 MB
2024-12-28 14:11:23,074 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:11:23,081 - INFO - Number of unique classes: 43
2024-12-28 14:11:23,220 - INFO - Fitted scaler and transformed data
2024-12-28 14:11:23,220 - INFO - Scaling time: 0.14s
2024-12-28 14:11:24,193 - INFO - Epoch 1/500, Train Loss: 11.7514, Val Loss: 8.6551
2024-12-28 14:11:25,349 - INFO - Epoch 2/500, Train Loss: 6.8003, Val Loss: 7.7258
2024-12-28 14:11:26,461 - INFO - Epoch 3/500, Train Loss: 5.5845, Val Loss: 7.4370
2024-12-28 14:11:27,626 - INFO - Epoch 4/500, Train Loss: 4.8288, Val Loss: 7.2978
2024-12-28 14:11:28,695 - INFO - Epoch 5/500, Train Loss: 4.3200, Val Loss: 7.0972
2024-12-28 14:11:29,805 - INFO - Epoch 6/500, Train Loss: 3.9210, Val Loss: 7.2524
2024-12-28 14:11:30,885 - INFO - Epoch 7/500, Train Loss: 3.6428, Val Loss: 7.1627
2024-12-28 14:11:31,975 - INFO - Epoch 8/500, Train Loss: 3.4081, Val Loss: 7.1944
2024-12-28 14:11:33,071 - INFO - Epoch 9/500, Train Loss: 3.2013, Val Loss: 6.9772
2024-12-28 14:11:34,112 - INFO - Epoch 10/500, Train Loss: 3.0325, Val Loss: 7.1779
2024-12-28 14:11:35,170 - INFO - Epoch 11/500, Train Loss: 2.9001, Val Loss: 7.0864
2024-12-28 14:11:36,288 - INFO - Epoch 12/500, Train Loss: 2.7834, Val Loss: 7.1674
2024-12-28 14:11:37,383 - INFO - Epoch 13/500, Train Loss: 2.6813, Val Loss: 7.0752
2024-12-28 14:11:38,449 - INFO - Epoch 14/500, Train Loss: 2.6041, Val Loss: 7.0591
2024-12-28 14:11:38,450 - INFO - Early stopping triggered at epoch 14
2024-12-28 14:11:38,450 - INFO - Training completed in 15.38s
2024-12-28 14:11:38,450 - INFO - Final memory usage: CPU 1727.6 MB, GPU 103.6 MB
2024-12-28 14:11:38,451 - INFO - Model training completed in 15.38s
2024-12-28 14:11:38,513 - INFO - Prediction completed in 0.06s
2024-12-28 14:11:38,524 - INFO - Poison rate 0.1 completed in 24.79s
2024-12-28 14:11:38,524 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:11:38,617 - INFO - Total number of labels flipped: 3951
2024-12-28 14:11:38,617 - INFO - Label flipping completed in 0.09s
2024-12-28 14:11:38,618 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:11:38,618 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:11:39,879 - INFO - Feature scaling completed in 1.26s
2024-12-28 14:11:39,880 - INFO - Starting feature selection (k=50)
2024-12-28 14:11:39,908 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:11:39,908 - INFO - Starting anomaly detection
2024-12-28 14:11:47,177 - INFO - Anomaly detection completed in 7.27s
2024-12-28 14:11:47,177 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:11:47,177 - INFO - Total fit_transform time: 8.56s
2024-12-28 14:11:47,177 - INFO - Training set processing completed in 8.56s
2024-12-28 14:11:47,177 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:11:47,178 - INFO - Memory usage at start_fit: CPU 1689.0 MB, GPU 103.5 MB
2024-12-28 14:11:47,179 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:11:47,186 - INFO - Number of unique classes: 43
2024-12-28 14:11:47,326 - INFO - Fitted scaler and transformed data
2024-12-28 14:11:47,326 - INFO - Scaling time: 0.14s
2024-12-28 14:11:48,296 - INFO - Epoch 1/500, Train Loss: 16.2086, Val Loss: 13.9394
2024-12-28 14:11:49,321 - INFO - Epoch 2/500, Train Loss: 10.9929, Val Loss: 13.3276
2024-12-28 14:11:50,364 - INFO - Epoch 3/500, Train Loss: 9.5735, Val Loss: 13.0494
2024-12-28 14:11:51,467 - INFO - Epoch 4/500, Train Loss: 8.5949, Val Loss: 12.8818
2024-12-28 14:11:52,554 - INFO - Epoch 5/500, Train Loss: 7.9523, Val Loss: 12.8988
2024-12-28 14:11:53,586 - INFO - Epoch 6/500, Train Loss: 7.3971, Val Loss: 12.6824
2024-12-28 14:11:54,640 - INFO - Epoch 7/500, Train Loss: 6.9967, Val Loss: 12.8155
2024-12-28 14:11:55,642 - INFO - Epoch 8/500, Train Loss: 6.6812, Val Loss: 12.8349
2024-12-28 14:11:56,803 - INFO - Epoch 9/500, Train Loss: 6.3989, Val Loss: 12.9009
2024-12-28 14:11:57,961 - INFO - Epoch 10/500, Train Loss: 6.1741, Val Loss: 12.8458
2024-12-28 14:11:59,133 - INFO - Epoch 11/500, Train Loss: 5.9276, Val Loss: 12.8143
2024-12-28 14:11:59,133 - INFO - Early stopping triggered at epoch 11
2024-12-28 14:11:59,133 - INFO - Training completed in 11.96s
2024-12-28 14:11:59,133 - INFO - Final memory usage: CPU 1727.6 MB, GPU 103.6 MB
2024-12-28 14:11:59,134 - INFO - Model training completed in 11.96s
2024-12-28 14:11:59,197 - INFO - Prediction completed in 0.06s
2024-12-28 14:11:59,208 - INFO - Poison rate 0.2 completed in 20.68s
2024-12-28 14:11:59,209 - INFO - Loaded 7 existing results
2024-12-28 14:11:59,209 - INFO - Total results to save: 14
2024-12-28 14:11:59,209 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:11:59,211 - INFO - Saved 14 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:11:59,211 - INFO - Total evaluation time: 276.21s
2024-12-28 14:11:59,216 - INFO - 
Progress: 3.1% - Evaluating GTSRB with LogisticRegression (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:11:59,446 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:11:59,446 - INFO - Dataset type: image
2024-12-28 14:11:59,446 - INFO - Sample size: 39209
2024-12-28 14:11:59,446 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:11:59,447 - INFO - Loading datasets...
2024-12-28 14:12:18,389 - INFO - Dataset loading completed in 18.94s
2024-12-28 14:12:18,389 - INFO - Extracting validation features...
2024-12-28 14:12:18,389 - INFO - Extracting features from 4435 samples...
2024-12-28 14:12:19,156 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:12:19,161 - INFO - Validation feature extraction completed in 0.77s
2024-12-28 14:12:19,161 - INFO - Extracting training features...
2024-12-28 14:12:19,161 - INFO - Extracting features from 19755 samples...
2024-12-28 14:12:21,820 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:12:21,833 - INFO - Training feature extraction completed in 2.67s
2024-12-28 14:12:21,834 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 14:12:21,834 - INFO - Using device: cuda
2024-12-28 14:12:21,834 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:12:21,834 - INFO - Training set processing completed in 0.00s
2024-12-28 14:12:21,834 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:12:21,836 - INFO - Memory usage at start_fit: CPU 1738.0 MB, GPU 103.1 MB
2024-12-28 14:12:21,836 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:12:21,845 - INFO - Number of unique classes: 43
2024-12-28 14:12:21,989 - INFO - Fitted scaler and transformed data
2024-12-28 14:12:21,989 - INFO - Scaling time: 0.14s
2024-12-28 14:12:22,475 - INFO - Epoch 1/1000, Train Loss: 1.8916, Val Loss: 1.4043
2024-12-28 14:12:22,870 - INFO - Epoch 2/1000, Train Loss: 1.1975, Val Loss: 1.1435
2024-12-28 14:12:23,311 - INFO - Epoch 3/1000, Train Loss: 1.0146, Val Loss: 1.0177
2024-12-28 14:12:23,707 - INFO - Epoch 4/1000, Train Loss: 0.9178, Val Loss: 0.9545
2024-12-28 14:12:24,115 - INFO - Epoch 5/1000, Train Loss: 0.8623, Val Loss: 0.9054
2024-12-28 14:12:24,575 - INFO - Epoch 6/1000, Train Loss: 0.8237, Val Loss: 0.8813
2024-12-28 14:12:25,012 - INFO - Epoch 7/1000, Train Loss: 0.7966, Val Loss: 0.8582
2024-12-28 14:12:25,449 - INFO - Epoch 8/1000, Train Loss: 0.7782, Val Loss: 0.8462
2024-12-28 14:12:25,887 - INFO - Epoch 9/1000, Train Loss: 0.7650, Val Loss: 0.8349
2024-12-28 14:12:26,306 - INFO - Epoch 10/1000, Train Loss: 0.7536, Val Loss: 0.8234
2024-12-28 14:12:26,744 - INFO - Epoch 11/1000, Train Loss: 0.7464, Val Loss: 0.8210
2024-12-28 14:12:27,159 - INFO - Epoch 12/1000, Train Loss: 0.7405, Val Loss: 0.8163
2024-12-28 14:12:27,616 - INFO - Epoch 13/1000, Train Loss: 0.7354, Val Loss: 0.8129
2024-12-28 14:12:28,064 - INFO - Epoch 14/1000, Train Loss: 0.7316, Val Loss: 0.8125
2024-12-28 14:12:28,501 - INFO - Epoch 15/1000, Train Loss: 0.7277, Val Loss: 0.8027
2024-12-28 14:12:28,922 - INFO - Epoch 16/1000, Train Loss: 0.7268, Val Loss: 0.7998
2024-12-28 14:12:29,321 - INFO - Epoch 17/1000, Train Loss: 0.7238, Val Loss: 0.7914
2024-12-28 14:12:29,823 - INFO - Epoch 18/1000, Train Loss: 0.7224, Val Loss: 0.8061
2024-12-28 14:12:30,415 - INFO - Epoch 19/1000, Train Loss: 0.7199, Val Loss: 0.7972
2024-12-28 14:12:31,027 - INFO - Epoch 20/1000, Train Loss: 0.7195, Val Loss: 0.8066
2024-12-28 14:12:31,596 - INFO - Epoch 21/1000, Train Loss: 0.7193, Val Loss: 0.8017
2024-12-28 14:12:32,081 - INFO - Epoch 22/1000, Train Loss: 0.7180, Val Loss: 0.7950
2024-12-28 14:12:32,081 - INFO - Early stopping triggered at epoch 22
2024-12-28 14:12:32,081 - INFO - Training completed in 10.25s
2024-12-28 14:12:32,082 - INFO - Final memory usage: CPU 1779.6 MB, GPU 103.6 MB
2024-12-28 14:12:32,083 - INFO - Model training completed in 10.25s
2024-12-28 14:12:32,160 - INFO - Prediction completed in 0.08s
2024-12-28 14:12:32,172 - INFO - Poison rate 0.0 completed in 10.34s
2024-12-28 14:12:32,172 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:12:32,177 - INFO - Total number of labels flipped: 197
2024-12-28 14:12:32,177 - INFO - Label flipping completed in 0.01s
2024-12-28 14:12:32,177 - INFO - Training set processing completed in 0.00s
2024-12-28 14:12:32,177 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:12:32,178 - INFO - Memory usage at start_fit: CPU 1741.0 MB, GPU 103.5 MB
2024-12-28 14:12:32,178 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:12:32,182 - INFO - Number of unique classes: 43
2024-12-28 14:12:32,334 - INFO - Fitted scaler and transformed data
2024-12-28 14:12:32,334 - INFO - Scaling time: 0.15s
2024-12-28 14:12:32,787 - INFO - Epoch 1/1000, Train Loss: 1.9430, Val Loss: 1.4396
2024-12-28 14:12:33,249 - INFO - Epoch 2/1000, Train Loss: 1.2594, Val Loss: 1.1950
2024-12-28 14:12:33,664 - INFO - Epoch 3/1000, Train Loss: 1.0794, Val Loss: 1.0780
2024-12-28 14:12:34,096 - INFO - Epoch 4/1000, Train Loss: 0.9868, Val Loss: 1.0114
2024-12-28 14:12:34,563 - INFO - Epoch 5/1000, Train Loss: 0.9296, Val Loss: 0.9762
2024-12-28 14:12:34,989 - INFO - Epoch 6/1000, Train Loss: 0.8937, Val Loss: 0.9527
2024-12-28 14:12:35,406 - INFO - Epoch 7/1000, Train Loss: 0.8673, Val Loss: 0.9347
2024-12-28 14:12:35,879 - INFO - Epoch 8/1000, Train Loss: 0.8488, Val Loss: 0.9153
2024-12-28 14:12:36,315 - INFO - Epoch 9/1000, Train Loss: 0.8340, Val Loss: 0.9161
2024-12-28 14:12:36,754 - INFO - Epoch 10/1000, Train Loss: 0.8256, Val Loss: 0.9024
2024-12-28 14:12:37,187 - INFO - Epoch 11/1000, Train Loss: 0.8161, Val Loss: 0.8946
2024-12-28 14:12:37,652 - INFO - Epoch 12/1000, Train Loss: 0.8123, Val Loss: 0.8897
2024-12-28 14:12:38,069 - INFO - Epoch 13/1000, Train Loss: 0.8040, Val Loss: 0.8886
2024-12-28 14:12:38,500 - INFO - Epoch 14/1000, Train Loss: 0.8018, Val Loss: 0.8854
2024-12-28 14:12:38,906 - INFO - Epoch 15/1000, Train Loss: 0.7971, Val Loss: 0.8893
2024-12-28 14:12:39,329 - INFO - Epoch 16/1000, Train Loss: 0.7936, Val Loss: 0.8764
2024-12-28 14:12:39,775 - INFO - Epoch 17/1000, Train Loss: 0.7931, Val Loss: 0.8733
2024-12-28 14:12:40,216 - INFO - Epoch 18/1000, Train Loss: 0.7912, Val Loss: 0.8829
2024-12-28 14:12:40,638 - INFO - Epoch 19/1000, Train Loss: 0.7910, Val Loss: 0.8762
2024-12-28 14:12:41,115 - INFO - Epoch 20/1000, Train Loss: 0.7896, Val Loss: 0.8812
2024-12-28 14:12:41,580 - INFO - Epoch 21/1000, Train Loss: 0.7907, Val Loss: 0.8756
2024-12-28 14:12:42,073 - INFO - Epoch 22/1000, Train Loss: 0.7891, Val Loss: 0.8730
2024-12-28 14:12:42,073 - INFO - Early stopping triggered at epoch 22
2024-12-28 14:12:42,074 - INFO - Training completed in 9.90s
2024-12-28 14:12:42,074 - INFO - Final memory usage: CPU 1789.3 MB, GPU 103.6 MB
2024-12-28 14:12:42,075 - INFO - Model training completed in 9.90s
2024-12-28 14:12:42,147 - INFO - Prediction completed in 0.07s
2024-12-28 14:12:42,158 - INFO - Poison rate 0.01 completed in 9.99s
2024-12-28 14:12:42,158 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:12:42,169 - INFO - Total number of labels flipped: 592
2024-12-28 14:12:42,170 - INFO - Label flipping completed in 0.01s
2024-12-28 14:12:42,170 - INFO - Training set processing completed in 0.00s
2024-12-28 14:12:42,170 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:12:42,171 - INFO - Memory usage at start_fit: CPU 1750.7 MB, GPU 103.5 MB
2024-12-28 14:12:42,171 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:12:42,175 - INFO - Number of unique classes: 43
2024-12-28 14:12:42,321 - INFO - Fitted scaler and transformed data
2024-12-28 14:12:42,322 - INFO - Scaling time: 0.15s
2024-12-28 14:12:42,800 - INFO - Epoch 1/1000, Train Loss: 2.0318, Val Loss: 1.5643
2024-12-28 14:12:43,261 - INFO - Epoch 2/1000, Train Loss: 1.3828, Val Loss: 1.3313
2024-12-28 14:12:43,747 - INFO - Epoch 3/1000, Train Loss: 1.2134, Val Loss: 1.2205
2024-12-28 14:12:44,224 - INFO - Epoch 4/1000, Train Loss: 1.1254, Val Loss: 1.1633
2024-12-28 14:12:44,894 - INFO - Epoch 5/1000, Train Loss: 1.0676, Val Loss: 1.1320
2024-12-28 14:12:45,597 - INFO - Epoch 6/1000, Train Loss: 1.0305, Val Loss: 1.0998
2024-12-28 14:12:46,479 - INFO - Epoch 7/1000, Train Loss: 1.0046, Val Loss: 1.0883
2024-12-28 14:12:47,121 - INFO - Epoch 8/1000, Train Loss: 0.9864, Val Loss: 1.0746
2024-12-28 14:12:47,591 - INFO - Epoch 9/1000, Train Loss: 0.9722, Val Loss: 1.0583
2024-12-28 14:12:48,314 - INFO - Epoch 10/1000, Train Loss: 0.9603, Val Loss: 1.0631
2024-12-28 14:12:49,188 - INFO - Epoch 11/1000, Train Loss: 0.9530, Val Loss: 1.0483
2024-12-28 14:12:49,890 - INFO - Epoch 12/1000, Train Loss: 0.9460, Val Loss: 1.0399
2024-12-28 14:12:50,465 - INFO - Epoch 13/1000, Train Loss: 0.9405, Val Loss: 1.0403
2024-12-28 14:12:50,882 - INFO - Epoch 14/1000, Train Loss: 0.9363, Val Loss: 1.0387
2024-12-28 14:12:51,294 - INFO - Epoch 15/1000, Train Loss: 0.9354, Val Loss: 1.0402
2024-12-28 14:12:51,728 - INFO - Epoch 16/1000, Train Loss: 0.9317, Val Loss: 1.0272
2024-12-28 14:12:52,184 - INFO - Epoch 17/1000, Train Loss: 0.9295, Val Loss: 1.0326
2024-12-28 14:12:52,628 - INFO - Epoch 18/1000, Train Loss: 0.9284, Val Loss: 1.0290
2024-12-28 14:12:53,061 - INFO - Epoch 19/1000, Train Loss: 0.9267, Val Loss: 1.0293
2024-12-28 14:12:53,513 - INFO - Epoch 20/1000, Train Loss: 0.9266, Val Loss: 1.0322
2024-12-28 14:12:54,052 - INFO - Epoch 21/1000, Train Loss: 0.9224, Val Loss: 1.0167
2024-12-28 14:12:54,474 - INFO - Epoch 22/1000, Train Loss: 0.9243, Val Loss: 1.0306
2024-12-28 14:12:54,920 - INFO - Epoch 23/1000, Train Loss: 0.9241, Val Loss: 1.0310
2024-12-28 14:12:55,361 - INFO - Epoch 24/1000, Train Loss: 0.9229, Val Loss: 1.0277
2024-12-28 14:12:55,778 - INFO - Epoch 25/1000, Train Loss: 0.9216, Val Loss: 1.0212
2024-12-28 14:12:56,201 - INFO - Epoch 26/1000, Train Loss: 0.9203, Val Loss: 1.0218
2024-12-28 14:12:56,201 - INFO - Early stopping triggered at epoch 26
2024-12-28 14:12:56,201 - INFO - Training completed in 14.03s
2024-12-28 14:12:56,202 - INFO - Final memory usage: CPU 1789.3 MB, GPU 103.6 MB
2024-12-28 14:12:56,203 - INFO - Model training completed in 14.03s
2024-12-28 14:12:56,275 - INFO - Prediction completed in 0.07s
2024-12-28 14:12:56,286 - INFO - Poison rate 0.03 completed in 14.13s
2024-12-28 14:12:56,286 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:12:56,305 - INFO - Total number of labels flipped: 987
2024-12-28 14:12:56,306 - INFO - Label flipping completed in 0.02s
2024-12-28 14:12:56,306 - INFO - Training set processing completed in 0.00s
2024-12-28 14:12:56,306 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:12:56,307 - INFO - Memory usage at start_fit: CPU 1750.7 MB, GPU 103.5 MB
2024-12-28 14:12:56,307 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:12:56,313 - INFO - Number of unique classes: 43
2024-12-28 14:12:56,464 - INFO - Fitted scaler and transformed data
2024-12-28 14:12:56,465 - INFO - Scaling time: 0.15s
2024-12-28 14:12:56,897 - INFO - Epoch 1/1000, Train Loss: 2.1149, Val Loss: 1.6536
2024-12-28 14:12:57,343 - INFO - Epoch 2/1000, Train Loss: 1.4923, Val Loss: 1.4425
2024-12-28 14:12:57,770 - INFO - Epoch 3/1000, Train Loss: 1.3261, Val Loss: 1.3469
2024-12-28 14:12:58,209 - INFO - Epoch 4/1000, Train Loss: 1.2393, Val Loss: 1.2932
2024-12-28 14:12:58,751 - INFO - Epoch 5/1000, Train Loss: 1.1838, Val Loss: 1.2526
2024-12-28 14:12:59,174 - INFO - Epoch 6/1000, Train Loss: 1.1472, Val Loss: 1.2389
2024-12-28 14:12:59,599 - INFO - Epoch 7/1000, Train Loss: 1.1204, Val Loss: 1.2211
2024-12-28 14:13:00,073 - INFO - Epoch 8/1000, Train Loss: 1.1022, Val Loss: 1.2049
2024-12-28 14:13:00,525 - INFO - Epoch 9/1000, Train Loss: 1.0859, Val Loss: 1.1955
2024-12-28 14:13:01,025 - INFO - Epoch 10/1000, Train Loss: 1.0757, Val Loss: 1.1825
2024-12-28 14:13:01,477 - INFO - Epoch 11/1000, Train Loss: 1.0677, Val Loss: 1.1919
2024-12-28 14:13:01,926 - INFO - Epoch 12/1000, Train Loss: 1.0616, Val Loss: 1.1724
2024-12-28 14:13:02,369 - INFO - Epoch 13/1000, Train Loss: 1.0544, Val Loss: 1.1744
2024-12-28 14:13:02,799 - INFO - Epoch 14/1000, Train Loss: 1.0515, Val Loss: 1.1757
2024-12-28 14:13:03,236 - INFO - Epoch 15/1000, Train Loss: 1.0492, Val Loss: 1.1781
2024-12-28 14:13:03,726 - INFO - Epoch 16/1000, Train Loss: 1.0451, Val Loss: 1.1754
2024-12-28 14:13:04,180 - INFO - Epoch 17/1000, Train Loss: 1.0424, Val Loss: 1.1649
2024-12-28 14:13:04,612 - INFO - Epoch 18/1000, Train Loss: 1.0415, Val Loss: 1.1686
2024-12-28 14:13:05,088 - INFO - Epoch 19/1000, Train Loss: 1.0412, Val Loss: 1.1704
2024-12-28 14:13:05,556 - INFO - Epoch 20/1000, Train Loss: 1.0374, Val Loss: 1.1636
2024-12-28 14:13:06,022 - INFO - Epoch 21/1000, Train Loss: 1.0372, Val Loss: 1.1719
2024-12-28 14:13:06,477 - INFO - Epoch 22/1000, Train Loss: 1.0387, Val Loss: 1.1625
2024-12-28 14:13:06,929 - INFO - Epoch 23/1000, Train Loss: 1.0359, Val Loss: 1.1707
2024-12-28 14:13:07,424 - INFO - Epoch 24/1000, Train Loss: 1.0347, Val Loss: 1.1622
2024-12-28 14:13:07,901 - INFO - Epoch 25/1000, Train Loss: 1.0354, Val Loss: 1.1635
2024-12-28 14:13:08,321 - INFO - Epoch 26/1000, Train Loss: 1.0346, Val Loss: 1.1682
2024-12-28 14:13:08,761 - INFO - Epoch 27/1000, Train Loss: 1.0348, Val Loss: 1.1621
2024-12-28 14:13:08,761 - INFO - Early stopping triggered at epoch 27
2024-12-28 14:13:08,761 - INFO - Training completed in 12.45s
2024-12-28 14:13:08,761 - INFO - Final memory usage: CPU 1790.0 MB, GPU 103.6 MB
2024-12-28 14:13:08,762 - INFO - Model training completed in 12.46s
2024-12-28 14:13:08,850 - INFO - Prediction completed in 0.09s
2024-12-28 14:13:08,861 - INFO - Poison rate 0.05 completed in 12.57s
2024-12-28 14:13:08,861 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:13:08,897 - INFO - Total number of labels flipped: 1382
2024-12-28 14:13:08,898 - INFO - Label flipping completed in 0.04s
2024-12-28 14:13:08,898 - INFO - Training set processing completed in 0.00s
2024-12-28 14:13:08,898 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:13:08,899 - INFO - Memory usage at start_fit: CPU 1751.4 MB, GPU 103.5 MB
2024-12-28 14:13:08,899 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:13:08,904 - INFO - Number of unique classes: 43
2024-12-28 14:13:09,042 - INFO - Fitted scaler and transformed data
2024-12-28 14:13:09,043 - INFO - Scaling time: 0.14s
2024-12-28 14:13:09,551 - INFO - Epoch 1/1000, Train Loss: 2.1743, Val Loss: 1.7947
2024-12-28 14:13:10,014 - INFO - Epoch 2/1000, Train Loss: 1.5898, Val Loss: 1.5853
2024-12-28 14:13:10,462 - INFO - Epoch 3/1000, Train Loss: 1.4300, Val Loss: 1.5047
2024-12-28 14:13:10,971 - INFO - Epoch 4/1000, Train Loss: 1.3474, Val Loss: 1.4533
2024-12-28 14:13:11,404 - INFO - Epoch 5/1000, Train Loss: 1.2923, Val Loss: 1.4207
2024-12-28 14:13:11,843 - INFO - Epoch 6/1000, Train Loss: 1.2554, Val Loss: 1.3956
2024-12-28 14:13:12,287 - INFO - Epoch 7/1000, Train Loss: 1.2282, Val Loss: 1.3812
2024-12-28 14:13:12,755 - INFO - Epoch 8/1000, Train Loss: 1.2107, Val Loss: 1.3646
2024-12-28 14:13:13,249 - INFO - Epoch 9/1000, Train Loss: 1.1968, Val Loss: 1.3595
2024-12-28 14:13:13,698 - INFO - Epoch 10/1000, Train Loss: 1.1847, Val Loss: 1.3482
2024-12-28 14:13:14,196 - INFO - Epoch 11/1000, Train Loss: 1.1753, Val Loss: 1.3476
2024-12-28 14:13:14,652 - INFO - Epoch 12/1000, Train Loss: 1.1703, Val Loss: 1.3439
2024-12-28 14:13:15,090 - INFO - Epoch 13/1000, Train Loss: 1.1641, Val Loss: 1.3293
2024-12-28 14:13:15,525 - INFO - Epoch 14/1000, Train Loss: 1.1602, Val Loss: 1.3267
2024-12-28 14:13:15,963 - INFO - Epoch 15/1000, Train Loss: 1.1568, Val Loss: 1.3255
2024-12-28 14:13:16,397 - INFO - Epoch 16/1000, Train Loss: 1.1537, Val Loss: 1.3316
2024-12-28 14:13:16,844 - INFO - Epoch 17/1000, Train Loss: 1.1522, Val Loss: 1.3311
2024-12-28 14:13:17,299 - INFO - Epoch 18/1000, Train Loss: 1.1488, Val Loss: 1.3240
2024-12-28 14:13:17,737 - INFO - Epoch 19/1000, Train Loss: 1.1479, Val Loss: 1.3157
2024-12-28 14:13:18,163 - INFO - Epoch 20/1000, Train Loss: 1.1473, Val Loss: 1.3178
2024-12-28 14:13:18,606 - INFO - Epoch 21/1000, Train Loss: 1.1459, Val Loss: 1.3161
2024-12-28 14:13:19,033 - INFO - Epoch 22/1000, Train Loss: 1.1457, Val Loss: 1.3139
2024-12-28 14:13:19,474 - INFO - Epoch 23/1000, Train Loss: 1.1434, Val Loss: 1.3150
2024-12-28 14:13:19,958 - INFO - Epoch 24/1000, Train Loss: 1.1431, Val Loss: 1.3093
2024-12-28 14:13:20,432 - INFO - Epoch 25/1000, Train Loss: 1.1417, Val Loss: 1.3198
2024-12-28 14:13:20,898 - INFO - Epoch 26/1000, Train Loss: 1.1436, Val Loss: 1.3180
2024-12-28 14:13:21,379 - INFO - Epoch 27/1000, Train Loss: 1.1410, Val Loss: 1.3145
2024-12-28 14:13:21,835 - INFO - Epoch 28/1000, Train Loss: 1.1416, Val Loss: 1.3090
2024-12-28 14:13:22,270 - INFO - Epoch 29/1000, Train Loss: 1.1413, Val Loss: 1.3200
2024-12-28 14:13:22,271 - INFO - Early stopping triggered at epoch 29
2024-12-28 14:13:22,271 - INFO - Training completed in 13.37s
2024-12-28 14:13:22,271 - INFO - Final memory usage: CPU 1790.0 MB, GPU 103.6 MB
2024-12-28 14:13:22,272 - INFO - Model training completed in 13.37s
2024-12-28 14:13:22,359 - INFO - Prediction completed in 0.09s
2024-12-28 14:13:22,370 - INFO - Poison rate 0.07 completed in 13.51s
2024-12-28 14:13:22,370 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:13:22,405 - INFO - Total number of labels flipped: 1975
2024-12-28 14:13:22,405 - INFO - Label flipping completed in 0.04s
2024-12-28 14:13:22,405 - INFO - Training set processing completed in 0.00s
2024-12-28 14:13:22,405 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:13:22,406 - INFO - Memory usage at start_fit: CPU 1751.4 MB, GPU 103.5 MB
2024-12-28 14:13:22,406 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:13:22,411 - INFO - Number of unique classes: 43
2024-12-28 14:13:22,545 - INFO - Fitted scaler and transformed data
2024-12-28 14:13:22,546 - INFO - Scaling time: 0.13s
2024-12-28 14:13:23,118 - INFO - Epoch 1/1000, Train Loss: 2.2892, Val Loss: 1.8623
2024-12-28 14:13:23,597 - INFO - Epoch 2/1000, Train Loss: 1.7470, Val Loss: 1.6900
2024-12-28 14:13:24,060 - INFO - Epoch 3/1000, Train Loss: 1.5940, Val Loss: 1.6084
2024-12-28 14:13:24,527 - INFO - Epoch 4/1000, Train Loss: 1.5130, Val Loss: 1.5589
2024-12-28 14:13:24,992 - INFO - Epoch 5/1000, Train Loss: 1.4582, Val Loss: 1.5216
2024-12-28 14:13:25,473 - INFO - Epoch 6/1000, Train Loss: 1.4227, Val Loss: 1.5116
2024-12-28 14:13:25,928 - INFO - Epoch 7/1000, Train Loss: 1.3957, Val Loss: 1.5040
2024-12-28 14:13:26,421 - INFO - Epoch 8/1000, Train Loss: 1.3765, Val Loss: 1.4838
2024-12-28 14:13:26,892 - INFO - Epoch 9/1000, Train Loss: 1.3617, Val Loss: 1.4750
2024-12-28 14:13:27,358 - INFO - Epoch 10/1000, Train Loss: 1.3515, Val Loss: 1.4677
2024-12-28 14:13:27,792 - INFO - Epoch 11/1000, Train Loss: 1.3412, Val Loss: 1.4614
2024-12-28 14:13:28,257 - INFO - Epoch 12/1000, Train Loss: 1.3333, Val Loss: 1.4680
2024-12-28 14:13:28,708 - INFO - Epoch 13/1000, Train Loss: 1.3289, Val Loss: 1.4599
2024-12-28 14:13:29,116 - INFO - Epoch 14/1000, Train Loss: 1.3218, Val Loss: 1.4499
2024-12-28 14:13:29,613 - INFO - Epoch 15/1000, Train Loss: 1.3187, Val Loss: 1.4497
2024-12-28 14:13:30,053 - INFO - Epoch 16/1000, Train Loss: 1.3161, Val Loss: 1.4511
2024-12-28 14:13:30,508 - INFO - Epoch 17/1000, Train Loss: 1.3160, Val Loss: 1.4528
2024-12-28 14:13:30,987 - INFO - Epoch 18/1000, Train Loss: 1.3136, Val Loss: 1.4531
2024-12-28 14:13:31,449 - INFO - Epoch 19/1000, Train Loss: 1.3128, Val Loss: 1.4609
2024-12-28 14:13:31,450 - INFO - Early stopping triggered at epoch 19
2024-12-28 14:13:31,450 - INFO - Training completed in 9.04s
2024-12-28 14:13:31,450 - INFO - Final memory usage: CPU 1790.0 MB, GPU 103.6 MB
2024-12-28 14:13:31,451 - INFO - Model training completed in 9.05s
2024-12-28 14:13:31,523 - INFO - Prediction completed in 0.07s
2024-12-28 14:13:31,533 - INFO - Poison rate 0.1 completed in 9.16s
2024-12-28 14:13:31,534 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:13:31,603 - INFO - Total number of labels flipped: 3951
2024-12-28 14:13:31,603 - INFO - Label flipping completed in 0.07s
2024-12-28 14:13:31,603 - INFO - Training set processing completed in 0.00s
2024-12-28 14:13:31,603 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:13:31,604 - INFO - Memory usage at start_fit: CPU 1751.4 MB, GPU 103.5 MB
2024-12-28 14:13:31,604 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:13:31,608 - INFO - Number of unique classes: 43
2024-12-28 14:13:31,758 - INFO - Fitted scaler and transformed data
2024-12-28 14:13:31,759 - INFO - Scaling time: 0.15s
2024-12-28 14:13:32,239 - INFO - Epoch 1/1000, Train Loss: 2.6344, Val Loss: 2.3513
2024-12-28 14:13:32,723 - INFO - Epoch 2/1000, Train Loss: 2.1762, Val Loss: 2.1979
2024-12-28 14:13:33,214 - INFO - Epoch 3/1000, Train Loss: 2.0447, Val Loss: 2.1423
2024-12-28 14:13:33,692 - INFO - Epoch 4/1000, Train Loss: 1.9698, Val Loss: 2.0987
2024-12-28 14:13:34,176 - INFO - Epoch 5/1000, Train Loss: 1.9196, Val Loss: 2.0832
2024-12-28 14:13:34,660 - INFO - Epoch 6/1000, Train Loss: 1.8869, Val Loss: 2.0450
2024-12-28 14:13:35,097 - INFO - Epoch 7/1000, Train Loss: 1.8608, Val Loss: 2.0343
2024-12-28 14:13:35,568 - INFO - Epoch 8/1000, Train Loss: 1.8409, Val Loss: 2.0215
2024-12-28 14:13:36,034 - INFO - Epoch 9/1000, Train Loss: 1.8275, Val Loss: 2.0212
2024-12-28 14:13:36,513 - INFO - Epoch 10/1000, Train Loss: 1.8139, Val Loss: 2.0159
2024-12-28 14:13:36,999 - INFO - Epoch 11/1000, Train Loss: 1.8073, Val Loss: 2.0062
2024-12-28 14:13:37,473 - INFO - Epoch 12/1000, Train Loss: 1.7999, Val Loss: 2.0148
2024-12-28 14:13:37,990 - INFO - Epoch 13/1000, Train Loss: 1.7938, Val Loss: 1.9966
2024-12-28 14:13:38,446 - INFO - Epoch 14/1000, Train Loss: 1.7878, Val Loss: 1.9997
2024-12-28 14:13:38,906 - INFO - Epoch 15/1000, Train Loss: 1.7841, Val Loss: 1.9934
2024-12-28 14:13:39,372 - INFO - Epoch 16/1000, Train Loss: 1.7813, Val Loss: 1.9922
2024-12-28 14:13:39,837 - INFO - Epoch 17/1000, Train Loss: 1.7795, Val Loss: 1.9986
2024-12-28 14:13:40,316 - INFO - Epoch 18/1000, Train Loss: 1.7758, Val Loss: 1.9850
2024-12-28 14:13:40,778 - INFO - Epoch 19/1000, Train Loss: 1.7756, Val Loss: 1.9861
2024-12-28 14:13:41,266 - INFO - Epoch 20/1000, Train Loss: 1.7732, Val Loss: 1.9908
2024-12-28 14:13:41,746 - INFO - Epoch 21/1000, Train Loss: 1.7708, Val Loss: 1.9894
2024-12-28 14:13:42,262 - INFO - Epoch 22/1000, Train Loss: 1.7731, Val Loss: 1.9836
2024-12-28 14:13:42,725 - INFO - Epoch 23/1000, Train Loss: 1.7697, Val Loss: 1.9968
2024-12-28 14:13:43,224 - INFO - Epoch 24/1000, Train Loss: 1.7706, Val Loss: 1.9913
2024-12-28 14:13:43,667 - INFO - Epoch 25/1000, Train Loss: 1.7675, Val Loss: 1.9933
2024-12-28 14:13:44,092 - INFO - Epoch 26/1000, Train Loss: 1.7674, Val Loss: 1.9820
2024-12-28 14:13:44,573 - INFO - Epoch 27/1000, Train Loss: 1.7682, Val Loss: 1.9891
2024-12-28 14:13:45,274 - INFO - Epoch 28/1000, Train Loss: 1.7673, Val Loss: 1.9888
2024-12-28 14:13:45,951 - INFO - Epoch 29/1000, Train Loss: 1.7648, Val Loss: 1.9851
2024-12-28 14:13:46,712 - INFO - Epoch 30/1000, Train Loss: 1.7652, Val Loss: 1.9886
2024-12-28 14:13:47,554 - INFO - Epoch 31/1000, Train Loss: 1.7651, Val Loss: 1.9867
2024-12-28 14:13:47,554 - INFO - Early stopping triggered at epoch 31
2024-12-28 14:13:47,554 - INFO - Training completed in 15.95s
2024-12-28 14:13:47,554 - INFO - Final memory usage: CPU 1790.0 MB, GPU 103.6 MB
2024-12-28 14:13:47,555 - INFO - Model training completed in 15.95s
2024-12-28 14:13:47,642 - INFO - Prediction completed in 0.09s
2024-12-28 14:13:47,653 - INFO - Poison rate 0.2 completed in 16.12s
2024-12-28 14:13:47,654 - INFO - Loaded 14 existing results
2024-12-28 14:13:47,654 - INFO - Total results to save: 21
2024-12-28 14:13:47,654 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:13:47,656 - INFO - Saved 21 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:13:47,656 - INFO - Total evaluation time: 108.21s
2024-12-28 14:13:47,662 - INFO - 
Progress: 4.2% - Evaluating GTSRB with LogisticRegression (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:13:47,837 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:13:47,837 - INFO - Dataset type: image
2024-12-28 14:13:47,837 - INFO - Sample size: 39209
2024-12-28 14:13:47,838 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:13:47,839 - INFO - Loading datasets...
2024-12-28 14:14:06,283 - INFO - Dataset loading completed in 18.44s
2024-12-28 14:14:06,283 - INFO - Extracting validation features...
2024-12-28 14:14:06,283 - INFO - Extracting features from 4435 samples...
2024-12-28 14:14:07,002 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:14:07,006 - INFO - Validation feature extraction completed in 0.72s
2024-12-28 14:14:07,006 - INFO - Extracting training features...
2024-12-28 14:14:07,006 - INFO - Extracting features from 19755 samples...
2024-12-28 14:14:09,863 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:14:09,873 - INFO - Training feature extraction completed in 2.87s
2024-12-28 14:14:09,874 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 14:14:09,874 - INFO - Using device: cuda
2024-12-28 14:14:09,875 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:14:09,875 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:14:09,875 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:14:11,120 - INFO - Feature scaling completed in 1.25s
2024-12-28 14:14:11,121 - INFO - Starting feature selection (k=50)
2024-12-28 14:14:11,150 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:14:11,151 - INFO - Starting anomaly detection
2024-12-28 14:14:18,874 - INFO - Anomaly detection completed in 7.72s
2024-12-28 14:14:18,874 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:14:18,875 - INFO - Total fit_transform time: 9.00s
2024-12-28 14:14:18,875 - INFO - Training set processing completed in 9.00s
2024-12-28 14:14:18,875 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:14:18,877 - INFO - Memory usage at start_fit: CPU 1779.8 MB, GPU 103.1 MB
2024-12-28 14:14:18,877 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:14:18,885 - INFO - Number of unique classes: 43
2024-12-28 14:14:19,043 - INFO - Fitted scaler and transformed data
2024-12-28 14:14:19,044 - INFO - Scaling time: 0.16s
2024-12-28 14:14:19,480 - INFO - Epoch 1/1000, Train Loss: 1.9107, Val Loss: 1.3791
2024-12-28 14:14:19,918 - INFO - Epoch 2/1000, Train Loss: 1.2050, Val Loss: 1.1235
2024-12-28 14:14:20,354 - INFO - Epoch 3/1000, Train Loss: 1.0172, Val Loss: 1.0112
2024-12-28 14:14:20,778 - INFO - Epoch 4/1000, Train Loss: 0.9228, Val Loss: 0.9370
2024-12-28 14:14:21,220 - INFO - Epoch 5/1000, Train Loss: 0.8634, Val Loss: 0.8991
2024-12-28 14:14:21,663 - INFO - Epoch 6/1000, Train Loss: 0.8261, Val Loss: 0.8714
2024-12-28 14:14:22,212 - INFO - Epoch 7/1000, Train Loss: 0.7993, Val Loss: 0.8418
2024-12-28 14:14:22,682 - INFO - Epoch 8/1000, Train Loss: 0.7812, Val Loss: 0.8294
2024-12-28 14:14:23,140 - INFO - Epoch 9/1000, Train Loss: 0.7631, Val Loss: 0.8216
2024-12-28 14:14:23,628 - INFO - Epoch 10/1000, Train Loss: 0.7558, Val Loss: 0.8081
2024-12-28 14:14:24,078 - INFO - Epoch 11/1000, Train Loss: 0.7482, Val Loss: 0.8167
2024-12-28 14:14:24,571 - INFO - Epoch 12/1000, Train Loss: 0.7403, Val Loss: 0.8014
2024-12-28 14:14:25,022 - INFO - Epoch 13/1000, Train Loss: 0.7356, Val Loss: 0.7939
2024-12-28 14:14:25,575 - INFO - Epoch 14/1000, Train Loss: 0.7312, Val Loss: 0.7958
2024-12-28 14:14:26,059 - INFO - Epoch 15/1000, Train Loss: 0.7301, Val Loss: 0.7935
2024-12-28 14:14:26,490 - INFO - Epoch 16/1000, Train Loss: 0.7261, Val Loss: 0.7879
2024-12-28 14:14:26,921 - INFO - Epoch 17/1000, Train Loss: 0.7244, Val Loss: 0.7852
2024-12-28 14:14:27,353 - INFO - Epoch 18/1000, Train Loss: 0.7225, Val Loss: 0.7820
2024-12-28 14:14:27,808 - INFO - Epoch 19/1000, Train Loss: 0.7227, Val Loss: 0.7787
2024-12-28 14:14:28,262 - INFO - Epoch 20/1000, Train Loss: 0.7207, Val Loss: 0.7769
2024-12-28 14:14:28,764 - INFO - Epoch 21/1000, Train Loss: 0.7191, Val Loss: 0.7899
2024-12-28 14:14:29,234 - INFO - Epoch 22/1000, Train Loss: 0.7186, Val Loss: 0.7800
2024-12-28 14:14:29,682 - INFO - Epoch 23/1000, Train Loss: 0.7176, Val Loss: 0.7844
2024-12-28 14:14:30,121 - INFO - Epoch 24/1000, Train Loss: 0.7179, Val Loss: 0.7823
2024-12-28 14:14:30,601 - INFO - Epoch 25/1000, Train Loss: 0.7178, Val Loss: 0.7801
2024-12-28 14:14:30,601 - INFO - Early stopping triggered at epoch 25
2024-12-28 14:14:30,601 - INFO - Training completed in 11.73s
2024-12-28 14:14:30,602 - INFO - Final memory usage: CPU 1818.6 MB, GPU 103.6 MB
2024-12-28 14:14:30,603 - INFO - Model training completed in 11.73s
2024-12-28 14:14:30,691 - INFO - Prediction completed in 0.09s
2024-12-28 14:14:30,702 - INFO - Poison rate 0.0 completed in 20.83s
2024-12-28 14:14:30,702 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:14:30,707 - INFO - Total number of labels flipped: 197
2024-12-28 14:14:30,708 - INFO - Label flipping completed in 0.01s
2024-12-28 14:14:30,708 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:14:30,708 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:14:32,007 - INFO - Feature scaling completed in 1.30s
2024-12-28 14:14:32,007 - INFO - Starting feature selection (k=50)
2024-12-28 14:14:32,035 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:14:32,035 - INFO - Starting anomaly detection
2024-12-28 14:14:39,830 - INFO - Anomaly detection completed in 7.79s
2024-12-28 14:14:39,830 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:14:39,830 - INFO - Total fit_transform time: 9.12s
2024-12-28 14:14:39,830 - INFO - Training set processing completed in 9.12s
2024-12-28 14:14:39,830 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:14:39,831 - INFO - Memory usage at start_fit: CPU 1780.0 MB, GPU 103.5 MB
2024-12-28 14:14:39,831 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:14:39,837 - INFO - Number of unique classes: 43
2024-12-28 14:14:39,977 - INFO - Fitted scaler and transformed data
2024-12-28 14:14:39,978 - INFO - Scaling time: 0.14s
2024-12-28 14:14:40,387 - INFO - Epoch 1/1000, Train Loss: 1.9502, Val Loss: 1.4795
2024-12-28 14:14:40,827 - INFO - Epoch 2/1000, Train Loss: 1.2618, Val Loss: 1.2297
2024-12-28 14:14:41,330 - INFO - Epoch 3/1000, Train Loss: 1.0829, Val Loss: 1.1091
2024-12-28 14:14:41,935 - INFO - Epoch 4/1000, Train Loss: 0.9890, Val Loss: 1.0496
2024-12-28 14:14:42,595 - INFO - Epoch 5/1000, Train Loss: 0.9322, Val Loss: 1.0150
2024-12-28 14:14:43,258 - INFO - Epoch 6/1000, Train Loss: 0.8949, Val Loss: 0.9863
2024-12-28 14:14:44,117 - INFO - Epoch 7/1000, Train Loss: 0.8682, Val Loss: 0.9711
2024-12-28 14:14:44,956 - INFO - Epoch 8/1000, Train Loss: 0.8493, Val Loss: 0.9493
2024-12-28 14:14:45,701 - INFO - Epoch 9/1000, Train Loss: 0.8357, Val Loss: 0.9540
2024-12-28 14:14:46,382 - INFO - Epoch 10/1000, Train Loss: 0.8254, Val Loss: 0.9387
2024-12-28 14:14:47,167 - INFO - Epoch 11/1000, Train Loss: 0.8172, Val Loss: 0.9308
2024-12-28 14:14:47,870 - INFO - Epoch 12/1000, Train Loss: 0.8135, Val Loss: 0.9341
2024-12-28 14:14:48,602 - INFO - Epoch 13/1000, Train Loss: 0.8064, Val Loss: 0.9173
2024-12-28 14:14:49,061 - INFO - Epoch 14/1000, Train Loss: 0.8026, Val Loss: 0.9136
2024-12-28 14:14:49,502 - INFO - Epoch 15/1000, Train Loss: 0.8000, Val Loss: 0.9139
2024-12-28 14:14:49,917 - INFO - Epoch 16/1000, Train Loss: 0.7972, Val Loss: 0.9073
2024-12-28 14:14:50,340 - INFO - Epoch 17/1000, Train Loss: 0.7932, Val Loss: 0.9130
2024-12-28 14:14:50,773 - INFO - Epoch 18/1000, Train Loss: 0.7927, Val Loss: 0.9148
2024-12-28 14:14:51,226 - INFO - Epoch 19/1000, Train Loss: 0.7910, Val Loss: 0.9018
2024-12-28 14:14:51,662 - INFO - Epoch 20/1000, Train Loss: 0.7916, Val Loss: 0.9010
2024-12-28 14:14:52,104 - INFO - Epoch 21/1000, Train Loss: 0.7890, Val Loss: 0.9118
2024-12-28 14:14:52,758 - INFO - Epoch 22/1000, Train Loss: 0.7886, Val Loss: 0.9067
2024-12-28 14:14:53,451 - INFO - Epoch 23/1000, Train Loss: 0.7884, Val Loss: 0.9030
2024-12-28 14:14:54,169 - INFO - Epoch 24/1000, Train Loss: 0.7885, Val Loss: 0.9002
2024-12-28 14:14:54,892 - INFO - Epoch 25/1000, Train Loss: 0.7878, Val Loss: 0.9056
2024-12-28 14:14:55,712 - INFO - Epoch 26/1000, Train Loss: 0.7874, Val Loss: 0.9016
2024-12-28 14:14:56,424 - INFO - Epoch 27/1000, Train Loss: 0.7870, Val Loss: 0.8997
2024-12-28 14:14:57,144 - INFO - Epoch 28/1000, Train Loss: 0.7869, Val Loss: 0.8994
2024-12-28 14:14:57,842 - INFO - Epoch 29/1000, Train Loss: 0.7875, Val Loss: 0.9124
2024-12-28 14:14:57,842 - INFO - Early stopping triggered at epoch 29
2024-12-28 14:14:57,843 - INFO - Training completed in 18.01s
2024-12-28 14:14:57,843 - INFO - Final memory usage: CPU 1818.6 MB, GPU 103.6 MB
2024-12-28 14:14:57,843 - INFO - Model training completed in 18.01s
2024-12-28 14:14:57,905 - INFO - Prediction completed in 0.06s
2024-12-28 14:14:57,916 - INFO - Poison rate 0.01 completed in 27.21s
2024-12-28 14:14:57,916 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:14:57,928 - INFO - Total number of labels flipped: 592
2024-12-28 14:14:57,928 - INFO - Label flipping completed in 0.01s
2024-12-28 14:14:57,928 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:14:57,928 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:14:59,182 - INFO - Feature scaling completed in 1.25s
2024-12-28 14:14:59,183 - INFO - Starting feature selection (k=50)
2024-12-28 14:14:59,209 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:14:59,209 - INFO - Starting anomaly detection
2024-12-28 14:15:07,495 - INFO - Anomaly detection completed in 8.29s
2024-12-28 14:15:07,495 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:15:07,496 - INFO - Total fit_transform time: 9.57s
2024-12-28 14:15:07,496 - INFO - Training set processing completed in 9.57s
2024-12-28 14:15:07,496 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:15:07,497 - INFO - Memory usage at start_fit: CPU 1780.0 MB, GPU 103.5 MB
2024-12-28 14:15:07,497 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:15:07,501 - INFO - Number of unique classes: 43
2024-12-28 14:15:07,647 - INFO - Fitted scaler and transformed data
2024-12-28 14:15:07,647 - INFO - Scaling time: 0.14s
2024-12-28 14:15:08,053 - INFO - Epoch 1/1000, Train Loss: 2.0068, Val Loss: 1.5566
2024-12-28 14:15:08,451 - INFO - Epoch 2/1000, Train Loss: 1.3737, Val Loss: 1.3500
2024-12-28 14:15:08,867 - INFO - Epoch 3/1000, Train Loss: 1.2025, Val Loss: 1.2418
2024-12-28 14:15:09,272 - INFO - Epoch 4/1000, Train Loss: 1.1158, Val Loss: 1.1896
2024-12-28 14:15:09,688 - INFO - Epoch 5/1000, Train Loss: 1.0605, Val Loss: 1.1412
2024-12-28 14:15:10,111 - INFO - Epoch 6/1000, Train Loss: 1.0230, Val Loss: 1.1234
2024-12-28 14:15:10,538 - INFO - Epoch 7/1000, Train Loss: 0.9963, Val Loss: 1.1154
2024-12-28 14:15:10,991 - INFO - Epoch 8/1000, Train Loss: 0.9792, Val Loss: 1.0968
2024-12-28 14:15:11,435 - INFO - Epoch 9/1000, Train Loss: 0.9660, Val Loss: 1.0818
2024-12-28 14:15:11,924 - INFO - Epoch 10/1000, Train Loss: 0.9541, Val Loss: 1.0675
2024-12-28 14:15:12,405 - INFO - Epoch 11/1000, Train Loss: 0.9482, Val Loss: 1.0648
2024-12-28 14:15:13,030 - INFO - Epoch 12/1000, Train Loss: 0.9387, Val Loss: 1.0645
2024-12-28 14:15:13,492 - INFO - Epoch 13/1000, Train Loss: 0.9342, Val Loss: 1.0681
2024-12-28 14:15:13,919 - INFO - Epoch 14/1000, Train Loss: 0.9302, Val Loss: 1.0572
2024-12-28 14:15:14,351 - INFO - Epoch 15/1000, Train Loss: 0.9277, Val Loss: 1.0456
2024-12-28 14:15:14,785 - INFO - Epoch 16/1000, Train Loss: 0.9250, Val Loss: 1.0477
2024-12-28 14:15:15,229 - INFO - Epoch 17/1000, Train Loss: 0.9236, Val Loss: 1.0583
2024-12-28 14:15:15,694 - INFO - Epoch 18/1000, Train Loss: 0.9211, Val Loss: 1.0437
2024-12-28 14:15:16,194 - INFO - Epoch 19/1000, Train Loss: 0.9186, Val Loss: 1.0512
2024-12-28 14:15:16,662 - INFO - Epoch 20/1000, Train Loss: 0.9196, Val Loss: 1.0484
2024-12-28 14:15:17,107 - INFO - Epoch 21/1000, Train Loss: 0.9177, Val Loss: 1.0445
2024-12-28 14:15:17,546 - INFO - Epoch 22/1000, Train Loss: 0.9161, Val Loss: 1.0470
2024-12-28 14:15:17,968 - INFO - Epoch 23/1000, Train Loss: 0.9178, Val Loss: 1.0457
2024-12-28 14:15:17,969 - INFO - Early stopping triggered at epoch 23
2024-12-28 14:15:17,969 - INFO - Training completed in 10.47s
2024-12-28 14:15:17,970 - INFO - Final memory usage: CPU 1818.6 MB, GPU 103.6 MB
2024-12-28 14:15:17,970 - INFO - Model training completed in 10.47s
2024-12-28 14:15:18,047 - INFO - Prediction completed in 0.08s
2024-12-28 14:15:18,058 - INFO - Poison rate 0.03 completed in 20.14s
2024-12-28 14:15:18,058 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:15:18,077 - INFO - Total number of labels flipped: 987
2024-12-28 14:15:18,077 - INFO - Label flipping completed in 0.02s
2024-12-28 14:15:18,078 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:15:18,078 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:15:19,312 - INFO - Feature scaling completed in 1.23s
2024-12-28 14:15:19,313 - INFO - Starting feature selection (k=50)
2024-12-28 14:15:19,339 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:15:19,340 - INFO - Starting anomaly detection
2024-12-28 14:15:27,334 - INFO - Anomaly detection completed in 7.99s
2024-12-28 14:15:27,335 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:15:27,335 - INFO - Total fit_transform time: 9.26s
2024-12-28 14:15:27,335 - INFO - Training set processing completed in 9.26s
2024-12-28 14:15:27,335 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:15:27,337 - INFO - Memory usage at start_fit: CPU 1780.0 MB, GPU 103.5 MB
2024-12-28 14:15:27,337 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:15:27,343 - INFO - Number of unique classes: 43
2024-12-28 14:15:27,506 - INFO - Fitted scaler and transformed data
2024-12-28 14:15:27,506 - INFO - Scaling time: 0.16s
2024-12-28 14:15:27,945 - INFO - Epoch 1/1000, Train Loss: 2.1101, Val Loss: 1.6445
2024-12-28 14:15:28,342 - INFO - Epoch 2/1000, Train Loss: 1.4947, Val Loss: 1.4196
2024-12-28 14:15:28,809 - INFO - Epoch 3/1000, Train Loss: 1.3329, Val Loss: 1.3274
2024-12-28 14:15:29,235 - INFO - Epoch 4/1000, Train Loss: 1.2461, Val Loss: 1.2615
2024-12-28 14:15:29,666 - INFO - Epoch 5/1000, Train Loss: 1.1912, Val Loss: 1.2269
2024-12-28 14:15:30,121 - INFO - Epoch 6/1000, Train Loss: 1.1555, Val Loss: 1.2072
2024-12-28 14:15:30,556 - INFO - Epoch 7/1000, Train Loss: 1.1291, Val Loss: 1.1807
2024-12-28 14:15:30,960 - INFO - Epoch 8/1000, Train Loss: 1.1107, Val Loss: 1.1801
2024-12-28 14:15:31,394 - INFO - Epoch 9/1000, Train Loss: 1.0973, Val Loss: 1.1601
2024-12-28 14:15:31,827 - INFO - Epoch 10/1000, Train Loss: 1.0849, Val Loss: 1.1428
2024-12-28 14:15:32,244 - INFO - Epoch 11/1000, Train Loss: 1.0747, Val Loss: 1.1473
2024-12-28 14:15:32,715 - INFO - Epoch 12/1000, Train Loss: 1.0718, Val Loss: 1.1503
2024-12-28 14:15:33,142 - INFO - Epoch 13/1000, Train Loss: 1.0650, Val Loss: 1.1479
2024-12-28 14:15:33,567 - INFO - Epoch 14/1000, Train Loss: 1.0601, Val Loss: 1.1383
2024-12-28 14:15:34,014 - INFO - Epoch 15/1000, Train Loss: 1.0587, Val Loss: 1.1263
2024-12-28 14:15:34,477 - INFO - Epoch 16/1000, Train Loss: 1.0542, Val Loss: 1.1363
2024-12-28 14:15:34,943 - INFO - Epoch 17/1000, Train Loss: 1.0529, Val Loss: 1.1355
2024-12-28 14:15:35,437 - INFO - Epoch 18/1000, Train Loss: 1.0506, Val Loss: 1.1310
2024-12-28 14:15:35,951 - INFO - Epoch 19/1000, Train Loss: 1.0507, Val Loss: 1.1354
2024-12-28 14:15:36,405 - INFO - Epoch 20/1000, Train Loss: 1.0500, Val Loss: 1.1302
2024-12-28 14:15:36,405 - INFO - Early stopping triggered at epoch 20
2024-12-28 14:15:36,405 - INFO - Training completed in 9.07s
2024-12-28 14:15:36,406 - INFO - Final memory usage: CPU 1818.6 MB, GPU 103.6 MB
2024-12-28 14:15:36,406 - INFO - Model training completed in 9.07s
2024-12-28 14:15:36,523 - INFO - Prediction completed in 0.12s
2024-12-28 14:15:36,534 - INFO - Poison rate 0.05 completed in 18.48s
2024-12-28 14:15:36,534 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:15:36,559 - INFO - Total number of labels flipped: 1382
2024-12-28 14:15:36,559 - INFO - Label flipping completed in 0.03s
2024-12-28 14:15:36,559 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:15:36,559 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:15:37,780 - INFO - Feature scaling completed in 1.22s
2024-12-28 14:15:37,780 - INFO - Starting feature selection (k=50)
2024-12-28 14:15:37,807 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:15:37,808 - INFO - Starting anomaly detection
2024-12-28 14:15:46,075 - INFO - Anomaly detection completed in 8.27s
2024-12-28 14:15:46,076 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:15:46,076 - INFO - Total fit_transform time: 9.52s
2024-12-28 14:15:46,076 - INFO - Training set processing completed in 9.52s
2024-12-28 14:15:46,076 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:15:46,077 - INFO - Memory usage at start_fit: CPU 1780.0 MB, GPU 103.5 MB
2024-12-28 14:15:46,077 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:15:46,082 - INFO - Number of unique classes: 43
2024-12-28 14:15:46,226 - INFO - Fitted scaler and transformed data
2024-12-28 14:15:46,226 - INFO - Scaling time: 0.14s
2024-12-28 14:15:46,652 - INFO - Epoch 1/1000, Train Loss: 2.1850, Val Loss: 1.6812
2024-12-28 14:15:47,068 - INFO - Epoch 2/1000, Train Loss: 1.6026, Val Loss: 1.4818
2024-12-28 14:15:47,470 - INFO - Epoch 3/1000, Train Loss: 1.4425, Val Loss: 1.4084
2024-12-28 14:15:47,881 - INFO - Epoch 4/1000, Train Loss: 1.3593, Val Loss: 1.3352
2024-12-28 14:15:48,312 - INFO - Epoch 5/1000, Train Loss: 1.3025, Val Loss: 1.3088
2024-12-28 14:15:48,733 - INFO - Epoch 6/1000, Train Loss: 1.2681, Val Loss: 1.2897
2024-12-28 14:15:49,138 - INFO - Epoch 7/1000, Train Loss: 1.2395, Val Loss: 1.2701
2024-12-28 14:15:49,588 - INFO - Epoch 8/1000, Train Loss: 1.2206, Val Loss: 1.2572
2024-12-28 14:15:50,016 - INFO - Epoch 9/1000, Train Loss: 1.2059, Val Loss: 1.2566
2024-12-28 14:15:50,436 - INFO - Epoch 10/1000, Train Loss: 1.1937, Val Loss: 1.2425
2024-12-28 14:15:50,884 - INFO - Epoch 11/1000, Train Loss: 1.1851, Val Loss: 1.2499
2024-12-28 14:15:51,349 - INFO - Epoch 12/1000, Train Loss: 1.1792, Val Loss: 1.2399
2024-12-28 14:15:51,774 - INFO - Epoch 13/1000, Train Loss: 1.1745, Val Loss: 1.2425
2024-12-28 14:15:52,215 - INFO - Epoch 14/1000, Train Loss: 1.1681, Val Loss: 1.2369
2024-12-28 14:15:52,647 - INFO - Epoch 15/1000, Train Loss: 1.1671, Val Loss: 1.2302
2024-12-28 14:15:53,070 - INFO - Epoch 16/1000, Train Loss: 1.1641, Val Loss: 1.2245
2024-12-28 14:15:53,536 - INFO - Epoch 17/1000, Train Loss: 1.1606, Val Loss: 1.2298
2024-12-28 14:15:53,998 - INFO - Epoch 18/1000, Train Loss: 1.1592, Val Loss: 1.2250
2024-12-28 14:15:54,471 - INFO - Epoch 19/1000, Train Loss: 1.1582, Val Loss: 1.2267
2024-12-28 14:15:54,870 - INFO - Epoch 20/1000, Train Loss: 1.1545, Val Loss: 1.2276
2024-12-28 14:15:55,282 - INFO - Epoch 21/1000, Train Loss: 1.1551, Val Loss: 1.2223
2024-12-28 14:15:55,730 - INFO - Epoch 22/1000, Train Loss: 1.1543, Val Loss: 1.2315
2024-12-28 14:15:56,170 - INFO - Epoch 23/1000, Train Loss: 1.1538, Val Loss: 1.2172
2024-12-28 14:15:56,625 - INFO - Epoch 24/1000, Train Loss: 1.1516, Val Loss: 1.2256
2024-12-28 14:15:57,100 - INFO - Epoch 25/1000, Train Loss: 1.1519, Val Loss: 1.2248
2024-12-28 14:15:57,530 - INFO - Epoch 26/1000, Train Loss: 1.1524, Val Loss: 1.2240
2024-12-28 14:15:57,979 - INFO - Epoch 27/1000, Train Loss: 1.1528, Val Loss: 1.2227
2024-12-28 14:15:58,419 - INFO - Epoch 28/1000, Train Loss: 1.1524, Val Loss: 1.2186
2024-12-28 14:15:58,420 - INFO - Early stopping triggered at epoch 28
2024-12-28 14:15:58,420 - INFO - Training completed in 12.34s
2024-12-28 14:15:58,421 - INFO - Final memory usage: CPU 1818.6 MB, GPU 103.6 MB
2024-12-28 14:15:58,422 - INFO - Model training completed in 12.35s
2024-12-28 14:15:58,504 - INFO - Prediction completed in 0.08s
2024-12-28 14:15:58,524 - INFO - Poison rate 0.07 completed in 21.99s
2024-12-28 14:15:58,524 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:15:58,559 - INFO - Total number of labels flipped: 1975
2024-12-28 14:15:58,559 - INFO - Label flipping completed in 0.03s
2024-12-28 14:15:58,559 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:15:58,559 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:15:59,923 - INFO - Feature scaling completed in 1.36s
2024-12-28 14:15:59,924 - INFO - Starting feature selection (k=50)
2024-12-28 14:15:59,951 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:15:59,952 - INFO - Starting anomaly detection
2024-12-28 14:16:06,899 - INFO - Anomaly detection completed in 6.95s
2024-12-28 14:16:06,900 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:16:06,900 - INFO - Total fit_transform time: 8.34s
2024-12-28 14:16:06,900 - INFO - Training set processing completed in 8.34s
2024-12-28 14:16:06,900 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:16:06,901 - INFO - Memory usage at start_fit: CPU 1780.0 MB, GPU 103.5 MB
2024-12-28 14:16:06,902 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:16:06,907 - INFO - Number of unique classes: 43
2024-12-28 14:16:07,055 - INFO - Fitted scaler and transformed data
2024-12-28 14:16:07,055 - INFO - Scaling time: 0.15s
2024-12-28 14:16:07,474 - INFO - Epoch 1/1000, Train Loss: 2.3000, Val Loss: 1.9042
2024-12-28 14:16:07,924 - INFO - Epoch 2/1000, Train Loss: 1.7450, Val Loss: 1.7169
2024-12-28 14:16:08,383 - INFO - Epoch 3/1000, Train Loss: 1.5931, Val Loss: 1.6263
2024-12-28 14:16:08,831 - INFO - Epoch 4/1000, Train Loss: 1.5106, Val Loss: 1.5758
2024-12-28 14:16:09,303 - INFO - Epoch 5/1000, Train Loss: 1.4592, Val Loss: 1.5439
2024-12-28 14:16:09,727 - INFO - Epoch 6/1000, Train Loss: 1.4225, Val Loss: 1.5191
2024-12-28 14:16:10,175 - INFO - Epoch 7/1000, Train Loss: 1.3967, Val Loss: 1.5089
2024-12-28 14:16:10,601 - INFO - Epoch 8/1000, Train Loss: 1.3754, Val Loss: 1.4966
2024-12-28 14:16:11,086 - INFO - Epoch 9/1000, Train Loss: 1.3597, Val Loss: 1.4914
2024-12-28 14:16:11,554 - INFO - Epoch 10/1000, Train Loss: 1.3508, Val Loss: 1.4781
2024-12-28 14:16:12,021 - INFO - Epoch 11/1000, Train Loss: 1.3387, Val Loss: 1.4818
2024-12-28 14:16:12,481 - INFO - Epoch 12/1000, Train Loss: 1.3350, Val Loss: 1.4617
2024-12-28 14:16:12,918 - INFO - Epoch 13/1000, Train Loss: 1.3277, Val Loss: 1.4634
2024-12-28 14:16:13,377 - INFO - Epoch 14/1000, Train Loss: 1.3250, Val Loss: 1.4679
2024-12-28 14:16:13,842 - INFO - Epoch 15/1000, Train Loss: 1.3208, Val Loss: 1.4615
2024-12-28 14:16:14,295 - INFO - Epoch 16/1000, Train Loss: 1.3181, Val Loss: 1.4525
2024-12-28 14:16:14,766 - INFO - Epoch 17/1000, Train Loss: 1.3163, Val Loss: 1.4559
2024-12-28 14:16:15,213 - INFO - Epoch 18/1000, Train Loss: 1.3143, Val Loss: 1.4469
2024-12-28 14:16:15,673 - INFO - Epoch 19/1000, Train Loss: 1.3107, Val Loss: 1.4592
2024-12-28 14:16:16,155 - INFO - Epoch 20/1000, Train Loss: 1.3105, Val Loss: 1.4550
2024-12-28 14:16:16,634 - INFO - Epoch 21/1000, Train Loss: 1.3090, Val Loss: 1.4469
2024-12-28 14:16:17,075 - INFO - Epoch 22/1000, Train Loss: 1.3077, Val Loss: 1.4481
2024-12-28 14:16:17,496 - INFO - Epoch 23/1000, Train Loss: 1.3069, Val Loss: 1.4543
2024-12-28 14:16:17,496 - INFO - Early stopping triggered at epoch 23
2024-12-28 14:16:17,496 - INFO - Training completed in 10.60s
2024-12-28 14:16:17,497 - INFO - Final memory usage: CPU 1818.6 MB, GPU 103.6 MB
2024-12-28 14:16:17,498 - INFO - Model training completed in 10.60s
2024-12-28 14:16:17,567 - INFO - Prediction completed in 0.07s
2024-12-28 14:16:17,578 - INFO - Poison rate 0.1 completed in 19.05s
2024-12-28 14:16:17,578 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:16:17,646 - INFO - Total number of labels flipped: 3951
2024-12-28 14:16:17,647 - INFO - Label flipping completed in 0.07s
2024-12-28 14:16:17,647 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:16:17,647 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:16:18,920 - INFO - Feature scaling completed in 1.27s
2024-12-28 14:16:18,920 - INFO - Starting feature selection (k=50)
2024-12-28 14:16:18,952 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:16:18,953 - INFO - Starting anomaly detection
2024-12-28 14:16:27,066 - INFO - Anomaly detection completed in 8.11s
2024-12-28 14:16:27,067 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:16:27,067 - INFO - Total fit_transform time: 9.42s
2024-12-28 14:16:27,067 - INFO - Training set processing completed in 9.42s
2024-12-28 14:16:27,067 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:16:27,068 - INFO - Memory usage at start_fit: CPU 1780.0 MB, GPU 103.5 MB
2024-12-28 14:16:27,068 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:16:27,073 - INFO - Number of unique classes: 43
2024-12-28 14:16:27,212 - INFO - Fitted scaler and transformed data
2024-12-28 14:16:27,212 - INFO - Scaling time: 0.14s
2024-12-28 14:16:27,662 - INFO - Epoch 1/1000, Train Loss: 2.6475, Val Loss: 2.2887
2024-12-28 14:16:28,106 - INFO - Epoch 2/1000, Train Loss: 2.1883, Val Loss: 2.1400
2024-12-28 14:16:28,572 - INFO - Epoch 3/1000, Train Loss: 2.0534, Val Loss: 2.0720
2024-12-28 14:16:29,032 - INFO - Epoch 4/1000, Train Loss: 1.9764, Val Loss: 2.0249
2024-12-28 14:16:29,483 - INFO - Epoch 5/1000, Train Loss: 1.9262, Val Loss: 1.9985
2024-12-28 14:16:29,929 - INFO - Epoch 6/1000, Train Loss: 1.8931, Val Loss: 1.9681
2024-12-28 14:16:30,389 - INFO - Epoch 7/1000, Train Loss: 1.8682, Val Loss: 1.9594
2024-12-28 14:16:30,813 - INFO - Epoch 8/1000, Train Loss: 1.8479, Val Loss: 1.9537
2024-12-28 14:16:31,262 - INFO - Epoch 9/1000, Train Loss: 1.8345, Val Loss: 1.9400
2024-12-28 14:16:31,689 - INFO - Epoch 10/1000, Train Loss: 1.8212, Val Loss: 1.9352
2024-12-28 14:16:32,146 - INFO - Epoch 11/1000, Train Loss: 1.8123, Val Loss: 1.9340
2024-12-28 14:16:32,613 - INFO - Epoch 12/1000, Train Loss: 1.8034, Val Loss: 1.9252
2024-12-28 14:16:33,041 - INFO - Epoch 13/1000, Train Loss: 1.8002, Val Loss: 1.9312
2024-12-28 14:16:33,507 - INFO - Epoch 14/1000, Train Loss: 1.7951, Val Loss: 1.9313
2024-12-28 14:16:33,937 - INFO - Epoch 15/1000, Train Loss: 1.7896, Val Loss: 1.9168
2024-12-28 14:16:34,376 - INFO - Epoch 16/1000, Train Loss: 1.7875, Val Loss: 1.9188
2024-12-28 14:16:34,797 - INFO - Epoch 17/1000, Train Loss: 1.7858, Val Loss: 1.9178
2024-12-28 14:16:35,228 - INFO - Epoch 18/1000, Train Loss: 1.7837, Val Loss: 1.9103
2024-12-28 14:16:35,677 - INFO - Epoch 19/1000, Train Loss: 1.7795, Val Loss: 1.9188
2024-12-28 14:16:36,131 - INFO - Epoch 20/1000, Train Loss: 1.7787, Val Loss: 1.9127
2024-12-28 14:16:36,586 - INFO - Epoch 21/1000, Train Loss: 1.7767, Val Loss: 1.9094
2024-12-28 14:16:37,023 - INFO - Epoch 22/1000, Train Loss: 1.7775, Val Loss: 1.9135
2024-12-28 14:16:37,445 - INFO - Epoch 23/1000, Train Loss: 1.7737, Val Loss: 1.9053
2024-12-28 14:16:37,858 - INFO - Epoch 24/1000, Train Loss: 1.7755, Val Loss: 1.9092
2024-12-28 14:16:38,322 - INFO - Epoch 25/1000, Train Loss: 1.7735, Val Loss: 1.9241
2024-12-28 14:16:38,797 - INFO - Epoch 26/1000, Train Loss: 1.7741, Val Loss: 1.9079
2024-12-28 14:16:39,225 - INFO - Epoch 27/1000, Train Loss: 1.7734, Val Loss: 1.9181
2024-12-28 14:16:39,649 - INFO - Epoch 28/1000, Train Loss: 1.7723, Val Loss: 1.9188
2024-12-28 14:16:39,650 - INFO - Early stopping triggered at epoch 28
2024-12-28 14:16:39,650 - INFO - Training completed in 12.58s
2024-12-28 14:16:39,651 - INFO - Final memory usage: CPU 1818.6 MB, GPU 103.6 MB
2024-12-28 14:16:39,652 - INFO - Model training completed in 12.58s
2024-12-28 14:16:39,719 - INFO - Prediction completed in 0.07s
2024-12-28 14:16:39,730 - INFO - Poison rate 0.2 completed in 22.15s
2024-12-28 14:16:39,731 - INFO - Loaded 21 existing results
2024-12-28 14:16:39,731 - INFO - Total results to save: 28
2024-12-28 14:16:39,732 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:16:39,734 - INFO - Saved 28 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:16:39,734 - INFO - Total evaluation time: 171.90s
2024-12-28 14:16:39,739 - INFO - 
Progress: 5.2% - Evaluating GTSRB with RandomForest (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:16:39,915 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:16:39,915 - INFO - Dataset type: image
2024-12-28 14:16:39,915 - INFO - Sample size: 39209
2024-12-28 14:16:39,915 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:16:39,916 - INFO - Loading datasets...
2024-12-28 14:16:58,418 - INFO - Dataset loading completed in 18.50s
2024-12-28 14:16:58,418 - INFO - Extracting validation features...
2024-12-28 14:16:58,418 - INFO - Extracting features from 4435 samples...
2024-12-28 14:16:59,163 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:16:59,168 - INFO - Validation feature extraction completed in 0.75s
2024-12-28 14:16:59,168 - INFO - Extracting training features...
2024-12-28 14:16:59,169 - INFO - Extracting features from 19755 samples...
2024-12-28 14:17:01,793 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:17:01,805 - INFO - Training feature extraction completed in 2.64s
2024-12-28 14:17:01,805 - INFO - Creating model for classifier: RandomForest
2024-12-28 14:17:01,806 - INFO - Using device: cuda
2024-12-28 14:17:01,806 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:17:01,806 - INFO - Training set processing completed in 0.00s
2024-12-28 14:17:01,806 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:17:01,807 - INFO - Memory usage at start_fit: CPU 1782.3 MB, GPU 103.1 MB
2024-12-28 14:17:01,807 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:17:02,347 - INFO - Fitted scaler and transformed data
2024-12-28 14:17:02,347 - INFO - Scaling time: 0.54s
2024-12-28 14:17:02,379 - INFO - Number of unique classes: 43
2024-12-28 14:17:09,212 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:17:15,326 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 14:17:22,160 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 14:17:28,109 - INFO - Epoch 4/10, Train Loss: 3.7587, Val Loss: 3.7585
2024-12-28 14:17:28,109 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:17:28,109 - INFO - Training completed in 26.30s
2024-12-28 14:17:28,109 - INFO - Final memory usage: CPU 1789.7 MB, GPU 152.7 MB
2024-12-28 14:17:28,110 - INFO - Model training completed in 26.30s
2024-12-28 14:17:28,349 - INFO - Prediction completed in 0.24s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:17:28,365 - INFO - Poison rate 0.0 completed in 26.56s
2024-12-28 14:17:28,366 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:17:28,380 - INFO - Total number of labels flipped: 197
2024-12-28 14:17:28,381 - INFO - Label flipping completed in 0.01s
2024-12-28 14:17:28,381 - INFO - Training set processing completed in 0.00s
2024-12-28 14:17:28,381 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:17:28,382 - INFO - Memory usage at start_fit: CPU 1789.7 MB, GPU 111.6 MB
2024-12-28 14:17:28,383 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:17:28,942 - INFO - Fitted scaler and transformed data
2024-12-28 14:17:28,942 - INFO - Scaling time: 0.56s
2024-12-28 14:17:28,962 - INFO - Number of unique classes: 43
2024-12-28 14:17:35,297 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:17:42,207 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 14:17:49,064 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7593
2024-12-28 14:17:55,424 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7586
2024-12-28 14:17:55,424 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:17:55,424 - INFO - Training completed in 27.04s
2024-12-28 14:17:55,424 - INFO - Final memory usage: CPU 1789.7 MB, GPU 152.7 MB
2024-12-28 14:17:55,425 - INFO - Model training completed in 27.04s
2024-12-28 14:17:55,596 - INFO - Prediction completed in 0.17s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:17:55,609 - INFO - Poison rate 0.01 completed in 27.24s
2024-12-28 14:17:55,609 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:17:55,621 - INFO - Total number of labels flipped: 592
2024-12-28 14:17:55,621 - INFO - Label flipping completed in 0.01s
2024-12-28 14:17:55,621 - INFO - Training set processing completed in 0.00s
2024-12-28 14:17:55,621 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:17:55,622 - INFO - Memory usage at start_fit: CPU 1789.7 MB, GPU 111.6 MB
2024-12-28 14:17:55,622 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:17:56,185 - INFO - Fitted scaler and transformed data
2024-12-28 14:17:56,185 - INFO - Scaling time: 0.56s
2024-12-28 14:17:56,205 - INFO - Number of unique classes: 43
2024-12-28 14:18:02,119 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:18:08,919 - INFO - Epoch 2/10, Train Loss: 3.7603, Val Loss: 3.7600
2024-12-28 14:18:15,717 - INFO - Epoch 3/10, Train Loss: 3.7596, Val Loss: 3.7594
2024-12-28 14:18:22,709 - INFO - Epoch 4/10, Train Loss: 3.7589, Val Loss: 3.7587
2024-12-28 14:18:22,709 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:18:22,709 - INFO - Training completed in 27.09s
2024-12-28 14:18:22,710 - INFO - Final memory usage: CPU 1789.7 MB, GPU 152.7 MB
2024-12-28 14:18:22,710 - INFO - Model training completed in 27.09s
2024-12-28 14:18:22,897 - INFO - Prediction completed in 0.19s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:18:22,909 - INFO - Poison rate 0.03 completed in 27.30s
2024-12-28 14:18:22,909 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:18:22,929 - INFO - Total number of labels flipped: 987
2024-12-28 14:18:22,929 - INFO - Label flipping completed in 0.02s
2024-12-28 14:18:22,929 - INFO - Training set processing completed in 0.00s
2024-12-28 14:18:22,929 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:18:22,930 - INFO - Memory usage at start_fit: CPU 1789.7 MB, GPU 111.6 MB
2024-12-28 14:18:22,930 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:18:23,476 - INFO - Fitted scaler and transformed data
2024-12-28 14:18:23,476 - INFO - Scaling time: 0.55s
2024-12-28 14:18:23,496 - INFO - Number of unique classes: 43
2024-12-28 14:18:30,060 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:18:35,998 - INFO - Epoch 2/10, Train Loss: 3.7603, Val Loss: 3.7600
2024-12-28 14:18:42,139 - INFO - Epoch 3/10, Train Loss: 3.7596, Val Loss: 3.7594
2024-12-28 14:18:49,306 - INFO - Epoch 4/10, Train Loss: 3.7589, Val Loss: 3.7587
2024-12-28 14:18:49,306 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:18:49,306 - INFO - Training completed in 26.38s
2024-12-28 14:18:49,307 - INFO - Final memory usage: CPU 1789.7 MB, GPU 152.7 MB
2024-12-28 14:18:49,307 - INFO - Model training completed in 26.38s
2024-12-28 14:18:49,536 - INFO - Prediction completed in 0.23s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:18:49,549 - INFO - Poison rate 0.05 completed in 26.64s
2024-12-28 14:18:49,549 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:18:49,598 - INFO - Total number of labels flipped: 1382
2024-12-28 14:18:49,599 - INFO - Label flipping completed in 0.05s
2024-12-28 14:18:49,599 - INFO - Training set processing completed in 0.00s
2024-12-28 14:18:49,599 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:18:49,600 - INFO - Memory usage at start_fit: CPU 1789.7 MB, GPU 111.6 MB
2024-12-28 14:18:49,600 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:18:50,214 - INFO - Fitted scaler and transformed data
2024-12-28 14:18:50,215 - INFO - Scaling time: 0.61s
2024-12-28 14:18:50,234 - INFO - Number of unique classes: 43
2024-12-28 14:18:57,727 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7607
2024-12-28 14:19:04,165 - INFO - Epoch 2/10, Train Loss: 3.7603, Val Loss: 3.7601
2024-12-28 14:19:11,107 - INFO - Epoch 3/10, Train Loss: 3.7597, Val Loss: 3.7596
2024-12-28 14:19:17,465 - INFO - Epoch 4/10, Train Loss: 3.7590, Val Loss: 3.7590
2024-12-28 14:19:17,466 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:19:17,466 - INFO - Training completed in 27.87s
2024-12-28 14:19:17,466 - INFO - Final memory usage: CPU 1789.7 MB, GPU 152.7 MB
2024-12-28 14:19:17,466 - INFO - Model training completed in 27.87s
2024-12-28 14:19:17,657 - INFO - Prediction completed in 0.19s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:19:17,669 - INFO - Poison rate 0.07 completed in 28.12s
2024-12-28 14:19:17,669 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:19:17,707 - INFO - Total number of labels flipped: 1975
2024-12-28 14:19:17,707 - INFO - Label flipping completed in 0.04s
2024-12-28 14:19:17,707 - INFO - Training set processing completed in 0.00s
2024-12-28 14:19:17,707 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:19:17,708 - INFO - Memory usage at start_fit: CPU 1789.7 MB, GPU 111.6 MB
2024-12-28 14:19:17,708 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:19:18,209 - INFO - Fitted scaler and transformed data
2024-12-28 14:19:18,210 - INFO - Scaling time: 0.50s
2024-12-28 14:19:18,230 - INFO - Number of unique classes: 43
2024-12-28 14:19:25,373 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7607
2024-12-28 14:19:31,339 - INFO - Epoch 2/10, Train Loss: 3.7604, Val Loss: 3.7601
2024-12-28 14:19:37,365 - INFO - Epoch 3/10, Train Loss: 3.7598, Val Loss: 3.7595
2024-12-28 14:19:43,591 - INFO - Epoch 4/10, Train Loss: 3.7591, Val Loss: 3.7590
2024-12-28 14:19:43,591 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:19:43,592 - INFO - Training completed in 25.88s
2024-12-28 14:19:43,592 - INFO - Final memory usage: CPU 1789.7 MB, GPU 152.7 MB
2024-12-28 14:19:43,592 - INFO - Model training completed in 25.89s
2024-12-28 14:19:43,765 - INFO - Prediction completed in 0.17s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:19:43,776 - INFO - Poison rate 0.1 completed in 26.11s
2024-12-28 14:19:43,776 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:19:43,846 - INFO - Total number of labels flipped: 3951
2024-12-28 14:19:43,846 - INFO - Label flipping completed in 0.07s
2024-12-28 14:19:43,846 - INFO - Training set processing completed in 0.00s
2024-12-28 14:19:43,846 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:19:43,847 - INFO - Memory usage at start_fit: CPU 1789.7 MB, GPU 111.6 MB
2024-12-28 14:19:43,848 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:19:44,450 - INFO - Fitted scaler and transformed data
2024-12-28 14:19:44,451 - INFO - Scaling time: 0.60s
2024-12-28 14:19:44,471 - INFO - Number of unique classes: 43
2024-12-28 14:19:52,160 - INFO - Epoch 1/10, Train Loss: 3.7610, Val Loss: 3.7608
2024-12-28 14:19:59,666 - INFO - Epoch 2/10, Train Loss: 3.7605, Val Loss: 3.7604
2024-12-28 14:20:05,294 - INFO - Epoch 3/10, Train Loss: 3.7600, Val Loss: 3.7599
2024-12-28 14:20:11,687 - INFO - Epoch 4/10, Train Loss: 3.7595, Val Loss: 3.7595
2024-12-28 14:20:11,687 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:20:11,687 - INFO - Training completed in 27.84s
2024-12-28 14:20:11,688 - INFO - Final memory usage: CPU 1789.7 MB, GPU 152.7 MB
2024-12-28 14:20:11,688 - INFO - Model training completed in 27.84s
2024-12-28 14:20:11,875 - INFO - Prediction completed in 0.19s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:20:11,886 - INFO - Poison rate 0.2 completed in 28.11s
2024-12-28 14:20:11,887 - INFO - Loaded 28 existing results
2024-12-28 14:20:11,887 - INFO - Total results to save: 35
2024-12-28 14:20:11,888 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:20:11,890 - INFO - Saved 35 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:20:11,890 - INFO - Total evaluation time: 211.97s
2024-12-28 14:20:11,896 - INFO - 
Progress: 6.2% - Evaluating GTSRB with RandomForest (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:20:12,083 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:20:12,083 - INFO - Dataset type: image
2024-12-28 14:20:12,083 - INFO - Sample size: 39209
2024-12-28 14:20:12,083 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:20:12,084 - INFO - Loading datasets...
2024-12-28 14:20:31,959 - INFO - Dataset loading completed in 19.87s
2024-12-28 14:20:31,960 - INFO - Extracting validation features...
2024-12-28 14:20:31,960 - INFO - Extracting features from 4435 samples...
2024-12-28 14:20:32,710 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:20:32,714 - INFO - Validation feature extraction completed in 0.75s
2024-12-28 14:20:32,714 - INFO - Extracting training features...
2024-12-28 14:20:32,714 - INFO - Extracting features from 19755 samples...
2024-12-28 14:20:35,252 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:20:35,261 - INFO - Training feature extraction completed in 2.55s
2024-12-28 14:20:35,261 - INFO - Creating model for classifier: RandomForest
2024-12-28 14:20:35,261 - INFO - Using device: cuda
2024-12-28 14:20:35,261 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:20:35,262 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:20:35,262 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:20:36,487 - INFO - Feature scaling completed in 1.23s
2024-12-28 14:20:36,488 - INFO - Starting feature selection (k=50)
2024-12-28 14:20:36,521 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:20:36,522 - INFO - Starting anomaly detection
2024-12-28 14:20:44,340 - INFO - Anomaly detection completed in 7.82s
2024-12-28 14:20:44,340 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:20:44,340 - INFO - Total fit_transform time: 9.08s
2024-12-28 14:20:44,340 - INFO - Training set processing completed in 9.08s
2024-12-28 14:20:44,340 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:20:44,342 - INFO - Memory usage at start_fit: CPU 1809.0 MB, GPU 103.1 MB
2024-12-28 14:20:44,342 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:20:44,908 - INFO - Fitted scaler and transformed data
2024-12-28 14:20:44,908 - INFO - Scaling time: 0.57s
2024-12-28 14:20:44,928 - INFO - Number of unique classes: 43
2024-12-28 14:20:52,019 - INFO - Epoch 1/10, Train Loss: 3.5721, Val Loss: 3.7606
2024-12-28 14:20:59,382 - INFO - Epoch 2/10, Train Loss: 3.5714, Val Loss: 3.7599
2024-12-28 14:21:06,326 - INFO - Epoch 3/10, Train Loss: 3.5707, Val Loss: 3.7592
2024-12-28 14:21:12,828 - INFO - Epoch 4/10, Train Loss: 3.5700, Val Loss: 3.7585
2024-12-28 14:21:12,828 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:21:12,828 - INFO - Training completed in 28.49s
2024-12-28 14:21:12,828 - INFO - Final memory usage: CPU 1809.0 MB, GPU 152.7 MB
2024-12-28 14:21:12,829 - INFO - Model training completed in 28.49s
2024-12-28 14:21:13,115 - INFO - Prediction completed in 0.29s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:21:13,146 - INFO - Poison rate 0.0 completed in 37.88s
2024-12-28 14:21:13,146 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:21:13,152 - INFO - Total number of labels flipped: 197
2024-12-28 14:21:13,152 - INFO - Label flipping completed in 0.01s
2024-12-28 14:21:13,152 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:21:13,152 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:21:14,386 - INFO - Feature scaling completed in 1.23s
2024-12-28 14:21:14,387 - INFO - Starting feature selection (k=50)
2024-12-28 14:21:14,413 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:21:14,413 - INFO - Starting anomaly detection
2024-12-28 14:21:21,413 - INFO - Anomaly detection completed in 7.00s
2024-12-28 14:21:21,413 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:21:21,413 - INFO - Total fit_transform time: 8.26s
2024-12-28 14:21:21,413 - INFO - Training set processing completed in 8.26s
2024-12-28 14:21:21,413 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:21:21,414 - INFO - Memory usage at start_fit: CPU 1809.0 MB, GPU 111.6 MB
2024-12-28 14:21:21,414 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:21:21,920 - INFO - Fitted scaler and transformed data
2024-12-28 14:21:21,920 - INFO - Scaling time: 0.51s
2024-12-28 14:21:21,933 - INFO - Number of unique classes: 43
2024-12-28 14:21:28,311 - INFO - Epoch 1/10, Train Loss: 3.5733, Val Loss: 3.7606
2024-12-28 14:21:35,162 - INFO - Epoch 2/10, Train Loss: 3.5726, Val Loss: 3.7599
2024-12-28 14:21:41,529 - INFO - Epoch 3/10, Train Loss: 3.5719, Val Loss: 3.7593
2024-12-28 14:21:48,088 - INFO - Epoch 4/10, Train Loss: 3.5712, Val Loss: 3.7586
2024-12-28 14:21:48,088 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:21:48,088 - INFO - Training completed in 26.67s
2024-12-28 14:21:48,089 - INFO - Final memory usage: CPU 1809.0 MB, GPU 152.7 MB
2024-12-28 14:21:48,089 - INFO - Model training completed in 26.68s
2024-12-28 14:21:48,360 - INFO - Prediction completed in 0.27s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:21:48,372 - INFO - Poison rate 0.01 completed in 35.23s
2024-12-28 14:21:48,372 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:21:48,384 - INFO - Total number of labels flipped: 592
2024-12-28 14:21:48,384 - INFO - Label flipping completed in 0.01s
2024-12-28 14:21:48,384 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:21:48,384 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:21:49,634 - INFO - Feature scaling completed in 1.25s
2024-12-28 14:21:49,634 - INFO - Starting feature selection (k=50)
2024-12-28 14:21:49,660 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:21:49,660 - INFO - Starting anomaly detection
2024-12-28 14:21:57,501 - INFO - Anomaly detection completed in 7.84s
2024-12-28 14:21:57,501 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:21:57,501 - INFO - Total fit_transform time: 9.12s
2024-12-28 14:21:57,501 - INFO - Training set processing completed in 9.12s
2024-12-28 14:21:57,501 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:21:57,502 - INFO - Memory usage at start_fit: CPU 1809.0 MB, GPU 111.6 MB
2024-12-28 14:21:57,503 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:21:58,028 - INFO - Fitted scaler and transformed data
2024-12-28 14:21:58,028 - INFO - Scaling time: 0.53s
2024-12-28 14:21:58,055 - INFO - Number of unique classes: 43
2024-12-28 14:22:04,115 - INFO - Epoch 1/10, Train Loss: 3.5722, Val Loss: 3.7606
2024-12-28 14:22:10,050 - INFO - Epoch 2/10, Train Loss: 3.5716, Val Loss: 3.7600
2024-12-28 14:22:15,396 - INFO - Epoch 3/10, Train Loss: 3.5709, Val Loss: 3.7593
2024-12-28 14:22:21,180 - INFO - Epoch 4/10, Train Loss: 3.5703, Val Loss: 3.7586
2024-12-28 14:22:21,180 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:22:21,180 - INFO - Training completed in 23.68s
2024-12-28 14:22:21,181 - INFO - Final memory usage: CPU 1809.0 MB, GPU 152.7 MB
2024-12-28 14:22:21,181 - INFO - Model training completed in 23.68s
2024-12-28 14:22:21,398 - INFO - Prediction completed in 0.22s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:22:21,411 - INFO - Poison rate 0.03 completed in 33.04s
2024-12-28 14:22:21,411 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:22:21,430 - INFO - Total number of labels flipped: 987
2024-12-28 14:22:21,431 - INFO - Label flipping completed in 0.02s
2024-12-28 14:22:21,431 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:22:21,431 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:22:22,642 - INFO - Feature scaling completed in 1.21s
2024-12-28 14:22:22,643 - INFO - Starting feature selection (k=50)
2024-12-28 14:22:22,669 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:22:22,669 - INFO - Starting anomaly detection
2024-12-28 14:22:30,459 - INFO - Anomaly detection completed in 7.79s
2024-12-28 14:22:30,460 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:22:30,460 - INFO - Total fit_transform time: 9.03s
2024-12-28 14:22:30,460 - INFO - Training set processing completed in 9.03s
2024-12-28 14:22:30,460 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:22:30,461 - INFO - Memory usage at start_fit: CPU 1809.0 MB, GPU 111.6 MB
2024-12-28 14:22:30,461 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:22:30,972 - INFO - Fitted scaler and transformed data
2024-12-28 14:22:30,972 - INFO - Scaling time: 0.51s
2024-12-28 14:22:30,986 - INFO - Number of unique classes: 43
2024-12-28 14:22:36,286 - INFO - Epoch 1/10, Train Loss: 3.5727, Val Loss: 3.7606
2024-12-28 14:22:41,643 - INFO - Epoch 2/10, Train Loss: 3.5721, Val Loss: 3.7600
2024-12-28 14:22:47,187 - INFO - Epoch 3/10, Train Loss: 3.5715, Val Loss: 3.7594
2024-12-28 14:22:52,879 - INFO - Epoch 4/10, Train Loss: 3.5709, Val Loss: 3.7588
2024-12-28 14:22:52,880 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:22:52,880 - INFO - Training completed in 22.42s
2024-12-28 14:22:52,880 - INFO - Final memory usage: CPU 1809.0 MB, GPU 152.7 MB
2024-12-28 14:22:52,880 - INFO - Model training completed in 22.42s
2024-12-28 14:22:53,107 - INFO - Prediction completed in 0.23s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:22:53,127 - INFO - Poison rate 0.05 completed in 31.72s
2024-12-28 14:22:53,127 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:22:53,200 - INFO - Total number of labels flipped: 1382
2024-12-28 14:22:53,201 - INFO - Label flipping completed in 0.07s
2024-12-28 14:22:53,201 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:22:53,201 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:22:54,461 - INFO - Feature scaling completed in 1.26s
2024-12-28 14:22:54,461 - INFO - Starting feature selection (k=50)
2024-12-28 14:22:54,487 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:22:54,487 - INFO - Starting anomaly detection
2024-12-28 14:23:03,058 - INFO - Anomaly detection completed in 8.57s
2024-12-28 14:23:03,058 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:23:03,058 - INFO - Total fit_transform time: 9.86s
2024-12-28 14:23:03,058 - INFO - Training set processing completed in 9.86s
2024-12-28 14:23:03,058 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:23:03,059 - INFO - Memory usage at start_fit: CPU 1809.0 MB, GPU 111.6 MB
2024-12-28 14:23:03,059 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:23:03,592 - INFO - Fitted scaler and transformed data
2024-12-28 14:23:03,593 - INFO - Scaling time: 0.53s
2024-12-28 14:23:03,607 - INFO - Number of unique classes: 43
2024-12-28 14:23:10,902 - INFO - Epoch 1/10, Train Loss: 3.5727, Val Loss: 3.7606
2024-12-28 14:23:18,221 - INFO - Epoch 2/10, Train Loss: 3.5721, Val Loss: 3.7601
2024-12-28 14:23:24,698 - INFO - Epoch 3/10, Train Loss: 3.5715, Val Loss: 3.7595
2024-12-28 14:23:31,187 - INFO - Epoch 4/10, Train Loss: 3.5709, Val Loss: 3.7588
2024-12-28 14:23:31,187 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:23:31,187 - INFO - Training completed in 28.13s
2024-12-28 14:23:31,188 - INFO - Final memory usage: CPU 1809.0 MB, GPU 152.7 MB
2024-12-28 14:23:31,188 - INFO - Model training completed in 28.13s
2024-12-28 14:23:31,434 - INFO - Prediction completed in 0.25s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:23:31,447 - INFO - Poison rate 0.07 completed in 38.32s
2024-12-28 14:23:31,447 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:23:31,484 - INFO - Total number of labels flipped: 1975
2024-12-28 14:23:31,484 - INFO - Label flipping completed in 0.04s
2024-12-28 14:23:31,484 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:23:31,484 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:23:32,648 - INFO - Feature scaling completed in 1.16s
2024-12-28 14:23:32,648 - INFO - Starting feature selection (k=50)
2024-12-28 14:23:32,672 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:23:32,673 - INFO - Starting anomaly detection
2024-12-28 14:23:38,325 - INFO - Anomaly detection completed in 5.65s
2024-12-28 14:23:38,326 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:23:38,326 - INFO - Total fit_transform time: 6.84s
2024-12-28 14:23:38,326 - INFO - Training set processing completed in 6.84s
2024-12-28 14:23:38,326 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:23:38,327 - INFO - Memory usage at start_fit: CPU 1809.0 MB, GPU 111.6 MB
2024-12-28 14:23:38,327 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:23:38,919 - INFO - Fitted scaler and transformed data
2024-12-28 14:23:38,919 - INFO - Scaling time: 0.59s
2024-12-28 14:23:38,935 - INFO - Number of unique classes: 43
2024-12-28 14:23:45,208 - INFO - Epoch 1/10, Train Loss: 3.5721, Val Loss: 3.7607
2024-12-28 14:23:50,807 - INFO - Epoch 2/10, Train Loss: 3.5716, Val Loss: 3.7601
2024-12-28 14:23:57,044 - INFO - Epoch 3/10, Train Loss: 3.5710, Val Loss: 3.7596
2024-12-28 14:24:02,946 - INFO - Epoch 4/10, Train Loss: 3.5704, Val Loss: 3.7590
2024-12-28 14:24:02,946 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:24:02,946 - INFO - Training completed in 24.62s
2024-12-28 14:24:02,946 - INFO - Final memory usage: CPU 1809.0 MB, GPU 152.7 MB
2024-12-28 14:24:02,947 - INFO - Model training completed in 24.62s
2024-12-28 14:24:03,136 - INFO - Prediction completed in 0.19s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:24:03,149 - INFO - Poison rate 0.1 completed in 31.70s
2024-12-28 14:24:03,149 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:24:03,220 - INFO - Total number of labels flipped: 3951
2024-12-28 14:24:03,220 - INFO - Label flipping completed in 0.07s
2024-12-28 14:24:03,220 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:24:03,220 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:24:04,438 - INFO - Feature scaling completed in 1.22s
2024-12-28 14:24:04,438 - INFO - Starting feature selection (k=50)
2024-12-28 14:24:04,462 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:24:04,462 - INFO - Starting anomaly detection
2024-12-28 14:24:12,672 - INFO - Anomaly detection completed in 8.21s
2024-12-28 14:24:12,672 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:24:12,673 - INFO - Total fit_transform time: 9.45s
2024-12-28 14:24:12,673 - INFO - Training set processing completed in 9.45s
2024-12-28 14:24:12,673 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:24:12,674 - INFO - Memory usage at start_fit: CPU 1809.0 MB, GPU 111.6 MB
2024-12-28 14:24:12,674 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:24:13,248 - INFO - Fitted scaler and transformed data
2024-12-28 14:24:13,248 - INFO - Scaling time: 0.57s
2024-12-28 14:24:13,262 - INFO - Number of unique classes: 43
2024-12-28 14:24:20,034 - INFO - Epoch 1/10, Train Loss: 3.5721, Val Loss: 3.7608
2024-12-28 14:24:26,306 - INFO - Epoch 2/10, Train Loss: 3.5716, Val Loss: 3.7603
2024-12-28 14:24:33,475 - INFO - Epoch 3/10, Train Loss: 3.5711, Val Loss: 3.7599
2024-12-28 14:24:40,493 - INFO - Epoch 4/10, Train Loss: 3.5707, Val Loss: 3.7594
2024-12-28 14:24:40,494 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:24:40,494 - INFO - Training completed in 27.82s
2024-12-28 14:24:40,494 - INFO - Final memory usage: CPU 1809.0 MB, GPU 152.7 MB
2024-12-28 14:24:40,494 - INFO - Model training completed in 27.82s
2024-12-28 14:24:40,815 - INFO - Prediction completed in 0.32s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:24:40,828 - INFO - Poison rate 0.2 completed in 37.68s
2024-12-28 14:24:40,829 - INFO - Loaded 35 existing results
2024-12-28 14:24:40,829 - INFO - Total results to save: 42
2024-12-28 14:24:40,830 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:24:40,833 - INFO - Saved 42 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:24:40,833 - INFO - Total evaluation time: 268.75s
2024-12-28 14:24:40,838 - INFO - 
Progress: 7.3% - Evaluating GTSRB with KNeighbors (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:24:41,039 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:24:41,040 - INFO - Dataset type: image
2024-12-28 14:24:41,040 - INFO - Sample size: 39209
2024-12-28 14:24:41,040 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:24:41,042 - INFO - Loading datasets...
2024-12-28 14:25:00,701 - INFO - Dataset loading completed in 19.66s
2024-12-28 14:25:00,701 - INFO - Extracting validation features...
2024-12-28 14:25:00,701 - INFO - Extracting features from 4435 samples...
2024-12-28 14:25:01,427 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:25:01,430 - INFO - Validation feature extraction completed in 0.73s
2024-12-28 14:25:01,430 - INFO - Extracting training features...
2024-12-28 14:25:01,430 - INFO - Extracting features from 19755 samples...
2024-12-28 14:25:04,206 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:25:04,217 - INFO - Training feature extraction completed in 2.79s
2024-12-28 14:25:04,217 - INFO - Creating model for classifier: KNeighbors
2024-12-28 14:25:04,217 - INFO - Using device: cuda
2024-12-28 14:25:04,217 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:25:04,217 - INFO - Training set processing completed in 0.00s
2024-12-28 14:25:04,217 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:25:04,219 - INFO - Memory usage at start_fit: CPU 1845.7 MB, GPU 103.1 MB
2024-12-28 14:25:04,219 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:25:04,799 - INFO - Fitted scaler and transformed data
2024-12-28 14:25:04,799 - INFO - Scaling time: 0.58s
2024-12-28 14:25:04,823 - INFO - Training completed in 0.61s
2024-12-28 14:25:04,824 - INFO - Final memory usage: CPU 1845.7 MB, GPU 141.9 MB
2024-12-28 14:25:04,825 - INFO - Model training completed in 0.61s
2024-12-28 14:25:05,178 - INFO - Prediction completed in 0.35s
2024-12-28 14:25:05,189 - INFO - Poison rate 0.0 completed in 0.97s
2024-12-28 14:25:05,189 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:25:05,195 - INFO - Total number of labels flipped: 197
2024-12-28 14:25:05,195 - INFO - Label flipping completed in 0.01s
2024-12-28 14:25:05,195 - INFO - Training set processing completed in 0.00s
2024-12-28 14:25:05,195 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:25:05,196 - INFO - Memory usage at start_fit: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:05,196 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:25:05,735 - INFO - Fitted scaler and transformed data
2024-12-28 14:25:05,735 - INFO - Scaling time: 0.54s
2024-12-28 14:25:05,748 - INFO - Training completed in 0.55s
2024-12-28 14:25:05,749 - INFO - Final memory usage: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:05,749 - INFO - Model training completed in 0.55s
2024-12-28 14:25:05,844 - INFO - Prediction completed in 0.09s
2024-12-28 14:25:05,854 - INFO - Poison rate 0.01 completed in 0.66s
2024-12-28 14:25:05,855 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:25:05,866 - INFO - Total number of labels flipped: 592
2024-12-28 14:25:05,866 - INFO - Label flipping completed in 0.01s
2024-12-28 14:25:05,866 - INFO - Training set processing completed in 0.00s
2024-12-28 14:25:05,867 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:25:05,867 - INFO - Memory usage at start_fit: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:05,867 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:25:06,434 - INFO - Fitted scaler and transformed data
2024-12-28 14:25:06,434 - INFO - Scaling time: 0.57s
2024-12-28 14:25:06,447 - INFO - Training completed in 0.58s
2024-12-28 14:25:06,448 - INFO - Final memory usage: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:06,448 - INFO - Model training completed in 0.58s
2024-12-28 14:25:06,539 - INFO - Prediction completed in 0.09s
2024-12-28 14:25:06,549 - INFO - Poison rate 0.03 completed in 0.69s
2024-12-28 14:25:06,549 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:25:06,570 - INFO - Total number of labels flipped: 987
2024-12-28 14:25:06,570 - INFO - Label flipping completed in 0.02s
2024-12-28 14:25:06,570 - INFO - Training set processing completed in 0.00s
2024-12-28 14:25:06,570 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:25:06,571 - INFO - Memory usage at start_fit: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:06,571 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:25:07,151 - INFO - Fitted scaler and transformed data
2024-12-28 14:25:07,151 - INFO - Scaling time: 0.58s
2024-12-28 14:25:07,168 - INFO - Training completed in 0.60s
2024-12-28 14:25:07,169 - INFO - Final memory usage: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:07,170 - INFO - Model training completed in 0.60s
2024-12-28 14:25:07,306 - INFO - Prediction completed in 0.14s
2024-12-28 14:25:07,317 - INFO - Poison rate 0.05 completed in 0.77s
2024-12-28 14:25:07,317 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:25:07,342 - INFO - Total number of labels flipped: 1382
2024-12-28 14:25:07,342 - INFO - Label flipping completed in 0.03s
2024-12-28 14:25:07,343 - INFO - Training set processing completed in 0.00s
2024-12-28 14:25:07,343 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:25:07,344 - INFO - Memory usage at start_fit: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:07,344 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:25:07,927 - INFO - Fitted scaler and transformed data
2024-12-28 14:25:07,928 - INFO - Scaling time: 0.58s
2024-12-28 14:25:07,940 - INFO - Training completed in 0.60s
2024-12-28 14:25:07,941 - INFO - Final memory usage: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:07,941 - INFO - Model training completed in 0.60s
2024-12-28 14:25:08,060 - INFO - Prediction completed in 0.12s
2024-12-28 14:25:08,071 - INFO - Poison rate 0.07 completed in 0.75s
2024-12-28 14:25:08,071 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:25:08,107 - INFO - Total number of labels flipped: 1975
2024-12-28 14:25:08,107 - INFO - Label flipping completed in 0.04s
2024-12-28 14:25:08,107 - INFO - Training set processing completed in 0.00s
2024-12-28 14:25:08,107 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:25:08,108 - INFO - Memory usage at start_fit: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:08,108 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:25:08,620 - INFO - Fitted scaler and transformed data
2024-12-28 14:25:08,620 - INFO - Scaling time: 0.51s
2024-12-28 14:25:08,633 - INFO - Training completed in 0.53s
2024-12-28 14:25:08,633 - INFO - Final memory usage: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:08,633 - INFO - Model training completed in 0.53s
2024-12-28 14:25:08,786 - INFO - Prediction completed in 0.15s
2024-12-28 14:25:08,796 - INFO - Poison rate 0.1 completed in 0.73s
2024-12-28 14:25:08,796 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:25:08,874 - INFO - Total number of labels flipped: 3951
2024-12-28 14:25:08,874 - INFO - Label flipping completed in 0.08s
2024-12-28 14:25:08,874 - INFO - Training set processing completed in 0.00s
2024-12-28 14:25:08,874 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:25:08,875 - INFO - Memory usage at start_fit: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:08,875 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:25:09,506 - INFO - Fitted scaler and transformed data
2024-12-28 14:25:09,507 - INFO - Scaling time: 0.63s
2024-12-28 14:25:09,523 - INFO - Training completed in 0.65s
2024-12-28 14:25:09,525 - INFO - Final memory usage: CPU 1845.9 MB, GPU 141.9 MB
2024-12-28 14:25:09,525 - INFO - Model training completed in 0.65s
2024-12-28 14:25:09,871 - INFO - Prediction completed in 0.35s
2024-12-28 14:25:09,881 - INFO - Poison rate 0.2 completed in 1.09s
2024-12-28 14:25:09,883 - INFO - Loaded 42 existing results
2024-12-28 14:25:09,883 - INFO - Total results to save: 49
2024-12-28 14:25:09,884 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:25:09,888 - INFO - Saved 49 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:25:09,888 - INFO - Total evaluation time: 28.85s
2024-12-28 14:25:09,894 - INFO - 
Progress: 8.3% - Evaluating GTSRB with KNeighbors (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:25:10,083 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:25:10,084 - INFO - Dataset type: image
2024-12-28 14:25:10,084 - INFO - Sample size: 39209
2024-12-28 14:25:10,084 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:25:10,084 - INFO - Loading datasets...
2024-12-28 14:25:29,839 - INFO - Dataset loading completed in 19.75s
2024-12-28 14:25:29,839 - INFO - Extracting validation features...
2024-12-28 14:25:29,839 - INFO - Extracting features from 4435 samples...
2024-12-28 14:25:30,605 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:25:30,608 - INFO - Validation feature extraction completed in 0.77s
2024-12-28 14:25:30,608 - INFO - Extracting training features...
2024-12-28 14:25:30,608 - INFO - Extracting features from 19755 samples...
2024-12-28 14:25:33,339 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:25:33,353 - INFO - Training feature extraction completed in 2.75s
2024-12-28 14:25:33,353 - INFO - Creating model for classifier: KNeighbors
2024-12-28 14:25:33,354 - INFO - Using device: cuda
2024-12-28 14:25:33,354 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:25:33,354 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:25:33,354 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:25:34,586 - INFO - Feature scaling completed in 1.23s
2024-12-28 14:25:34,587 - INFO - Starting feature selection (k=50)
2024-12-28 14:25:34,616 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:25:34,616 - INFO - Starting anomaly detection
2024-12-28 14:25:42,767 - INFO - Anomaly detection completed in 8.15s
2024-12-28 14:25:42,767 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:25:42,767 - INFO - Total fit_transform time: 9.41s
2024-12-28 14:25:42,767 - INFO - Training set processing completed in 9.41s
2024-12-28 14:25:42,767 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:25:42,768 - INFO - Memory usage at start_fit: CPU 1809.3 MB, GPU 102.5 MB
2024-12-28 14:25:42,768 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:25:43,304 - INFO - Fitted scaler and transformed data
2024-12-28 14:25:43,304 - INFO - Scaling time: 0.54s
2024-12-28 14:25:43,322 - INFO - Training completed in 0.55s
2024-12-28 14:25:43,323 - INFO - Final memory usage: CPU 1809.3 MB, GPU 141.3 MB
2024-12-28 14:25:43,323 - INFO - Model training completed in 0.56s
2024-12-28 14:25:43,639 - INFO - Prediction completed in 0.32s
2024-12-28 14:25:43,664 - INFO - Poison rate 0.0 completed in 10.31s
2024-12-28 14:25:43,664 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:25:43,676 - INFO - Total number of labels flipped: 197
2024-12-28 14:25:43,676 - INFO - Label flipping completed in 0.01s
2024-12-28 14:25:43,676 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:25:43,676 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:25:44,977 - INFO - Feature scaling completed in 1.30s
2024-12-28 14:25:44,978 - INFO - Starting feature selection (k=50)
2024-12-28 14:25:45,007 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:25:45,008 - INFO - Starting anomaly detection
2024-12-28 14:25:52,952 - INFO - Anomaly detection completed in 7.94s
2024-12-28 14:25:52,952 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:25:52,952 - INFO - Total fit_transform time: 9.28s
2024-12-28 14:25:52,952 - INFO - Training set processing completed in 9.28s
2024-12-28 14:25:52,952 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:25:52,953 - INFO - Memory usage at start_fit: CPU 1809.7 MB, GPU 141.3 MB
2024-12-28 14:25:52,953 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:25:53,508 - INFO - Fitted scaler and transformed data
2024-12-28 14:25:53,508 - INFO - Scaling time: 0.55s
2024-12-28 14:25:53,522 - INFO - Training completed in 0.57s
2024-12-28 14:25:53,522 - INFO - Final memory usage: CPU 1809.7 MB, GPU 141.3 MB
2024-12-28 14:25:53,522 - INFO - Model training completed in 0.57s
2024-12-28 14:25:53,842 - INFO - Prediction completed in 0.32s
2024-12-28 14:25:53,854 - INFO - Poison rate 0.01 completed in 10.19s
2024-12-28 14:25:53,854 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:25:53,865 - INFO - Total number of labels flipped: 592
2024-12-28 14:25:53,866 - INFO - Label flipping completed in 0.01s
2024-12-28 14:25:53,866 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:25:53,866 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:25:55,066 - INFO - Feature scaling completed in 1.20s
2024-12-28 14:25:55,066 - INFO - Starting feature selection (k=50)
2024-12-28 14:25:55,084 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:25:55,085 - INFO - Starting anomaly detection
2024-12-28 14:26:01,671 - INFO - Anomaly detection completed in 6.59s
2024-12-28 14:26:01,671 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:26:01,671 - INFO - Total fit_transform time: 7.81s
2024-12-28 14:26:01,671 - INFO - Training set processing completed in 7.81s
2024-12-28 14:26:01,671 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:26:01,672 - INFO - Memory usage at start_fit: CPU 1809.7 MB, GPU 141.3 MB
2024-12-28 14:26:01,672 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:26:02,240 - INFO - Fitted scaler and transformed data
2024-12-28 14:26:02,240 - INFO - Scaling time: 0.57s
2024-12-28 14:26:02,254 - INFO - Training completed in 0.58s
2024-12-28 14:26:02,255 - INFO - Final memory usage: CPU 1809.7 MB, GPU 141.3 MB
2024-12-28 14:26:02,255 - INFO - Model training completed in 0.58s
2024-12-28 14:26:02,533 - INFO - Prediction completed in 0.28s
2024-12-28 14:26:02,544 - INFO - Poison rate 0.03 completed in 8.69s
2024-12-28 14:26:02,544 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:26:02,572 - INFO - Total number of labels flipped: 987
2024-12-28 14:26:02,572 - INFO - Label flipping completed in 0.03s
2024-12-28 14:26:02,572 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:26:02,573 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:26:03,906 - INFO - Feature scaling completed in 1.33s
2024-12-28 14:26:03,906 - INFO - Starting feature selection (k=50)
2024-12-28 14:26:03,920 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 14:26:03,921 - INFO - Starting anomaly detection
2024-12-28 14:26:09,193 - INFO - Anomaly detection completed in 5.27s
2024-12-28 14:26:09,193 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:26:09,193 - INFO - Total fit_transform time: 6.62s
2024-12-28 14:26:09,193 - INFO - Training set processing completed in 6.62s
2024-12-28 14:26:09,193 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:26:09,194 - INFO - Memory usage at start_fit: CPU 1809.7 MB, GPU 141.3 MB
2024-12-28 14:26:09,195 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:26:09,742 - INFO - Fitted scaler and transformed data
2024-12-28 14:26:09,742 - INFO - Scaling time: 0.55s
2024-12-28 14:26:09,756 - INFO - Training completed in 0.56s
2024-12-28 14:26:09,757 - INFO - Final memory usage: CPU 1809.7 MB, GPU 141.3 MB
2024-12-28 14:26:09,757 - INFO - Model training completed in 0.56s
2024-12-28 14:26:10,078 - INFO - Prediction completed in 0.32s
2024-12-28 14:26:10,088 - INFO - Poison rate 0.05 completed in 7.54s
2024-12-28 14:26:10,088 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:26:10,114 - INFO - Total number of labels flipped: 1382
2024-12-28 14:26:10,114 - INFO - Label flipping completed in 0.03s
2024-12-28 14:26:10,114 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:26:10,114 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:26:11,315 - INFO - Feature scaling completed in 1.20s
2024-12-28 14:26:11,315 - INFO - Starting feature selection (k=50)
2024-12-28 14:26:11,334 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:26:11,335 - INFO - Starting anomaly detection
2024-12-28 14:26:18,101 - INFO - Anomaly detection completed in 6.77s
2024-12-28 14:26:18,101 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:26:18,101 - INFO - Total fit_transform time: 7.99s
2024-12-28 14:26:18,101 - INFO - Training set processing completed in 7.99s
2024-12-28 14:26:18,101 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:26:18,103 - INFO - Memory usage at start_fit: CPU 1809.7 MB, GPU 141.3 MB
2024-12-28 14:26:18,103 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:26:18,698 - INFO - Fitted scaler and transformed data
2024-12-28 14:26:18,698 - INFO - Scaling time: 0.59s
2024-12-28 14:26:18,711 - INFO - Training completed in 0.61s
2024-12-28 14:26:18,712 - INFO - Final memory usage: CPU 1809.7 MB, GPU 141.3 MB
2024-12-28 14:26:18,712 - INFO - Model training completed in 0.61s
2024-12-28 14:26:19,028 - INFO - Prediction completed in 0.32s
2024-12-28 14:26:19,039 - INFO - Poison rate 0.07 completed in 8.95s
2024-12-28 14:26:19,039 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:26:19,074 - INFO - Total number of labels flipped: 1975
2024-12-28 14:26:19,074 - INFO - Label flipping completed in 0.04s
2024-12-28 14:26:19,074 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:26:19,074 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:26:20,336 - INFO - Feature scaling completed in 1.26s
2024-12-28 14:26:20,336 - INFO - Starting feature selection (k=50)
2024-12-28 14:26:20,357 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:26:20,357 - INFO - Starting anomaly detection
2024-12-28 14:26:28,117 - INFO - Anomaly detection completed in 7.76s
2024-12-28 14:26:28,117 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:26:28,118 - INFO - Total fit_transform time: 9.04s
2024-12-28 14:26:28,118 - INFO - Training set processing completed in 9.04s
2024-12-28 14:26:28,118 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:26:28,119 - INFO - Memory usage at start_fit: CPU 1809.8 MB, GPU 141.3 MB
2024-12-28 14:26:28,120 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:26:28,718 - INFO - Fitted scaler and transformed data
2024-12-28 14:26:28,719 - INFO - Scaling time: 0.60s
2024-12-28 14:26:28,732 - INFO - Training completed in 0.61s
2024-12-28 14:26:28,732 - INFO - Final memory usage: CPU 1809.8 MB, GPU 141.3 MB
2024-12-28 14:26:28,732 - INFO - Model training completed in 0.61s
2024-12-28 14:26:29,048 - INFO - Prediction completed in 0.32s
2024-12-28 14:26:29,059 - INFO - Poison rate 0.1 completed in 10.02s
2024-12-28 14:26:29,059 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:26:29,128 - INFO - Total number of labels flipped: 3951
2024-12-28 14:26:29,128 - INFO - Label flipping completed in 0.07s
2024-12-28 14:26:29,128 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:26:29,128 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:26:30,340 - INFO - Feature scaling completed in 1.21s
2024-12-28 14:26:30,340 - INFO - Starting feature selection (k=50)
2024-12-28 14:26:30,354 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 14:26:30,354 - INFO - Starting anomaly detection
2024-12-28 14:26:38,305 - INFO - Anomaly detection completed in 7.95s
2024-12-28 14:26:38,305 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:26:38,305 - INFO - Total fit_transform time: 9.18s
2024-12-28 14:26:38,305 - INFO - Training set processing completed in 9.18s
2024-12-28 14:26:38,305 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:26:38,307 - INFO - Memory usage at start_fit: CPU 1809.9 MB, GPU 141.3 MB
2024-12-28 14:26:38,307 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:26:38,932 - INFO - Fitted scaler and transformed data
2024-12-28 14:26:38,932 - INFO - Scaling time: 0.62s
2024-12-28 14:26:38,951 - INFO - Training completed in 0.65s
2024-12-28 14:26:38,952 - INFO - Final memory usage: CPU 1809.9 MB, GPU 141.3 MB
2024-12-28 14:26:38,952 - INFO - Model training completed in 0.65s
2024-12-28 14:26:39,312 - INFO - Prediction completed in 0.36s
2024-12-28 14:26:39,322 - INFO - Poison rate 0.2 completed in 10.26s
2024-12-28 14:26:39,324 - INFO - Loaded 49 existing results
2024-12-28 14:26:39,324 - INFO - Total results to save: 56
2024-12-28 14:26:39,325 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:26:39,329 - INFO - Saved 56 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:26:39,329 - INFO - Total evaluation time: 89.24s
2024-12-28 14:26:39,335 - INFO - Completed evaluation for GTSRB
2024-12-28 14:26:39,335 - INFO - 
Processing dataset: GTSRB
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:26:39,530 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:26:39,530 - INFO - Dataset type: image
2024-12-28 14:26:39,530 - INFO - Sample size: 39209
2024-12-28 14:26:39,530 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:26:39,531 - INFO - 
Progress: 9.4% - Evaluating GTSRB with SVM (standard mode, iteration 1/1)
2024-12-28 14:26:39,714 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:26:39,714 - INFO - Dataset type: image
2024-12-28 14:26:39,714 - INFO - Sample size: 39209
2024-12-28 14:26:39,714 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:26:39,715 - INFO - Loading datasets...
2024-12-28 14:26:58,176 - INFO - Dataset loading completed in 18.46s
2024-12-28 14:26:58,176 - INFO - Extracting validation features...
2024-12-28 14:26:58,176 - INFO - Extracting features from 4435 samples...
2024-12-28 14:26:58,926 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:26:58,932 - INFO - Validation feature extraction completed in 0.76s
2024-12-28 14:26:58,933 - INFO - Extracting training features...
2024-12-28 14:26:58,933 - INFO - Extracting features from 19755 samples...
2024-12-28 14:27:01,592 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:27:01,604 - INFO - Training feature extraction completed in 2.67s
2024-12-28 14:27:01,604 - INFO - Creating model for classifier: SVM
2024-12-28 14:27:01,605 - INFO - Using device: cuda
2024-12-28 14:27:01,605 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 14:27:01,605 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:27:01,605 - INFO - Training set processing completed in 0.00s
2024-12-28 14:27:01,605 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:27:01,606 - INFO - Memory usage at start_fit: CPU 1790.8 MB, GPU 104.0 MB
2024-12-28 14:27:01,606 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:27:01,614 - INFO - Number of unique classes: 43
2024-12-28 14:27:01,757 - INFO - Fitted scaler and transformed data
2024-12-28 14:27:01,757 - INFO - Scaling time: 0.14s
2024-12-28 14:27:02,844 - INFO - Epoch 1/500, Train Loss: 6.8322, Val Loss: 3.2646
2024-12-28 14:27:03,974 - INFO - Epoch 2/500, Train Loss: 2.5104, Val Loss: 2.2518
2024-12-28 14:27:05,089 - INFO - Epoch 3/500, Train Loss: 1.8205, Val Loss: 1.8115
2024-12-28 14:27:06,231 - INFO - Epoch 4/500, Train Loss: 1.4718, Val Loss: 1.6074
2024-12-28 14:27:07,414 - INFO - Epoch 5/500, Train Loss: 1.2569, Val Loss: 1.4632
2024-12-28 14:27:08,637 - INFO - Epoch 6/500, Train Loss: 1.1064, Val Loss: 1.3186
2024-12-28 14:27:09,803 - INFO - Epoch 7/500, Train Loss: 0.9994, Val Loss: 1.2488
2024-12-28 14:27:10,967 - INFO - Epoch 8/500, Train Loss: 0.9154, Val Loss: 1.1872
2024-12-28 14:27:12,068 - INFO - Epoch 9/500, Train Loss: 0.8486, Val Loss: 1.1200
2024-12-28 14:27:13,150 - INFO - Epoch 10/500, Train Loss: 0.7918, Val Loss: 1.1230
2024-12-28 14:27:14,243 - INFO - Epoch 11/500, Train Loss: 0.7500, Val Loss: 1.0317
2024-12-28 14:27:15,323 - INFO - Epoch 12/500, Train Loss: 0.7104, Val Loss: 1.0305
2024-12-28 14:27:16,416 - INFO - Epoch 13/500, Train Loss: 0.6757, Val Loss: 0.9813
2024-12-28 14:27:17,511 - INFO - Epoch 14/500, Train Loss: 0.6510, Val Loss: 0.9698
2024-12-28 14:27:18,582 - INFO - Epoch 15/500, Train Loss: 0.6266, Val Loss: 0.9665
2024-12-28 14:27:19,700 - INFO - Epoch 16/500, Train Loss: 0.6023, Val Loss: 0.9225
2024-12-28 14:27:20,921 - INFO - Epoch 17/500, Train Loss: 0.5900, Val Loss: 0.9107
2024-12-28 14:27:22,118 - INFO - Epoch 18/500, Train Loss: 0.5714, Val Loss: 0.9154
2024-12-28 14:27:23,284 - INFO - Epoch 19/500, Train Loss: 0.5615, Val Loss: 0.8735
2024-12-28 14:27:24,481 - INFO - Epoch 20/500, Train Loss: 0.5444, Val Loss: 0.8954
2024-12-28 14:27:25,584 - INFO - Epoch 21/500, Train Loss: 0.5379, Val Loss: 0.9011
2024-12-28 14:27:26,674 - INFO - Epoch 22/500, Train Loss: 0.5271, Val Loss: 0.8839
2024-12-28 14:27:27,803 - INFO - Epoch 23/500, Train Loss: 0.5175, Val Loss: 0.8662
2024-12-28 14:27:28,888 - INFO - Epoch 24/500, Train Loss: 0.5087, Val Loss: 0.8397
2024-12-28 14:27:29,996 - INFO - Epoch 25/500, Train Loss: 0.5062, Val Loss: 0.8571
2024-12-28 14:27:31,076 - INFO - Epoch 26/500, Train Loss: 0.5012, Val Loss: 0.8353
2024-12-28 14:27:32,210 - INFO - Epoch 27/500, Train Loss: 0.4906, Val Loss: 0.8273
2024-12-28 14:27:33,319 - INFO - Epoch 28/500, Train Loss: 0.4853, Val Loss: 0.8284
2024-12-28 14:27:34,408 - INFO - Epoch 29/500, Train Loss: 0.4809, Val Loss: 0.8407
2024-12-28 14:27:35,651 - INFO - Epoch 30/500, Train Loss: 0.4747, Val Loss: 0.8266
2024-12-28 14:27:36,805 - INFO - Epoch 31/500, Train Loss: 0.4739, Val Loss: 0.8205
2024-12-28 14:27:37,876 - INFO - Epoch 32/500, Train Loss: 0.4723, Val Loss: 0.8107
2024-12-28 14:27:38,928 - INFO - Epoch 33/500, Train Loss: 0.4601, Val Loss: 0.8451
2024-12-28 14:27:40,116 - INFO - Epoch 34/500, Train Loss: 0.4684, Val Loss: 0.8297
2024-12-28 14:27:41,190 - INFO - Epoch 35/500, Train Loss: 0.4622, Val Loss: 0.8057
2024-12-28 14:27:42,290 - INFO - Epoch 36/500, Train Loss: 0.4595, Val Loss: 0.8303
2024-12-28 14:27:43,396 - INFO - Epoch 37/500, Train Loss: 0.4620, Val Loss: 0.8334
2024-12-28 14:27:44,516 - INFO - Epoch 38/500, Train Loss: 0.4574, Val Loss: 0.8026
2024-12-28 14:27:45,812 - INFO - Epoch 39/500, Train Loss: 0.4552, Val Loss: 0.8216
2024-12-28 14:27:46,953 - INFO - Epoch 40/500, Train Loss: 0.4522, Val Loss: 0.7913
2024-12-28 14:27:48,153 - INFO - Epoch 41/500, Train Loss: 0.4567, Val Loss: 0.8597
2024-12-28 14:27:49,339 - INFO - Epoch 42/500, Train Loss: 0.4486, Val Loss: 0.7964
2024-12-28 14:27:50,488 - INFO - Epoch 43/500, Train Loss: 0.4499, Val Loss: 0.8279
2024-12-28 14:27:51,646 - INFO - Epoch 44/500, Train Loss: 0.4566, Val Loss: 0.8171
2024-12-28 14:27:52,813 - INFO - Epoch 45/500, Train Loss: 0.4506, Val Loss: 0.8163
2024-12-28 14:27:52,813 - INFO - Early stopping triggered at epoch 45
2024-12-28 14:27:52,813 - INFO - Training completed in 51.21s
2024-12-28 14:27:52,813 - INFO - Final memory usage: CPU 1840.9 MB, GPU 104.5 MB
2024-12-28 14:27:52,814 - INFO - Model training completed in 51.21s
2024-12-28 14:27:52,889 - INFO - Prediction completed in 0.08s
2024-12-28 14:27:52,907 - INFO - Poison rate 0.0 completed in 51.30s
2024-12-28 14:27:52,908 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:27:52,912 - INFO - Total number of labels flipped: 193
2024-12-28 14:27:52,912 - INFO - Label flipping completed in 0.00s
2024-12-28 14:27:52,912 - INFO - Training set processing completed in 0.00s
2024-12-28 14:27:52,913 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:27:52,914 - INFO - Memory usage at start_fit: CPU 1802.3 MB, GPU 104.3 MB
2024-12-28 14:27:52,914 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:27:52,920 - INFO - Number of unique classes: 43
2024-12-28 14:27:53,063 - INFO - Fitted scaler and transformed data
2024-12-28 14:27:53,064 - INFO - Scaling time: 0.14s
2024-12-28 14:27:54,263 - INFO - Epoch 1/500, Train Loss: 7.5597, Val Loss: 3.6654
2024-12-28 14:27:55,388 - INFO - Epoch 2/500, Train Loss: 2.9536, Val Loss: 2.6785
2024-12-28 14:27:56,593 - INFO - Epoch 3/500, Train Loss: 2.1852, Val Loss: 2.2844
2024-12-28 14:27:57,758 - INFO - Epoch 4/500, Train Loss: 1.7935, Val Loss: 2.0027
2024-12-28 14:27:58,922 - INFO - Epoch 5/500, Train Loss: 1.5406, Val Loss: 1.8184
2024-12-28 14:28:00,077 - INFO - Epoch 6/500, Train Loss: 1.3553, Val Loss: 1.7097
2024-12-28 14:28:01,109 - INFO - Epoch 7/500, Train Loss: 1.2219, Val Loss: 1.6060
2024-12-28 14:28:02,151 - INFO - Epoch 8/500, Train Loss: 1.1165, Val Loss: 1.5781
2024-12-28 14:28:03,239 - INFO - Epoch 9/500, Train Loss: 1.0406, Val Loss: 1.5264
2024-12-28 14:28:04,385 - INFO - Epoch 10/500, Train Loss: 0.9701, Val Loss: 1.4221
2024-12-28 14:28:05,656 - INFO - Epoch 11/500, Train Loss: 0.9094, Val Loss: 1.3800
2024-12-28 14:28:06,846 - INFO - Epoch 12/500, Train Loss: 0.8672, Val Loss: 1.3589
2024-12-28 14:28:08,057 - INFO - Epoch 13/500, Train Loss: 0.8181, Val Loss: 1.3171
2024-12-28 14:28:09,272 - INFO - Epoch 14/500, Train Loss: 0.7876, Val Loss: 1.2781
2024-12-28 14:28:10,441 - INFO - Epoch 15/500, Train Loss: 0.7619, Val Loss: 1.2752
2024-12-28 14:28:11,609 - INFO - Epoch 16/500, Train Loss: 0.7333, Val Loss: 1.2686
2024-12-28 14:28:12,861 - INFO - Epoch 17/500, Train Loss: 0.7140, Val Loss: 1.2263
2024-12-28 14:28:14,041 - INFO - Epoch 18/500, Train Loss: 0.6930, Val Loss: 1.2306
2024-12-28 14:28:15,225 - INFO - Epoch 19/500, Train Loss: 0.6848, Val Loss: 1.1973
2024-12-28 14:28:16,470 - INFO - Epoch 20/500, Train Loss: 0.6651, Val Loss: 1.1801
2024-12-28 14:28:17,703 - INFO - Epoch 21/500, Train Loss: 0.6508, Val Loss: 1.1463
2024-12-28 14:28:18,874 - INFO - Epoch 22/500, Train Loss: 0.6362, Val Loss: 1.1517
2024-12-28 14:28:19,916 - INFO - Epoch 23/500, Train Loss: 0.6208, Val Loss: 1.1344
2024-12-28 14:28:21,024 - INFO - Epoch 24/500, Train Loss: 0.6154, Val Loss: 1.1175
2024-12-28 14:28:22,186 - INFO - Epoch 25/500, Train Loss: 0.6075, Val Loss: 1.1477
2024-12-28 14:28:23,404 - INFO - Epoch 26/500, Train Loss: 0.5988, Val Loss: 1.1025
2024-12-28 14:28:24,635 - INFO - Epoch 27/500, Train Loss: 0.5952, Val Loss: 1.0970
2024-12-28 14:28:25,857 - INFO - Epoch 28/500, Train Loss: 0.5832, Val Loss: 1.1415
2024-12-28 14:28:27,101 - INFO - Epoch 29/500, Train Loss: 0.5806, Val Loss: 1.1153
2024-12-28 14:28:28,311 - INFO - Epoch 30/500, Train Loss: 0.5718, Val Loss: 1.0900
2024-12-28 14:28:29,448 - INFO - Epoch 31/500, Train Loss: 0.5733, Val Loss: 1.0672
2024-12-28 14:28:30,583 - INFO - Epoch 32/500, Train Loss: 0.5691, Val Loss: 1.0646
2024-12-28 14:28:31,736 - INFO - Epoch 33/500, Train Loss: 0.5609, Val Loss: 1.0782
2024-12-28 14:28:32,969 - INFO - Epoch 34/500, Train Loss: 0.5556, Val Loss: 1.0666
2024-12-28 14:28:34,125 - INFO - Epoch 35/500, Train Loss: 0.5491, Val Loss: 1.0871
2024-12-28 14:28:35,289 - INFO - Epoch 36/500, Train Loss: 0.5543, Val Loss: 1.0856
2024-12-28 14:28:36,467 - INFO - Epoch 37/500, Train Loss: 0.5514, Val Loss: 1.0523
2024-12-28 14:28:37,568 - INFO - Epoch 38/500, Train Loss: 0.5417, Val Loss: 1.0520
2024-12-28 14:28:38,666 - INFO - Epoch 39/500, Train Loss: 0.5418, Val Loss: 1.0651
2024-12-28 14:28:39,707 - INFO - Epoch 40/500, Train Loss: 0.5433, Val Loss: 1.0755
2024-12-28 14:28:40,823 - INFO - Epoch 41/500, Train Loss: 0.5426, Val Loss: 1.0469
2024-12-28 14:28:41,949 - INFO - Epoch 42/500, Train Loss: 0.5356, Val Loss: 1.0286
2024-12-28 14:28:43,012 - INFO - Epoch 43/500, Train Loss: 0.5368, Val Loss: 1.0546
2024-12-28 14:28:44,144 - INFO - Epoch 44/500, Train Loss: 0.5336, Val Loss: 1.0404
2024-12-28 14:28:45,547 - INFO - Epoch 45/500, Train Loss: 0.5348, Val Loss: 1.0517
2024-12-28 14:28:46,773 - INFO - Epoch 46/500, Train Loss: 0.5322, Val Loss: 1.0929
2024-12-28 14:28:47,855 - INFO - Epoch 47/500, Train Loss: 0.5310, Val Loss: 1.0327
2024-12-28 14:28:47,855 - INFO - Early stopping triggered at epoch 47
2024-12-28 14:28:47,855 - INFO - Training completed in 54.94s
2024-12-28 14:28:47,855 - INFO - Final memory usage: CPU 1844.7 MB, GPU 104.5 MB
2024-12-28 14:28:47,856 - INFO - Model training completed in 54.94s
2024-12-28 14:28:47,920 - INFO - Prediction completed in 0.06s
2024-12-28 14:28:47,931 - INFO - Poison rate 0.01 completed in 55.02s
2024-12-28 14:28:47,931 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:28:47,933 - INFO - Total number of labels flipped: 588
2024-12-28 14:28:47,933 - INFO - Label flipping completed in 0.00s
2024-12-28 14:28:47,933 - INFO - Training set processing completed in 0.00s
2024-12-28 14:28:47,933 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:28:47,934 - INFO - Memory usage at start_fit: CPU 1806.1 MB, GPU 104.3 MB
2024-12-28 14:28:47,934 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:28:47,940 - INFO - Number of unique classes: 43
2024-12-28 14:28:48,104 - INFO - Fitted scaler and transformed data
2024-12-28 14:28:48,105 - INFO - Scaling time: 0.16s
2024-12-28 14:28:49,339 - INFO - Epoch 1/500, Train Loss: 8.3554, Val Loss: 4.7121
2024-12-28 14:28:50,522 - INFO - Epoch 2/500, Train Loss: 3.7331, Val Loss: 3.5937
2024-12-28 14:28:51,710 - INFO - Epoch 3/500, Train Loss: 2.8742, Val Loss: 3.0254
2024-12-28 14:28:52,862 - INFO - Epoch 4/500, Train Loss: 2.3453, Val Loss: 2.6337
2024-12-28 14:28:53,996 - INFO - Epoch 5/500, Train Loss: 2.0023, Val Loss: 2.3901
2024-12-28 14:28:55,260 - INFO - Epoch 6/500, Train Loss: 1.7550, Val Loss: 2.2228
2024-12-28 14:28:56,467 - INFO - Epoch 7/500, Train Loss: 1.5647, Val Loss: 2.0837
2024-12-28 14:28:57,589 - INFO - Epoch 8/500, Train Loss: 1.4186, Val Loss: 1.9555
2024-12-28 14:28:58,700 - INFO - Epoch 9/500, Train Loss: 1.3150, Val Loss: 1.8944
2024-12-28 14:28:59,775 - INFO - Epoch 10/500, Train Loss: 1.2150, Val Loss: 1.8113
2024-12-28 14:29:00,890 - INFO - Epoch 11/500, Train Loss: 1.1418, Val Loss: 1.7810
2024-12-28 14:29:02,021 - INFO - Epoch 12/500, Train Loss: 1.0797, Val Loss: 1.7034
2024-12-28 14:29:03,152 - INFO - Epoch 13/500, Train Loss: 1.0228, Val Loss: 1.6663
2024-12-28 14:29:04,306 - INFO - Epoch 14/500, Train Loss: 0.9765, Val Loss: 1.6337
2024-12-28 14:29:05,360 - INFO - Epoch 15/500, Train Loss: 0.9529, Val Loss: 1.5934
2024-12-28 14:29:06,433 - INFO - Epoch 16/500, Train Loss: 0.9151, Val Loss: 1.5633
2024-12-28 14:29:07,608 - INFO - Epoch 17/500, Train Loss: 0.8821, Val Loss: 1.5155
2024-12-28 14:29:08,622 - INFO - Epoch 18/500, Train Loss: 0.8588, Val Loss: 1.5283
2024-12-28 14:29:09,716 - INFO - Epoch 19/500, Train Loss: 0.8374, Val Loss: 1.4861
2024-12-28 14:29:10,692 - INFO - Epoch 20/500, Train Loss: 0.8212, Val Loss: 1.4630
2024-12-28 14:29:11,719 - INFO - Epoch 21/500, Train Loss: 0.8036, Val Loss: 1.4607
2024-12-28 14:29:12,733 - INFO - Epoch 22/500, Train Loss: 0.7895, Val Loss: 1.4580
2024-12-28 14:29:13,885 - INFO - Epoch 23/500, Train Loss: 0.7785, Val Loss: 1.4300
2024-12-28 14:29:15,052 - INFO - Epoch 24/500, Train Loss: 0.7615, Val Loss: 1.3901
2024-12-28 14:29:16,127 - INFO - Epoch 25/500, Train Loss: 0.7521, Val Loss: 1.3921
2024-12-28 14:29:17,355 - INFO - Epoch 26/500, Train Loss: 0.7422, Val Loss: 1.3571
2024-12-28 14:29:18,618 - INFO - Epoch 27/500, Train Loss: 0.7353, Val Loss: 1.3936
2024-12-28 14:29:19,938 - INFO - Epoch 28/500, Train Loss: 0.7238, Val Loss: 1.3431
2024-12-28 14:29:21,052 - INFO - Epoch 29/500, Train Loss: 0.7204, Val Loss: 1.3068
2024-12-28 14:29:22,138 - INFO - Epoch 30/500, Train Loss: 0.7071, Val Loss: 1.3411
2024-12-28 14:29:23,193 - INFO - Epoch 31/500, Train Loss: 0.7110, Val Loss: 1.3032
2024-12-28 14:29:24,304 - INFO - Epoch 32/500, Train Loss: 0.6995, Val Loss: 1.3105
2024-12-28 14:29:25,416 - INFO - Epoch 33/500, Train Loss: 0.7003, Val Loss: 1.3102
2024-12-28 14:29:26,630 - INFO - Epoch 34/500, Train Loss: 0.6897, Val Loss: 1.2823
2024-12-28 14:29:27,736 - INFO - Epoch 35/500, Train Loss: 0.6802, Val Loss: 1.2887
2024-12-28 14:29:28,815 - INFO - Epoch 36/500, Train Loss: 0.6809, Val Loss: 1.3133
2024-12-28 14:29:29,915 - INFO - Epoch 37/500, Train Loss: 0.6850, Val Loss: 1.3015
2024-12-28 14:29:31,055 - INFO - Epoch 38/500, Train Loss: 0.6708, Val Loss: 1.3047
2024-12-28 14:29:32,102 - INFO - Epoch 39/500, Train Loss: 0.6752, Val Loss: 1.2853
2024-12-28 14:29:32,103 - INFO - Early stopping triggered at epoch 39
2024-12-28 14:29:32,103 - INFO - Training completed in 44.17s
2024-12-28 14:29:32,103 - INFO - Final memory usage: CPU 1854.9 MB, GPU 104.5 MB
2024-12-28 14:29:32,104 - INFO - Model training completed in 44.17s
2024-12-28 14:29:32,165 - INFO - Prediction completed in 0.06s
2024-12-28 14:29:32,176 - INFO - Poison rate 0.03 completed in 44.25s
2024-12-28 14:29:32,176 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:29:32,179 - INFO - Total number of labels flipped: 979
2024-12-28 14:29:32,179 - INFO - Label flipping completed in 0.00s
2024-12-28 14:29:32,179 - INFO - Training set processing completed in 0.00s
2024-12-28 14:29:32,179 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:29:32,180 - INFO - Memory usage at start_fit: CPU 1816.3 MB, GPU 104.3 MB
2024-12-28 14:29:32,180 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:29:32,185 - INFO - Number of unique classes: 43
2024-12-28 14:29:32,333 - INFO - Fitted scaler and transformed data
2024-12-28 14:29:32,334 - INFO - Scaling time: 0.15s
2024-12-28 14:29:33,491 - INFO - Epoch 1/500, Train Loss: 8.9713, Val Loss: 5.4733
2024-12-28 14:29:34,714 - INFO - Epoch 2/500, Train Loss: 4.3649, Val Loss: 4.3509
2024-12-28 14:29:35,803 - INFO - Epoch 3/500, Train Loss: 3.3629, Val Loss: 3.6380
2024-12-28 14:29:36,828 - INFO - Epoch 4/500, Train Loss: 2.7690, Val Loss: 3.2588
2024-12-28 14:29:37,860 - INFO - Epoch 5/500, Train Loss: 2.3388, Val Loss: 2.7927
2024-12-28 14:29:38,910 - INFO - Epoch 6/500, Train Loss: 2.0259, Val Loss: 2.6009
2024-12-28 14:29:39,994 - INFO - Epoch 7/500, Train Loss: 1.7951, Val Loss: 2.3833
2024-12-28 14:29:41,128 - INFO - Epoch 8/500, Train Loss: 1.6232, Val Loss: 2.2211
2024-12-28 14:29:42,219 - INFO - Epoch 9/500, Train Loss: 1.4883, Val Loss: 2.1368
2024-12-28 14:29:43,412 - INFO - Epoch 10/500, Train Loss: 1.3823, Val Loss: 1.9746
2024-12-28 14:29:44,626 - INFO - Epoch 11/500, Train Loss: 1.2939, Val Loss: 1.9584
2024-12-28 14:29:45,831 - INFO - Epoch 12/500, Train Loss: 1.2190, Val Loss: 1.8401
2024-12-28 14:29:47,042 - INFO - Epoch 13/500, Train Loss: 1.1520, Val Loss: 1.8024
2024-12-28 14:29:48,246 - INFO - Epoch 14/500, Train Loss: 1.1150, Val Loss: 1.7208
2024-12-28 14:29:49,476 - INFO - Epoch 15/500, Train Loss: 1.0690, Val Loss: 1.7029
2024-12-28 14:29:50,749 - INFO - Epoch 16/500, Train Loss: 1.0315, Val Loss: 1.6752
2024-12-28 14:29:52,000 - INFO - Epoch 17/500, Train Loss: 0.9957, Val Loss: 1.6427
2024-12-28 14:29:53,249 - INFO - Epoch 18/500, Train Loss: 0.9704, Val Loss: 1.6056
2024-12-28 14:29:54,448 - INFO - Epoch 19/500, Train Loss: 0.9478, Val Loss: 1.6032
2024-12-28 14:29:55,589 - INFO - Epoch 20/500, Train Loss: 0.9288, Val Loss: 1.5865
2024-12-28 14:29:56,746 - INFO - Epoch 21/500, Train Loss: 0.9144, Val Loss: 1.4922
2024-12-28 14:29:57,960 - INFO - Epoch 22/500, Train Loss: 0.8901, Val Loss: 1.5566
2024-12-28 14:29:59,135 - INFO - Epoch 23/500, Train Loss: 0.8757, Val Loss: 1.5195
2024-12-28 14:30:00,340 - INFO - Epoch 24/500, Train Loss: 0.8710, Val Loss: 1.5209
2024-12-28 14:30:01,506 - INFO - Epoch 25/500, Train Loss: 0.8534, Val Loss: 1.4819
2024-12-28 14:30:02,643 - INFO - Epoch 26/500, Train Loss: 0.8428, Val Loss: 1.4859
2024-12-28 14:30:03,803 - INFO - Epoch 27/500, Train Loss: 0.8310, Val Loss: 1.4995
2024-12-28 14:30:04,804 - INFO - Epoch 28/500, Train Loss: 0.8243, Val Loss: 1.4575
2024-12-28 14:30:05,838 - INFO - Epoch 29/500, Train Loss: 0.8189, Val Loss: 1.4521
2024-12-28 14:30:06,926 - INFO - Epoch 30/500, Train Loss: 0.8101, Val Loss: 1.4261
2024-12-28 14:30:08,034 - INFO - Epoch 31/500, Train Loss: 0.7988, Val Loss: 1.4254
2024-12-28 14:30:09,122 - INFO - Epoch 32/500, Train Loss: 0.7959, Val Loss: 1.4620
2024-12-28 14:30:10,286 - INFO - Epoch 33/500, Train Loss: 0.7859, Val Loss: 1.4692
2024-12-28 14:30:11,440 - INFO - Epoch 34/500, Train Loss: 0.7826, Val Loss: 1.4508
2024-12-28 14:30:12,694 - INFO - Epoch 35/500, Train Loss: 0.7811, Val Loss: 1.4071
2024-12-28 14:30:13,921 - INFO - Epoch 36/500, Train Loss: 0.7720, Val Loss: 1.4152
2024-12-28 14:30:15,077 - INFO - Epoch 37/500, Train Loss: 0.7665, Val Loss: 1.3797
2024-12-28 14:30:16,238 - INFO - Epoch 38/500, Train Loss: 0.7678, Val Loss: 1.4138
2024-12-28 14:30:17,350 - INFO - Epoch 39/500, Train Loss: 0.7697, Val Loss: 1.4323
2024-12-28 14:30:18,461 - INFO - Epoch 40/500, Train Loss: 0.7572, Val Loss: 1.3787
2024-12-28 14:30:19,651 - INFO - Epoch 41/500, Train Loss: 0.7488, Val Loss: 1.4035
2024-12-28 14:30:20,903 - INFO - Epoch 42/500, Train Loss: 0.7499, Val Loss: 1.4150
2024-12-28 14:30:20,903 - INFO - Early stopping triggered at epoch 42
2024-12-28 14:30:20,903 - INFO - Training completed in 48.72s
2024-12-28 14:30:20,903 - INFO - Final memory usage: CPU 1863.3 MB, GPU 104.5 MB
2024-12-28 14:30:20,904 - INFO - Model training completed in 48.72s
2024-12-28 14:30:20,966 - INFO - Prediction completed in 0.06s
2024-12-28 14:30:20,976 - INFO - Poison rate 0.05 completed in 48.80s
2024-12-28 14:30:20,976 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:30:20,979 - INFO - Total number of labels flipped: 1372
2024-12-28 14:30:20,979 - INFO - Label flipping completed in 0.00s
2024-12-28 14:30:20,979 - INFO - Training set processing completed in 0.00s
2024-12-28 14:30:20,979 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:30:20,980 - INFO - Memory usage at start_fit: CPU 1824.7 MB, GPU 104.3 MB
2024-12-28 14:30:20,980 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:30:20,986 - INFO - Number of unique classes: 43
2024-12-28 14:30:21,147 - INFO - Fitted scaler and transformed data
2024-12-28 14:30:21,147 - INFO - Scaling time: 0.16s
2024-12-28 14:30:22,389 - INFO - Epoch 1/500, Train Loss: 9.6295, Val Loss: 6.3288
2024-12-28 14:30:23,618 - INFO - Epoch 2/500, Train Loss: 4.9792, Val Loss: 4.7609
2024-12-28 14:30:24,730 - INFO - Epoch 3/500, Train Loss: 3.8180, Val Loss: 3.9733
2024-12-28 14:30:25,869 - INFO - Epoch 4/500, Train Loss: 3.1189, Val Loss: 3.4441
2024-12-28 14:30:26,944 - INFO - Epoch 5/500, Train Loss: 2.6222, Val Loss: 3.0395
2024-12-28 14:30:28,138 - INFO - Epoch 6/500, Train Loss: 2.2491, Val Loss: 2.7430
2024-12-28 14:30:29,308 - INFO - Epoch 7/500, Train Loss: 1.9903, Val Loss: 2.5215
2024-12-28 14:30:30,513 - INFO - Epoch 8/500, Train Loss: 1.7844, Val Loss: 2.2786
2024-12-28 14:30:31,686 - INFO - Epoch 9/500, Train Loss: 1.6343, Val Loss: 2.1678
2024-12-28 14:30:32,889 - INFO - Epoch 10/500, Train Loss: 1.5137, Val Loss: 2.0241
2024-12-28 14:30:34,105 - INFO - Epoch 11/500, Train Loss: 1.4269, Val Loss: 1.9365
2024-12-28 14:30:35,277 - INFO - Epoch 12/500, Train Loss: 1.3418, Val Loss: 1.8674
2024-12-28 14:30:36,481 - INFO - Epoch 13/500, Train Loss: 1.2872, Val Loss: 1.8283
2024-12-28 14:30:37,724 - INFO - Epoch 14/500, Train Loss: 1.2339, Val Loss: 1.7281
2024-12-28 14:30:38,815 - INFO - Epoch 15/500, Train Loss: 1.1893, Val Loss: 1.7313
2024-12-28 14:30:39,888 - INFO - Epoch 16/500, Train Loss: 1.1525, Val Loss: 1.6554
2024-12-28 14:30:40,955 - INFO - Epoch 17/500, Train Loss: 1.1133, Val Loss: 1.6151
2024-12-28 14:30:41,983 - INFO - Epoch 18/500, Train Loss: 1.0821, Val Loss: 1.5876
2024-12-28 14:30:43,020 - INFO - Epoch 19/500, Train Loss: 1.0610, Val Loss: 1.5988
2024-12-28 14:30:44,137 - INFO - Epoch 20/500, Train Loss: 1.0372, Val Loss: 1.5611
2024-12-28 14:30:45,230 - INFO - Epoch 21/500, Train Loss: 1.0120, Val Loss: 1.5532
2024-12-28 14:30:46,273 - INFO - Epoch 22/500, Train Loss: 0.9914, Val Loss: 1.5457
2024-12-28 14:30:47,391 - INFO - Epoch 23/500, Train Loss: 0.9801, Val Loss: 1.5239
2024-12-28 14:30:48,492 - INFO - Epoch 24/500, Train Loss: 0.9683, Val Loss: 1.4655
2024-12-28 14:30:49,574 - INFO - Epoch 25/500, Train Loss: 0.9512, Val Loss: 1.4663
2024-12-28 14:30:50,725 - INFO - Epoch 26/500, Train Loss: 0.9471, Val Loss: 1.4309
2024-12-28 14:30:51,962 - INFO - Epoch 27/500, Train Loss: 0.9363, Val Loss: 1.4974
2024-12-28 14:30:53,232 - INFO - Epoch 28/500, Train Loss: 0.9206, Val Loss: 1.4192
2024-12-28 14:30:54,452 - INFO - Epoch 29/500, Train Loss: 0.9181, Val Loss: 1.4406
2024-12-28 14:30:55,635 - INFO - Epoch 30/500, Train Loss: 0.9060, Val Loss: 1.4270
2024-12-28 14:30:56,847 - INFO - Epoch 31/500, Train Loss: 0.8935, Val Loss: 1.4023
2024-12-28 14:30:58,044 - INFO - Epoch 32/500, Train Loss: 0.8939, Val Loss: 1.3869
2024-12-28 14:30:59,202 - INFO - Epoch 33/500, Train Loss: 0.8785, Val Loss: 1.3803
2024-12-28 14:31:00,432 - INFO - Epoch 34/500, Train Loss: 0.8719, Val Loss: 1.3830
2024-12-28 14:31:01,653 - INFO - Epoch 35/500, Train Loss: 0.8757, Val Loss: 1.4040
2024-12-28 14:31:02,822 - INFO - Epoch 36/500, Train Loss: 0.8670, Val Loss: 1.3929
2024-12-28 14:31:03,979 - INFO - Epoch 37/500, Train Loss: 0.8668, Val Loss: 1.3657
2024-12-28 14:31:05,123 - INFO - Epoch 38/500, Train Loss: 0.8569, Val Loss: 1.3828
2024-12-28 14:31:06,280 - INFO - Epoch 39/500, Train Loss: 0.8448, Val Loss: 1.3811
2024-12-28 14:31:07,489 - INFO - Epoch 40/500, Train Loss: 0.8450, Val Loss: 1.3497
2024-12-28 14:31:08,596 - INFO - Epoch 41/500, Train Loss: 0.8447, Val Loss: 1.3644
2024-12-28 14:31:09,712 - INFO - Epoch 42/500, Train Loss: 0.8380, Val Loss: 1.3952
2024-12-28 14:31:10,886 - INFO - Epoch 43/500, Train Loss: 0.8320, Val Loss: 1.3563
2024-12-28 14:31:12,066 - INFO - Epoch 44/500, Train Loss: 0.8300, Val Loss: 1.3487
2024-12-28 14:31:13,217 - INFO - Epoch 45/500, Train Loss: 0.8304, Val Loss: 1.3431
2024-12-28 14:31:14,354 - INFO - Epoch 46/500, Train Loss: 0.8242, Val Loss: 1.3546
2024-12-28 14:31:15,466 - INFO - Epoch 47/500, Train Loss: 0.8323, Val Loss: 1.3475
2024-12-28 14:31:16,539 - INFO - Epoch 48/500, Train Loss: 0.8234, Val Loss: 1.3403
2024-12-28 14:31:17,675 - INFO - Epoch 49/500, Train Loss: 0.8212, Val Loss: 1.3530
2024-12-28 14:31:18,823 - INFO - Epoch 50/500, Train Loss: 0.8229, Val Loss: 1.3409
2024-12-28 14:31:19,879 - INFO - Epoch 51/500, Train Loss: 0.8215, Val Loss: 1.3440
2024-12-28 14:31:20,949 - INFO - Epoch 52/500, Train Loss: 0.8152, Val Loss: 1.3396
2024-12-28 14:31:22,072 - INFO - Epoch 53/500, Train Loss: 0.8094, Val Loss: 1.3504
2024-12-28 14:31:22,072 - INFO - Early stopping triggered at epoch 53
2024-12-28 14:31:22,073 - INFO - Training completed in 61.09s
2024-12-28 14:31:22,073 - INFO - Final memory usage: CPU 1876.1 MB, GPU 104.5 MB
2024-12-28 14:31:22,074 - INFO - Model training completed in 61.09s
2024-12-28 14:31:22,150 - INFO - Prediction completed in 0.08s
2024-12-28 14:31:22,161 - INFO - Poison rate 0.07 completed in 61.18s
2024-12-28 14:31:22,161 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:31:22,164 - INFO - Total number of labels flipped: 1962
2024-12-28 14:31:22,164 - INFO - Label flipping completed in 0.00s
2024-12-28 14:31:22,164 - INFO - Training set processing completed in 0.00s
2024-12-28 14:31:22,164 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:31:22,165 - INFO - Memory usage at start_fit: CPU 1837.5 MB, GPU 104.3 MB
2024-12-28 14:31:22,165 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:31:22,172 - INFO - Number of unique classes: 43
2024-12-28 14:31:22,330 - INFO - Fitted scaler and transformed data
2024-12-28 14:31:22,330 - INFO - Scaling time: 0.16s
2024-12-28 14:31:23,439 - INFO - Epoch 1/500, Train Loss: 10.9209, Val Loss: 7.0999
2024-12-28 14:31:24,583 - INFO - Epoch 2/500, Train Loss: 5.9011, Val Loss: 5.8101
2024-12-28 14:31:25,736 - INFO - Epoch 3/500, Train Loss: 4.5159, Val Loss: 4.6966
2024-12-28 14:31:26,940 - INFO - Epoch 4/500, Train Loss: 3.6171, Val Loss: 3.9983
2024-12-28 14:31:28,143 - INFO - Epoch 5/500, Train Loss: 2.9940, Val Loss: 3.5185
2024-12-28 14:31:29,307 - INFO - Epoch 6/500, Train Loss: 2.5046, Val Loss: 3.0892
2024-12-28 14:31:30,499 - INFO - Epoch 7/500, Train Loss: 2.2001, Val Loss: 2.7530
2024-12-28 14:31:31,621 - INFO - Epoch 8/500, Train Loss: 1.9624, Val Loss: 2.5954
2024-12-28 14:31:32,749 - INFO - Epoch 9/500, Train Loss: 1.8093, Val Loss: 2.3926
2024-12-28 14:31:33,868 - INFO - Epoch 10/500, Train Loss: 1.6700, Val Loss: 2.3004
2024-12-28 14:31:35,069 - INFO - Epoch 11/500, Train Loss: 1.5752, Val Loss: 2.1710
2024-12-28 14:31:36,290 - INFO - Epoch 12/500, Train Loss: 1.4917, Val Loss: 2.1100
2024-12-28 14:31:37,469 - INFO - Epoch 13/500, Train Loss: 1.4216, Val Loss: 2.0512
2024-12-28 14:31:38,657 - INFO - Epoch 14/500, Train Loss: 1.3677, Val Loss: 1.9788
2024-12-28 14:31:39,853 - INFO - Epoch 15/500, Train Loss: 1.3087, Val Loss: 1.9399
2024-12-28 14:31:41,084 - INFO - Epoch 16/500, Train Loss: 1.2835, Val Loss: 1.8868
2024-12-28 14:31:42,256 - INFO - Epoch 17/500, Train Loss: 1.2465, Val Loss: 1.8576
2024-12-28 14:31:43,417 - INFO - Epoch 18/500, Train Loss: 1.2060, Val Loss: 1.8242
2024-12-28 14:31:44,616 - INFO - Epoch 19/500, Train Loss: 1.1803, Val Loss: 1.8331
2024-12-28 14:31:45,832 - INFO - Epoch 20/500, Train Loss: 1.1597, Val Loss: 1.8073
2024-12-28 14:31:47,037 - INFO - Epoch 21/500, Train Loss: 1.1402, Val Loss: 1.8092
2024-12-28 14:31:48,174 - INFO - Epoch 22/500, Train Loss: 1.1200, Val Loss: 1.7739
2024-12-28 14:31:49,350 - INFO - Epoch 23/500, Train Loss: 1.0962, Val Loss: 1.7791
2024-12-28 14:31:50,589 - INFO - Epoch 24/500, Train Loss: 1.0866, Val Loss: 1.7265
2024-12-28 14:31:51,676 - INFO - Epoch 25/500, Train Loss: 1.0631, Val Loss: 1.7043
2024-12-28 14:31:52,766 - INFO - Epoch 26/500, Train Loss: 1.0585, Val Loss: 1.7270
2024-12-28 14:31:53,877 - INFO - Epoch 27/500, Train Loss: 1.0434, Val Loss: 1.6781
2024-12-28 14:31:54,996 - INFO - Epoch 28/500, Train Loss: 1.0332, Val Loss: 1.6838
2024-12-28 14:31:56,072 - INFO - Epoch 29/500, Train Loss: 1.0196, Val Loss: 1.6907
2024-12-28 14:31:57,195 - INFO - Epoch 30/500, Train Loss: 1.0094, Val Loss: 1.6701
2024-12-28 14:31:58,334 - INFO - Epoch 31/500, Train Loss: 1.0028, Val Loss: 1.7100
2024-12-28 14:31:59,531 - INFO - Epoch 32/500, Train Loss: 0.9941, Val Loss: 1.6721
2024-12-28 14:32:00,804 - INFO - Epoch 33/500, Train Loss: 0.9997, Val Loss: 1.6391
2024-12-28 14:32:01,971 - INFO - Epoch 34/500, Train Loss: 0.9782, Val Loss: 1.6608
2024-12-28 14:32:03,039 - INFO - Epoch 35/500, Train Loss: 0.9683, Val Loss: 1.6374
2024-12-28 14:32:04,118 - INFO - Epoch 36/500, Train Loss: 0.9639, Val Loss: 1.6302
2024-12-28 14:32:05,206 - INFO - Epoch 37/500, Train Loss: 0.9566, Val Loss: 1.6262
2024-12-28 14:32:06,415 - INFO - Epoch 38/500, Train Loss: 0.9613, Val Loss: 1.6221
2024-12-28 14:32:07,562 - INFO - Epoch 39/500, Train Loss: 0.9546, Val Loss: 1.5871
2024-12-28 14:32:08,780 - INFO - Epoch 40/500, Train Loss: 0.9452, Val Loss: 1.5998
2024-12-28 14:32:09,895 - INFO - Epoch 41/500, Train Loss: 0.9492, Val Loss: 1.6119
2024-12-28 14:32:11,035 - INFO - Epoch 42/500, Train Loss: 0.9381, Val Loss: 1.6084
2024-12-28 14:32:12,175 - INFO - Epoch 43/500, Train Loss: 0.9326, Val Loss: 1.5841
2024-12-28 14:32:13,390 - INFO - Epoch 44/500, Train Loss: 0.9303, Val Loss: 1.5904
2024-12-28 14:32:14,590 - INFO - Epoch 45/500, Train Loss: 0.9288, Val Loss: 1.5752
2024-12-28 14:32:15,730 - INFO - Epoch 46/500, Train Loss: 0.9224, Val Loss: 1.6087
2024-12-28 14:32:16,953 - INFO - Epoch 47/500, Train Loss: 0.9175, Val Loss: 1.5821
2024-12-28 14:32:18,131 - INFO - Epoch 48/500, Train Loss: 0.9140, Val Loss: 1.5683
2024-12-28 14:32:19,226 - INFO - Epoch 49/500, Train Loss: 0.9153, Val Loss: 1.5679
2024-12-28 14:32:20,430 - INFO - Epoch 50/500, Train Loss: 0.9156, Val Loss: 1.5195
2024-12-28 14:32:21,496 - INFO - Epoch 51/500, Train Loss: 0.9042, Val Loss: 1.5956
2024-12-28 14:32:22,599 - INFO - Epoch 52/500, Train Loss: 0.9186, Val Loss: 1.5588
2024-12-28 14:32:23,630 - INFO - Epoch 53/500, Train Loss: 0.9155, Val Loss: 1.6253
2024-12-28 14:32:24,666 - INFO - Epoch 54/500, Train Loss: 0.9043, Val Loss: 1.5672
2024-12-28 14:32:25,761 - INFO - Epoch 55/500, Train Loss: 0.9003, Val Loss: 1.5493
2024-12-28 14:32:25,761 - INFO - Early stopping triggered at epoch 55
2024-12-28 14:32:25,762 - INFO - Training completed in 63.60s
2024-12-28 14:32:25,762 - INFO - Final memory usage: CPU 1880.6 MB, GPU 104.5 MB
2024-12-28 14:32:25,762 - INFO - Model training completed in 63.60s
2024-12-28 14:32:25,825 - INFO - Prediction completed in 0.06s
2024-12-28 14:32:25,836 - INFO - Poison rate 0.1 completed in 63.67s
2024-12-28 14:32:25,836 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:32:25,839 - INFO - Total number of labels flipped: 3920
2024-12-28 14:32:25,839 - INFO - Label flipping completed in 0.00s
2024-12-28 14:32:25,839 - INFO - Training set processing completed in 0.00s
2024-12-28 14:32:25,839 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:32:25,840 - INFO - Memory usage at start_fit: CPU 1842.1 MB, GPU 104.3 MB
2024-12-28 14:32:25,840 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:32:25,847 - INFO - Number of unique classes: 43
2024-12-28 14:32:25,986 - INFO - Fitted scaler and transformed data
2024-12-28 14:32:25,987 - INFO - Scaling time: 0.14s
2024-12-28 14:32:27,047 - INFO - Epoch 1/500, Train Loss: 14.4749, Val Loss: 10.3466
2024-12-28 14:32:28,068 - INFO - Epoch 2/500, Train Loss: 8.7748, Val Loss: 7.8792
2024-12-28 14:32:29,230 - INFO - Epoch 3/500, Train Loss: 6.5127, Val Loss: 6.1252
2024-12-28 14:32:30,417 - INFO - Epoch 4/500, Train Loss: 4.8869, Val Loss: 4.7552
2024-12-28 14:32:31,605 - INFO - Epoch 5/500, Train Loss: 3.8263, Val Loss: 3.9166
2024-12-28 14:32:32,666 - INFO - Epoch 6/500, Train Loss: 3.1275, Val Loss: 3.4583
2024-12-28 14:32:33,739 - INFO - Epoch 7/500, Train Loss: 2.6667, Val Loss: 3.0607
2024-12-28 14:32:34,852 - INFO - Epoch 8/500, Train Loss: 2.3997, Val Loss: 2.8656
2024-12-28 14:32:36,062 - INFO - Epoch 9/500, Train Loss: 2.1996, Val Loss: 2.6452
2024-12-28 14:32:37,300 - INFO - Epoch 10/500, Train Loss: 2.0438, Val Loss: 2.5423
2024-12-28 14:32:38,506 - INFO - Epoch 11/500, Train Loss: 1.9310, Val Loss: 2.4352
2024-12-28 14:32:39,732 - INFO - Epoch 12/500, Train Loss: 1.8431, Val Loss: 2.3147
2024-12-28 14:32:40,984 - INFO - Epoch 13/500, Train Loss: 1.7551, Val Loss: 2.2659
2024-12-28 14:32:42,218 - INFO - Epoch 14/500, Train Loss: 1.6896, Val Loss: 2.2317
2024-12-28 14:32:43,451 - INFO - Epoch 15/500, Train Loss: 1.6447, Val Loss: 2.1574
2024-12-28 14:32:44,644 - INFO - Epoch 16/500, Train Loss: 1.5896, Val Loss: 2.1159
2024-12-28 14:32:45,837 - INFO - Epoch 17/500, Train Loss: 1.5509, Val Loss: 2.0680
2024-12-28 14:32:46,966 - INFO - Epoch 18/500, Train Loss: 1.5132, Val Loss: 2.0501
2024-12-28 14:32:48,112 - INFO - Epoch 19/500, Train Loss: 1.4714, Val Loss: 2.0160
2024-12-28 14:32:49,225 - INFO - Epoch 20/500, Train Loss: 1.4416, Val Loss: 1.9953
2024-12-28 14:32:50,346 - INFO - Epoch 21/500, Train Loss: 1.4245, Val Loss: 2.0006
2024-12-28 14:32:51,473 - INFO - Epoch 22/500, Train Loss: 1.4077, Val Loss: 1.9519
2024-12-28 14:32:52,581 - INFO - Epoch 23/500, Train Loss: 1.3782, Val Loss: 1.9403
2024-12-28 14:32:53,665 - INFO - Epoch 24/500, Train Loss: 1.3623, Val Loss: 1.9337
2024-12-28 14:32:54,779 - INFO - Epoch 25/500, Train Loss: 1.3447, Val Loss: 1.8883
2024-12-28 14:32:55,951 - INFO - Epoch 26/500, Train Loss: 1.3209, Val Loss: 1.8894
2024-12-28 14:32:57,053 - INFO - Epoch 27/500, Train Loss: 1.3145, Val Loss: 1.8757
2024-12-28 14:32:58,242 - INFO - Epoch 28/500, Train Loss: 1.3036, Val Loss: 1.9357
2024-12-28 14:32:59,325 - INFO - Epoch 29/500, Train Loss: 1.2904, Val Loss: 1.8568
2024-12-28 14:33:00,395 - INFO - Epoch 30/500, Train Loss: 1.2761, Val Loss: 1.8536
2024-12-28 14:33:01,441 - INFO - Epoch 31/500, Train Loss: 1.2679, Val Loss: 1.8659
2024-12-28 14:33:02,502 - INFO - Epoch 32/500, Train Loss: 1.2456, Val Loss: 1.8465
2024-12-28 14:33:03,544 - INFO - Epoch 33/500, Train Loss: 1.2489, Val Loss: 1.8050
2024-12-28 14:33:04,662 - INFO - Epoch 34/500, Train Loss: 1.2407, Val Loss: 1.8262
2024-12-28 14:33:05,753 - INFO - Epoch 35/500, Train Loss: 1.2273, Val Loss: 1.7998
2024-12-28 14:33:06,830 - INFO - Epoch 36/500, Train Loss: 1.2238, Val Loss: 1.8119
2024-12-28 14:33:07,923 - INFO - Epoch 37/500, Train Loss: 1.2216, Val Loss: 1.8218
2024-12-28 14:33:09,011 - INFO - Epoch 38/500, Train Loss: 1.2107, Val Loss: 1.8035
2024-12-28 14:33:10,068 - INFO - Epoch 39/500, Train Loss: 1.1962, Val Loss: 1.8158
2024-12-28 14:33:11,156 - INFO - Epoch 40/500, Train Loss: 1.1903, Val Loss: 1.8229
2024-12-28 14:33:11,156 - INFO - Early stopping triggered at epoch 40
2024-12-28 14:33:11,156 - INFO - Training completed in 45.32s
2024-12-28 14:33:11,156 - INFO - Final memory usage: CPU 1881.3 MB, GPU 104.5 MB
2024-12-28 14:33:11,157 - INFO - Model training completed in 45.32s
2024-12-28 14:33:11,218 - INFO - Prediction completed in 0.06s
2024-12-28 14:33:11,229 - INFO - Poison rate 0.2 completed in 45.39s
2024-12-28 14:33:11,231 - INFO - Loaded 56 existing results
2024-12-28 14:33:11,231 - INFO - Total results to save: 63
2024-12-28 14:33:11,232 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:33:11,236 - INFO - Saved 63 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:33:11,236 - INFO - Total evaluation time: 391.52s
2024-12-28 14:33:11,242 - INFO - 
Progress: 10.4% - Evaluating GTSRB with SVM (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:33:11,420 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:33:11,420 - INFO - Dataset type: image
2024-12-28 14:33:11,420 - INFO - Sample size: 39209
2024-12-28 14:33:11,420 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:33:11,421 - INFO - Loading datasets...
2024-12-28 14:33:30,888 - INFO - Dataset loading completed in 19.47s
2024-12-28 14:33:30,889 - INFO - Extracting validation features...
2024-12-28 14:33:30,889 - INFO - Extracting features from 4435 samples...
2024-12-28 14:33:31,614 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:33:31,620 - INFO - Validation feature extraction completed in 0.73s
2024-12-28 14:33:31,620 - INFO - Extracting training features...
2024-12-28 14:33:31,620 - INFO - Extracting features from 19755 samples...
2024-12-28 14:33:34,344 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:33:34,356 - INFO - Training feature extraction completed in 2.74s
2024-12-28 14:33:34,356 - INFO - Creating model for classifier: SVM
2024-12-28 14:33:34,357 - INFO - Using device: cuda
2024-12-28 14:33:34,357 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 14:33:34,357 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:33:34,357 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:33:34,358 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:33:35,581 - INFO - Feature scaling completed in 1.22s
2024-12-28 14:33:35,582 - INFO - Starting feature selection (k=50)
2024-12-28 14:33:35,604 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:33:35,604 - INFO - Starting anomaly detection
2024-12-28 14:33:43,445 - INFO - Anomaly detection completed in 7.84s
2024-12-28 14:33:43,445 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:33:43,445 - INFO - Total fit_transform time: 9.09s
2024-12-28 14:33:43,446 - INFO - Training set processing completed in 9.09s
2024-12-28 14:33:43,446 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:33:43,447 - INFO - Memory usage at start_fit: CPU 1852.3 MB, GPU 103.7 MB
2024-12-28 14:33:43,447 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:33:43,453 - INFO - Number of unique classes: 43
2024-12-28 14:33:43,607 - INFO - Fitted scaler and transformed data
2024-12-28 14:33:43,607 - INFO - Scaling time: 0.15s
2024-12-28 14:33:44,505 - INFO - Epoch 1/500, Train Loss: 6.4109, Val Loss: 3.4534
2024-12-28 14:33:45,439 - INFO - Epoch 2/500, Train Loss: 2.3711, Val Loss: 2.4225
2024-12-28 14:33:46,429 - INFO - Epoch 3/500, Train Loss: 1.7139, Val Loss: 1.9819
2024-12-28 14:33:47,499 - INFO - Epoch 4/500, Train Loss: 1.3907, Val Loss: 1.6995
2024-12-28 14:33:48,572 - INFO - Epoch 5/500, Train Loss: 1.1845, Val Loss: 1.5388
2024-12-28 14:33:49,735 - INFO - Epoch 6/500, Train Loss: 1.0473, Val Loss: 1.4312
2024-12-28 14:33:50,946 - INFO - Epoch 7/500, Train Loss: 0.9388, Val Loss: 1.3404
2024-12-28 14:33:52,146 - INFO - Epoch 8/500, Train Loss: 0.8641, Val Loss: 1.2709
2024-12-28 14:33:53,352 - INFO - Epoch 9/500, Train Loss: 0.8019, Val Loss: 1.1998
2024-12-28 14:33:54,378 - INFO - Epoch 10/500, Train Loss: 0.7507, Val Loss: 1.1768
2024-12-28 14:33:55,516 - INFO - Epoch 11/500, Train Loss: 0.7052, Val Loss: 1.1484
2024-12-28 14:33:56,531 - INFO - Epoch 12/500, Train Loss: 0.6734, Val Loss: 1.0907
2024-12-28 14:33:57,514 - INFO - Epoch 13/500, Train Loss: 0.6413, Val Loss: 1.0863
2024-12-28 14:33:58,589 - INFO - Epoch 14/500, Train Loss: 0.6140, Val Loss: 1.0364
2024-12-28 14:33:59,839 - INFO - Epoch 15/500, Train Loss: 0.5943, Val Loss: 1.0531
2024-12-28 14:34:01,086 - INFO - Epoch 16/500, Train Loss: 0.5793, Val Loss: 1.0254
2024-12-28 14:34:02,261 - INFO - Epoch 17/500, Train Loss: 0.5588, Val Loss: 0.9772
2024-12-28 14:34:03,496 - INFO - Epoch 18/500, Train Loss: 0.5423, Val Loss: 1.0133
2024-12-28 14:34:04,701 - INFO - Epoch 19/500, Train Loss: 0.5310, Val Loss: 0.9658
2024-12-28 14:34:05,838 - INFO - Epoch 20/500, Train Loss: 0.5226, Val Loss: 0.9512
2024-12-28 14:34:07,061 - INFO - Epoch 21/500, Train Loss: 0.5115, Val Loss: 0.9792
2024-12-28 14:34:08,227 - INFO - Epoch 22/500, Train Loss: 0.5064, Val Loss: 0.9384
2024-12-28 14:34:09,477 - INFO - Epoch 23/500, Train Loss: 0.4985, Val Loss: 0.9451
2024-12-28 14:34:10,708 - INFO - Epoch 24/500, Train Loss: 0.4927, Val Loss: 0.9284
2024-12-28 14:34:11,962 - INFO - Epoch 25/500, Train Loss: 0.4851, Val Loss: 0.9393
2024-12-28 14:34:13,196 - INFO - Epoch 26/500, Train Loss: 0.4756, Val Loss: 0.9248
2024-12-28 14:34:14,457 - INFO - Epoch 27/500, Train Loss: 0.4756, Val Loss: 0.9212
2024-12-28 14:34:15,635 - INFO - Epoch 28/500, Train Loss: 0.4727, Val Loss: 0.9159
2024-12-28 14:34:16,807 - INFO - Epoch 29/500, Train Loss: 0.4652, Val Loss: 0.9306
2024-12-28 14:34:17,968 - INFO - Epoch 30/500, Train Loss: 0.4582, Val Loss: 0.9296
2024-12-28 14:34:19,117 - INFO - Epoch 31/500, Train Loss: 0.4655, Val Loss: 0.9282
2024-12-28 14:34:20,355 - INFO - Epoch 32/500, Train Loss: 0.4580, Val Loss: 0.9292
2024-12-28 14:34:21,385 - INFO - Epoch 33/500, Train Loss: 0.4541, Val Loss: 0.9258
2024-12-28 14:34:21,386 - INFO - Early stopping triggered at epoch 33
2024-12-28 14:34:21,386 - INFO - Training completed in 37.94s
2024-12-28 14:34:21,386 - INFO - Final memory usage: CPU 1891.3 MB, GPU 104.2 MB
2024-12-28 14:34:21,387 - INFO - Model training completed in 37.94s
2024-12-28 14:34:21,451 - INFO - Prediction completed in 0.06s
2024-12-28 14:34:21,463 - INFO - Poison rate 0.0 completed in 47.11s
2024-12-28 14:34:21,463 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:34:21,465 - INFO - Total number of labels flipped: 197
2024-12-28 14:34:21,465 - INFO - Label flipping completed in 0.00s
2024-12-28 14:34:21,465 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:34:21,465 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:34:22,693 - INFO - Feature scaling completed in 1.23s
2024-12-28 14:34:22,693 - INFO - Starting feature selection (k=50)
2024-12-28 14:34:22,722 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:34:22,723 - INFO - Starting anomaly detection
2024-12-28 14:34:30,453 - INFO - Anomaly detection completed in 7.73s
2024-12-28 14:34:30,453 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:34:30,454 - INFO - Total fit_transform time: 8.99s
2024-12-28 14:34:30,454 - INFO - Training set processing completed in 8.99s
2024-12-28 14:34:30,454 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:34:30,455 - INFO - Memory usage at start_fit: CPU 1852.7 MB, GPU 104.1 MB
2024-12-28 14:34:30,456 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:34:30,463 - INFO - Number of unique classes: 43
2024-12-28 14:34:30,612 - INFO - Fitted scaler and transformed data
2024-12-28 14:34:30,612 - INFO - Scaling time: 0.15s
2024-12-28 14:34:31,657 - INFO - Epoch 1/500, Train Loss: 6.9934, Val Loss: 3.9421
2024-12-28 14:34:32,760 - INFO - Epoch 2/500, Train Loss: 2.7810, Val Loss: 3.0021
2024-12-28 14:34:33,827 - INFO - Epoch 3/500, Train Loss: 2.0761, Val Loss: 2.4906
2024-12-28 14:34:34,896 - INFO - Epoch 4/500, Train Loss: 1.6881, Val Loss: 2.2934
2024-12-28 14:34:35,954 - INFO - Epoch 5/500, Train Loss: 1.4603, Val Loss: 2.1017
2024-12-28 14:34:37,114 - INFO - Epoch 6/500, Train Loss: 1.2931, Val Loss: 1.9349
2024-12-28 14:34:38,271 - INFO - Epoch 7/500, Train Loss: 1.1590, Val Loss: 1.8560
2024-12-28 14:34:39,451 - INFO - Epoch 8/500, Train Loss: 1.0590, Val Loss: 1.7564
2024-12-28 14:34:40,593 - INFO - Epoch 9/500, Train Loss: 0.9846, Val Loss: 1.6897
2024-12-28 14:34:41,748 - INFO - Epoch 10/500, Train Loss: 0.9221, Val Loss: 1.6384
2024-12-28 14:34:42,917 - INFO - Epoch 11/500, Train Loss: 0.8656, Val Loss: 1.5688
2024-12-28 14:34:44,136 - INFO - Epoch 12/500, Train Loss: 0.8228, Val Loss: 1.5496
2024-12-28 14:34:45,223 - INFO - Epoch 13/500, Train Loss: 0.7825, Val Loss: 1.4994
2024-12-28 14:34:46,318 - INFO - Epoch 14/500, Train Loss: 0.7512, Val Loss: 1.4727
2024-12-28 14:34:47,428 - INFO - Epoch 15/500, Train Loss: 0.7222, Val Loss: 1.4269
2024-12-28 14:34:48,536 - INFO - Epoch 16/500, Train Loss: 0.7035, Val Loss: 1.3618
2024-12-28 14:34:49,787 - INFO - Epoch 17/500, Train Loss: 0.6710, Val Loss: 1.4018
2024-12-28 14:34:50,989 - INFO - Epoch 18/500, Train Loss: 0.6587, Val Loss: 1.3437
2024-12-28 14:34:52,177 - INFO - Epoch 19/500, Train Loss: 0.6438, Val Loss: 1.3274
2024-12-28 14:34:53,349 - INFO - Epoch 20/500, Train Loss: 0.6307, Val Loss: 1.3350
2024-12-28 14:34:54,573 - INFO - Epoch 21/500, Train Loss: 0.6137, Val Loss: 1.3351
2024-12-28 14:34:55,643 - INFO - Epoch 22/500, Train Loss: 0.6070, Val Loss: 1.3186
2024-12-28 14:34:56,642 - INFO - Epoch 23/500, Train Loss: 0.5912, Val Loss: 1.3159
2024-12-28 14:34:57,621 - INFO - Epoch 24/500, Train Loss: 0.5860, Val Loss: 1.3162
2024-12-28 14:34:58,626 - INFO - Epoch 25/500, Train Loss: 0.5826, Val Loss: 1.2733
2024-12-28 14:34:59,589 - INFO - Epoch 26/500, Train Loss: 0.5665, Val Loss: 1.3027
2024-12-28 14:35:00,562 - INFO - Epoch 27/500, Train Loss: 0.5582, Val Loss: 1.2699
2024-12-28 14:35:01,609 - INFO - Epoch 28/500, Train Loss: 0.5619, Val Loss: 1.2831
2024-12-28 14:35:02,516 - INFO - Epoch 29/500, Train Loss: 0.5527, Val Loss: 1.2178
2024-12-28 14:35:03,471 - INFO - Epoch 30/500, Train Loss: 0.5496, Val Loss: 1.2465
2024-12-28 14:35:04,479 - INFO - Epoch 31/500, Train Loss: 0.5476, Val Loss: 1.2771
2024-12-28 14:35:05,616 - INFO - Epoch 32/500, Train Loss: 0.5452, Val Loss: 1.2380
2024-12-28 14:35:06,805 - INFO - Epoch 33/500, Train Loss: 0.5377, Val Loss: 1.2290
2024-12-28 14:35:07,915 - INFO - Epoch 34/500, Train Loss: 0.5334, Val Loss: 1.2044
2024-12-28 14:35:09,079 - INFO - Epoch 35/500, Train Loss: 0.5402, Val Loss: 1.2348
2024-12-28 14:35:10,290 - INFO - Epoch 36/500, Train Loss: 0.5291, Val Loss: 1.2117
2024-12-28 14:35:11,462 - INFO - Epoch 37/500, Train Loss: 0.5251, Val Loss: 1.2458
2024-12-28 14:35:12,524 - INFO - Epoch 38/500, Train Loss: 0.5218, Val Loss: 1.1788
2024-12-28 14:35:13,600 - INFO - Epoch 39/500, Train Loss: 0.5229, Val Loss: 1.1948
2024-12-28 14:35:14,675 - INFO - Epoch 40/500, Train Loss: 0.5204, Val Loss: 1.1784
2024-12-28 14:35:15,896 - INFO - Epoch 41/500, Train Loss: 0.5134, Val Loss: 1.1723
2024-12-28 14:35:17,055 - INFO - Epoch 42/500, Train Loss: 0.5187, Val Loss: 1.1877
2024-12-28 14:35:18,228 - INFO - Epoch 43/500, Train Loss: 0.5167, Val Loss: 1.1780
2024-12-28 14:35:19,345 - INFO - Epoch 44/500, Train Loss: 0.5079, Val Loss: 1.2712
2024-12-28 14:35:20,406 - INFO - Epoch 45/500, Train Loss: 0.5132, Val Loss: 1.2288
2024-12-28 14:35:21,587 - INFO - Epoch 46/500, Train Loss: 0.5144, Val Loss: 1.2239
2024-12-28 14:35:21,587 - INFO - Early stopping triggered at epoch 46
2024-12-28 14:35:21,587 - INFO - Training completed in 51.13s
2024-12-28 14:35:21,587 - INFO - Final memory usage: CPU 1898.6 MB, GPU 104.2 MB
2024-12-28 14:35:21,588 - INFO - Model training completed in 51.13s
2024-12-28 14:35:21,649 - INFO - Prediction completed in 0.06s
2024-12-28 14:35:21,661 - INFO - Poison rate 0.01 completed in 60.20s
2024-12-28 14:35:21,661 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:35:21,663 - INFO - Total number of labels flipped: 590
2024-12-28 14:35:21,663 - INFO - Label flipping completed in 0.00s
2024-12-28 14:35:21,663 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:35:21,663 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:35:22,940 - INFO - Feature scaling completed in 1.28s
2024-12-28 14:35:22,940 - INFO - Starting feature selection (k=50)
2024-12-28 14:35:22,970 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:35:22,970 - INFO - Starting anomaly detection
2024-12-28 14:35:31,150 - INFO - Anomaly detection completed in 8.18s
2024-12-28 14:35:31,151 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:35:31,151 - INFO - Total fit_transform time: 9.49s
2024-12-28 14:35:31,152 - INFO - Training set processing completed in 9.49s
2024-12-28 14:35:31,152 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:35:31,153 - INFO - Memory usage at start_fit: CPU 1862.4 MB, GPU 104.1 MB
2024-12-28 14:35:31,153 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:35:31,159 - INFO - Number of unique classes: 43
2024-12-28 14:35:31,314 - INFO - Fitted scaler and transformed data
2024-12-28 14:35:31,315 - INFO - Scaling time: 0.15s
2024-12-28 14:35:32,321 - INFO - Epoch 1/500, Train Loss: 7.7944, Val Loss: 4.7184
2024-12-28 14:35:33,496 - INFO - Epoch 2/500, Train Loss: 3.5161, Val Loss: 3.7474
2024-12-28 14:35:34,591 - INFO - Epoch 3/500, Train Loss: 2.6887, Val Loss: 3.2101
2024-12-28 14:35:35,746 - INFO - Epoch 4/500, Train Loss: 2.2322, Val Loss: 2.8068
2024-12-28 14:35:36,913 - INFO - Epoch 5/500, Train Loss: 1.8848, Val Loss: 2.6058
2024-12-28 14:35:38,050 - INFO - Epoch 6/500, Train Loss: 1.6729, Val Loss: 2.3907
2024-12-28 14:35:39,254 - INFO - Epoch 7/500, Train Loss: 1.4959, Val Loss: 2.2350
2024-12-28 14:35:40,411 - INFO - Epoch 8/500, Train Loss: 1.3647, Val Loss: 2.1696
2024-12-28 14:35:41,624 - INFO - Epoch 9/500, Train Loss: 1.2465, Val Loss: 2.0622
2024-12-28 14:35:42,858 - INFO - Epoch 10/500, Train Loss: 1.1759, Val Loss: 1.9510
2024-12-28 14:35:44,002 - INFO - Epoch 11/500, Train Loss: 1.0921, Val Loss: 1.8263
2024-12-28 14:35:45,159 - INFO - Epoch 12/500, Train Loss: 1.0258, Val Loss: 1.7920
2024-12-28 14:35:46,366 - INFO - Epoch 13/500, Train Loss: 0.9873, Val Loss: 1.7328
2024-12-28 14:35:47,578 - INFO - Epoch 14/500, Train Loss: 0.9373, Val Loss: 1.7287
2024-12-28 14:35:48,728 - INFO - Epoch 15/500, Train Loss: 0.9129, Val Loss: 1.6481
2024-12-28 14:35:49,894 - INFO - Epoch 16/500, Train Loss: 0.8815, Val Loss: 1.6299
2024-12-28 14:35:51,184 - INFO - Epoch 17/500, Train Loss: 0.8458, Val Loss: 1.6453
2024-12-28 14:35:52,343 - INFO - Epoch 18/500, Train Loss: 0.8232, Val Loss: 1.5999
2024-12-28 14:35:53,535 - INFO - Epoch 19/500, Train Loss: 0.8093, Val Loss: 1.5641
2024-12-28 14:35:54,691 - INFO - Epoch 20/500, Train Loss: 0.7831, Val Loss: 1.5509
2024-12-28 14:35:55,908 - INFO - Epoch 21/500, Train Loss: 0.7758, Val Loss: 1.5474
2024-12-28 14:35:57,095 - INFO - Epoch 22/500, Train Loss: 0.7600, Val Loss: 1.5219
2024-12-28 14:35:58,177 - INFO - Epoch 23/500, Train Loss: 0.7482, Val Loss: 1.5359
2024-12-28 14:35:59,253 - INFO - Epoch 24/500, Train Loss: 0.7363, Val Loss: 1.4410
2024-12-28 14:36:00,297 - INFO - Epoch 25/500, Train Loss: 0.7301, Val Loss: 1.4422
2024-12-28 14:36:01,354 - INFO - Epoch 26/500, Train Loss: 0.7224, Val Loss: 1.4487
2024-12-28 14:36:02,291 - INFO - Epoch 27/500, Train Loss: 0.7043, Val Loss: 1.4270
2024-12-28 14:36:03,367 - INFO - Epoch 28/500, Train Loss: 0.7011, Val Loss: 1.4739
2024-12-28 14:36:04,563 - INFO - Epoch 29/500, Train Loss: 0.6969, Val Loss: 1.3891
2024-12-28 14:36:05,831 - INFO - Epoch 30/500, Train Loss: 0.6903, Val Loss: 1.3685
2024-12-28 14:36:07,004 - INFO - Epoch 31/500, Train Loss: 0.6812, Val Loss: 1.3950
2024-12-28 14:36:08,164 - INFO - Epoch 32/500, Train Loss: 0.6821, Val Loss: 1.3552
2024-12-28 14:36:09,294 - INFO - Epoch 33/500, Train Loss: 0.6752, Val Loss: 1.3285
2024-12-28 14:36:10,432 - INFO - Epoch 34/500, Train Loss: 0.6657, Val Loss: 1.3746
2024-12-28 14:36:11,567 - INFO - Epoch 35/500, Train Loss: 0.6613, Val Loss: 1.3904
2024-12-28 14:36:12,689 - INFO - Epoch 36/500, Train Loss: 0.6640, Val Loss: 1.3479
2024-12-28 14:36:13,902 - INFO - Epoch 37/500, Train Loss: 0.6595, Val Loss: 1.3436
2024-12-28 14:36:15,019 - INFO - Epoch 38/500, Train Loss: 0.6554, Val Loss: 1.2818
2024-12-28 14:36:16,147 - INFO - Epoch 39/500, Train Loss: 0.6464, Val Loss: 1.3223
2024-12-28 14:36:17,274 - INFO - Epoch 40/500, Train Loss: 0.6480, Val Loss: 1.3377
2024-12-28 14:36:18,435 - INFO - Epoch 41/500, Train Loss: 0.6481, Val Loss: 1.3503
2024-12-28 14:36:19,583 - INFO - Epoch 42/500, Train Loss: 0.6442, Val Loss: 1.3115
2024-12-28 14:36:20,783 - INFO - Epoch 43/500, Train Loss: 0.6471, Val Loss: 1.2985
2024-12-28 14:36:20,783 - INFO - Early stopping triggered at epoch 43
2024-12-28 14:36:20,783 - INFO - Training completed in 49.63s
2024-12-28 14:36:20,783 - INFO - Final memory usage: CPU 1903.9 MB, GPU 104.2 MB
2024-12-28 14:36:20,784 - INFO - Model training completed in 49.63s
2024-12-28 14:36:20,846 - INFO - Prediction completed in 0.06s
2024-12-28 14:36:20,858 - INFO - Poison rate 0.03 completed in 59.20s
2024-12-28 14:36:20,858 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:36:20,860 - INFO - Total number of labels flipped: 982
2024-12-28 14:36:20,860 - INFO - Label flipping completed in 0.00s
2024-12-28 14:36:20,860 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:36:20,860 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:36:22,146 - INFO - Feature scaling completed in 1.29s
2024-12-28 14:36:22,146 - INFO - Starting feature selection (k=50)
2024-12-28 14:36:22,175 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:36:22,176 - INFO - Starting anomaly detection
2024-12-28 14:36:30,216 - INFO - Anomaly detection completed in 8.04s
2024-12-28 14:36:30,216 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:36:30,217 - INFO - Total fit_transform time: 9.36s
2024-12-28 14:36:30,217 - INFO - Training set processing completed in 9.36s
2024-12-28 14:36:30,217 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:36:30,218 - INFO - Memory usage at start_fit: CPU 1865.3 MB, GPU 104.1 MB
2024-12-28 14:36:30,218 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:36:30,224 - INFO - Number of unique classes: 43
2024-12-28 14:36:30,366 - INFO - Fitted scaler and transformed data
2024-12-28 14:36:30,367 - INFO - Scaling time: 0.14s
2024-12-28 14:36:31,380 - INFO - Epoch 1/500, Train Loss: 8.7790, Val Loss: 5.8904
2024-12-28 14:36:32,462 - INFO - Epoch 2/500, Train Loss: 4.1855, Val Loss: 4.7148
2024-12-28 14:36:33,547 - INFO - Epoch 3/500, Train Loss: 3.2377, Val Loss: 3.8753
2024-12-28 14:36:34,550 - INFO - Epoch 4/500, Train Loss: 2.6403, Val Loss: 3.4695
2024-12-28 14:36:35,532 - INFO - Epoch 5/500, Train Loss: 2.2416, Val Loss: 3.1961
2024-12-28 14:36:36,471 - INFO - Epoch 6/500, Train Loss: 1.9485, Val Loss: 2.7822
2024-12-28 14:36:37,417 - INFO - Epoch 7/500, Train Loss: 1.7264, Val Loss: 2.5196
2024-12-28 14:36:38,365 - INFO - Epoch 8/500, Train Loss: 1.5568, Val Loss: 2.3776
2024-12-28 14:36:39,291 - INFO - Epoch 9/500, Train Loss: 1.4417, Val Loss: 2.2032
2024-12-28 14:36:40,239 - INFO - Epoch 10/500, Train Loss: 1.3292, Val Loss: 2.1779
2024-12-28 14:36:41,195 - INFO - Epoch 11/500, Train Loss: 1.2433, Val Loss: 2.0477
2024-12-28 14:36:42,363 - INFO - Epoch 12/500, Train Loss: 1.1823, Val Loss: 1.9783
2024-12-28 14:36:43,552 - INFO - Epoch 13/500, Train Loss: 1.1233, Val Loss: 1.8756
2024-12-28 14:36:44,701 - INFO - Epoch 14/500, Train Loss: 1.0759, Val Loss: 1.8863
2024-12-28 14:36:45,773 - INFO - Epoch 15/500, Train Loss: 1.0382, Val Loss: 1.7747
2024-12-28 14:36:46,853 - INFO - Epoch 16/500, Train Loss: 0.9937, Val Loss: 1.7677
2024-12-28 14:36:47,971 - INFO - Epoch 17/500, Train Loss: 0.9673, Val Loss: 1.7049
2024-12-28 14:36:49,112 - INFO - Epoch 18/500, Train Loss: 0.9431, Val Loss: 1.7330
2024-12-28 14:36:50,199 - INFO - Epoch 19/500, Train Loss: 0.9194, Val Loss: 1.6981
2024-12-28 14:36:51,157 - INFO - Epoch 20/500, Train Loss: 0.9021, Val Loss: 1.6450
2024-12-28 14:36:52,217 - INFO - Epoch 21/500, Train Loss: 0.8835, Val Loss: 1.6800
2024-12-28 14:36:53,464 - INFO - Epoch 22/500, Train Loss: 0.8621, Val Loss: 1.6035
2024-12-28 14:36:54,744 - INFO - Epoch 23/500, Train Loss: 0.8530, Val Loss: 1.6212
2024-12-28 14:36:55,971 - INFO - Epoch 24/500, Train Loss: 0.8376, Val Loss: 1.5770
2024-12-28 14:36:57,151 - INFO - Epoch 25/500, Train Loss: 0.8239, Val Loss: 1.6185
2024-12-28 14:36:58,413 - INFO - Epoch 26/500, Train Loss: 0.8200, Val Loss: 1.6122
2024-12-28 14:36:59,643 - INFO - Epoch 27/500, Train Loss: 0.8156, Val Loss: 1.5664
2024-12-28 14:37:00,735 - INFO - Epoch 28/500, Train Loss: 0.7970, Val Loss: 1.5571
2024-12-28 14:37:01,945 - INFO - Epoch 29/500, Train Loss: 0.7906, Val Loss: 1.5342
2024-12-28 14:37:03,131 - INFO - Epoch 30/500, Train Loss: 0.7909, Val Loss: 1.4860
2024-12-28 14:37:04,274 - INFO - Epoch 31/500, Train Loss: 0.7773, Val Loss: 1.4921
2024-12-28 14:37:05,437 - INFO - Epoch 32/500, Train Loss: 0.7701, Val Loss: 1.4905
2024-12-28 14:37:06,623 - INFO - Epoch 33/500, Train Loss: 0.7654, Val Loss: 1.4651
2024-12-28 14:37:07,778 - INFO - Epoch 34/500, Train Loss: 0.7626, Val Loss: 1.4846
2024-12-28 14:37:08,893 - INFO - Epoch 35/500, Train Loss: 0.7531, Val Loss: 1.4245
2024-12-28 14:37:09,929 - INFO - Epoch 36/500, Train Loss: 0.7488, Val Loss: 1.4681
2024-12-28 14:37:10,944 - INFO - Epoch 37/500, Train Loss: 0.7468, Val Loss: 1.4569
2024-12-28 14:37:11,993 - INFO - Epoch 38/500, Train Loss: 0.7424, Val Loss: 1.4290
2024-12-28 14:37:13,085 - INFO - Epoch 39/500, Train Loss: 0.7402, Val Loss: 1.4519
2024-12-28 14:37:14,266 - INFO - Epoch 40/500, Train Loss: 0.7377, Val Loss: 1.4082
2024-12-28 14:37:15,448 - INFO - Epoch 41/500, Train Loss: 0.7322, Val Loss: 1.4515
2024-12-28 14:37:16,620 - INFO - Epoch 42/500, Train Loss: 0.7331, Val Loss: 1.4055
2024-12-28 14:37:17,825 - INFO - Epoch 43/500, Train Loss: 0.7289, Val Loss: 1.4192
2024-12-28 14:37:19,078 - INFO - Epoch 44/500, Train Loss: 0.7326, Val Loss: 1.4517
2024-12-28 14:37:20,255 - INFO - Epoch 45/500, Train Loss: 0.7203, Val Loss: 1.3904
2024-12-28 14:37:21,434 - INFO - Epoch 46/500, Train Loss: 0.7158, Val Loss: 1.4400
2024-12-28 14:37:22,666 - INFO - Epoch 47/500, Train Loss: 0.7097, Val Loss: 1.4030
2024-12-28 14:37:23,895 - INFO - Epoch 48/500, Train Loss: 0.7163, Val Loss: 1.4254
2024-12-28 14:37:25,118 - INFO - Epoch 49/500, Train Loss: 0.7160, Val Loss: 1.3824
2024-12-28 14:37:26,309 - INFO - Epoch 50/500, Train Loss: 0.7062, Val Loss: 1.4277
2024-12-28 14:37:27,500 - INFO - Epoch 51/500, Train Loss: 0.7138, Val Loss: 1.3887
2024-12-28 14:37:28,596 - INFO - Epoch 52/500, Train Loss: 0.7117, Val Loss: 1.3938
2024-12-28 14:37:29,600 - INFO - Epoch 53/500, Train Loss: 0.7085, Val Loss: 1.4509
2024-12-28 14:37:30,686 - INFO - Epoch 54/500, Train Loss: 0.7104, Val Loss: 1.4557
2024-12-28 14:37:30,686 - INFO - Early stopping triggered at epoch 54
2024-12-28 14:37:30,686 - INFO - Training completed in 60.47s
2024-12-28 14:37:30,687 - INFO - Final memory usage: CPU 1907.4 MB, GPU 104.2 MB
2024-12-28 14:37:30,687 - INFO - Model training completed in 60.47s
2024-12-28 14:37:30,749 - INFO - Prediction completed in 0.06s
2024-12-28 14:37:30,766 - INFO - Poison rate 0.05 completed in 69.91s
2024-12-28 14:37:30,767 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:37:30,769 - INFO - Total number of labels flipped: 1376
2024-12-28 14:37:30,769 - INFO - Label flipping completed in 0.00s
2024-12-28 14:37:30,769 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:37:30,769 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:37:31,942 - INFO - Feature scaling completed in 1.17s
2024-12-28 14:37:31,942 - INFO - Starting feature selection (k=50)
2024-12-28 14:37:31,972 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:37:31,973 - INFO - Starting anomaly detection
2024-12-28 14:37:40,093 - INFO - Anomaly detection completed in 8.12s
2024-12-28 14:37:40,093 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:37:40,093 - INFO - Total fit_transform time: 9.32s
2024-12-28 14:37:40,094 - INFO - Training set processing completed in 9.32s
2024-12-28 14:37:40,094 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:37:40,095 - INFO - Memory usage at start_fit: CPU 1868.8 MB, GPU 104.1 MB
2024-12-28 14:37:40,095 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:37:40,102 - INFO - Number of unique classes: 43
2024-12-28 14:37:40,250 - INFO - Fitted scaler and transformed data
2024-12-28 14:37:40,251 - INFO - Scaling time: 0.15s
2024-12-28 14:37:41,364 - INFO - Epoch 1/500, Train Loss: 9.2403, Val Loss: 6.5714
2024-12-28 14:37:42,559 - INFO - Epoch 2/500, Train Loss: 4.7005, Val Loss: 5.3200
2024-12-28 14:37:43,611 - INFO - Epoch 3/500, Train Loss: 3.6461, Val Loss: 4.3268
2024-12-28 14:37:44,686 - INFO - Epoch 4/500, Train Loss: 2.9708, Val Loss: 3.8835
2024-12-28 14:37:45,741 - INFO - Epoch 5/500, Train Loss: 2.4920, Val Loss: 3.3330
2024-12-28 14:37:46,815 - INFO - Epoch 6/500, Train Loss: 2.1557, Val Loss: 3.0073
2024-12-28 14:37:47,939 - INFO - Epoch 7/500, Train Loss: 1.9005, Val Loss: 2.7375
2024-12-28 14:37:49,049 - INFO - Epoch 8/500, Train Loss: 1.6947, Val Loss: 2.4695
2024-12-28 14:37:50,171 - INFO - Epoch 9/500, Train Loss: 1.5570, Val Loss: 2.3069
2024-12-28 14:37:51,373 - INFO - Epoch 10/500, Train Loss: 1.4418, Val Loss: 2.2647
2024-12-28 14:37:52,414 - INFO - Epoch 11/500, Train Loss: 1.3544, Val Loss: 2.1262
2024-12-28 14:37:53,383 - INFO - Epoch 12/500, Train Loss: 1.2815, Val Loss: 2.0423
2024-12-28 14:37:54,399 - INFO - Epoch 13/500, Train Loss: 1.2217, Val Loss: 1.9567
2024-12-28 14:37:55,572 - INFO - Epoch 14/500, Train Loss: 1.1773, Val Loss: 1.9173
2024-12-28 14:37:56,658 - INFO - Epoch 15/500, Train Loss: 1.1313, Val Loss: 1.8916
2024-12-28 14:37:57,796 - INFO - Epoch 16/500, Train Loss: 1.0936, Val Loss: 1.8186
2024-12-28 14:37:58,984 - INFO - Epoch 17/500, Train Loss: 1.0629, Val Loss: 1.8020
2024-12-28 14:38:00,162 - INFO - Epoch 18/500, Train Loss: 1.0347, Val Loss: 1.8118
2024-12-28 14:38:01,371 - INFO - Epoch 19/500, Train Loss: 1.0093, Val Loss: 1.7461
2024-12-28 14:38:02,540 - INFO - Epoch 20/500, Train Loss: 0.9924, Val Loss: 1.7221
2024-12-28 14:38:03,776 - INFO - Epoch 21/500, Train Loss: 0.9695, Val Loss: 1.7561
2024-12-28 14:38:04,980 - INFO - Epoch 22/500, Train Loss: 0.9563, Val Loss: 1.6928
2024-12-28 14:38:06,109 - INFO - Epoch 23/500, Train Loss: 0.9437, Val Loss: 1.6905
2024-12-28 14:38:07,199 - INFO - Epoch 24/500, Train Loss: 0.9239, Val Loss: 1.6480
2024-12-28 14:38:08,330 - INFO - Epoch 25/500, Train Loss: 0.9104, Val Loss: 1.6279
2024-12-28 14:38:09,402 - INFO - Epoch 26/500, Train Loss: 0.8985, Val Loss: 1.6433
2024-12-28 14:38:10,416 - INFO - Epoch 27/500, Train Loss: 0.8932, Val Loss: 1.6169
2024-12-28 14:38:11,475 - INFO - Epoch 28/500, Train Loss: 0.8836, Val Loss: 1.6377
2024-12-28 14:38:12,586 - INFO - Epoch 29/500, Train Loss: 0.8716, Val Loss: 1.6575
2024-12-28 14:38:13,646 - INFO - Epoch 30/500, Train Loss: 0.8681, Val Loss: 1.6045
2024-12-28 14:38:14,776 - INFO - Epoch 31/500, Train Loss: 0.8587, Val Loss: 1.5864
2024-12-28 14:38:15,831 - INFO - Epoch 32/500, Train Loss: 0.8562, Val Loss: 1.5635
2024-12-28 14:38:16,895 - INFO - Epoch 33/500, Train Loss: 0.8445, Val Loss: 1.5619
2024-12-28 14:38:17,987 - INFO - Epoch 34/500, Train Loss: 0.8351, Val Loss: 1.5618
2024-12-28 14:38:19,233 - INFO - Epoch 35/500, Train Loss: 0.8360, Val Loss: 1.5466
2024-12-28 14:38:20,428 - INFO - Epoch 36/500, Train Loss: 0.8277, Val Loss: 1.5412
2024-12-28 14:38:21,588 - INFO - Epoch 37/500, Train Loss: 0.8238, Val Loss: 1.5104
2024-12-28 14:38:22,757 - INFO - Epoch 38/500, Train Loss: 0.8211, Val Loss: 1.5320
2024-12-28 14:38:23,959 - INFO - Epoch 39/500, Train Loss: 0.8198, Val Loss: 1.5631
2024-12-28 14:38:25,173 - INFO - Epoch 40/500, Train Loss: 0.8184, Val Loss: 1.5462
2024-12-28 14:38:26,389 - INFO - Epoch 41/500, Train Loss: 0.8198, Val Loss: 1.5349
2024-12-28 14:38:27,650 - INFO - Epoch 42/500, Train Loss: 0.8098, Val Loss: 1.4911
2024-12-28 14:38:28,826 - INFO - Epoch 43/500, Train Loss: 0.8006, Val Loss: 1.5326
2024-12-28 14:38:30,082 - INFO - Epoch 44/500, Train Loss: 0.8007, Val Loss: 1.5220
2024-12-28 14:38:31,300 - INFO - Epoch 45/500, Train Loss: 0.7980, Val Loss: 1.5195
2024-12-28 14:38:32,499 - INFO - Epoch 46/500, Train Loss: 0.8006, Val Loss: 1.5125
2024-12-28 14:38:33,692 - INFO - Epoch 47/500, Train Loss: 0.7935, Val Loss: 1.5647
2024-12-28 14:38:33,692 - INFO - Early stopping triggered at epoch 47
2024-12-28 14:38:33,692 - INFO - Training completed in 53.60s
2024-12-28 14:38:33,693 - INFO - Final memory usage: CPU 1915.4 MB, GPU 104.2 MB
2024-12-28 14:38:33,693 - INFO - Model training completed in 53.60s
2024-12-28 14:38:33,755 - INFO - Prediction completed in 0.06s
2024-12-28 14:38:33,766 - INFO - Poison rate 0.07 completed in 63.00s
2024-12-28 14:38:33,766 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:38:33,773 - INFO - Total number of labels flipped: 1958
2024-12-28 14:38:33,773 - INFO - Label flipping completed in 0.01s
2024-12-28 14:38:33,774 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:38:33,774 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:38:34,948 - INFO - Feature scaling completed in 1.17s
2024-12-28 14:38:34,948 - INFO - Starting feature selection (k=50)
2024-12-28 14:38:34,975 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:38:34,975 - INFO - Starting anomaly detection
2024-12-28 14:38:42,869 - INFO - Anomaly detection completed in 7.89s
2024-12-28 14:38:42,869 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:38:42,869 - INFO - Total fit_transform time: 9.09s
2024-12-28 14:38:42,869 - INFO - Training set processing completed in 9.10s
2024-12-28 14:38:42,869 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:38:42,871 - INFO - Memory usage at start_fit: CPU 1876.8 MB, GPU 104.1 MB
2024-12-28 14:38:42,871 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:38:42,878 - INFO - Number of unique classes: 43
2024-12-28 14:38:43,021 - INFO - Fitted scaler and transformed data
2024-12-28 14:38:43,021 - INFO - Scaling time: 0.14s
2024-12-28 14:38:44,033 - INFO - Epoch 1/500, Train Loss: 10.5492, Val Loss: 7.3722
2024-12-28 14:38:45,200 - INFO - Epoch 2/500, Train Loss: 5.7103, Val Loss: 5.8177
2024-12-28 14:38:46,346 - INFO - Epoch 3/500, Train Loss: 4.3955, Val Loss: 4.8450
2024-12-28 14:38:47,429 - INFO - Epoch 4/500, Train Loss: 3.4991, Val Loss: 4.2290
2024-12-28 14:38:48,468 - INFO - Epoch 5/500, Train Loss: 2.8819, Val Loss: 3.5570
2024-12-28 14:38:49,496 - INFO - Epoch 6/500, Train Loss: 2.4320, Val Loss: 3.2106
2024-12-28 14:38:50,570 - INFO - Epoch 7/500, Train Loss: 2.1299, Val Loss: 2.8652
2024-12-28 14:38:51,692 - INFO - Epoch 8/500, Train Loss: 1.8945, Val Loss: 2.6220
2024-12-28 14:38:52,747 - INFO - Epoch 9/500, Train Loss: 1.7453, Val Loss: 2.4602
2024-12-28 14:38:53,825 - INFO - Epoch 10/500, Train Loss: 1.6074, Val Loss: 2.3320
2024-12-28 14:38:55,019 - INFO - Epoch 11/500, Train Loss: 1.5113, Val Loss: 2.2417
2024-12-28 14:38:56,240 - INFO - Epoch 12/500, Train Loss: 1.4300, Val Loss: 2.1773
2024-12-28 14:38:57,480 - INFO - Epoch 13/500, Train Loss: 1.3659, Val Loss: 2.0909
2024-12-28 14:38:58,695 - INFO - Epoch 14/500, Train Loss: 1.3163, Val Loss: 2.0244
2024-12-28 14:38:59,893 - INFO - Epoch 15/500, Train Loss: 1.2665, Val Loss: 2.0176
2024-12-28 14:39:00,990 - INFO - Epoch 16/500, Train Loss: 1.2289, Val Loss: 2.0253
2024-12-28 14:39:02,138 - INFO - Epoch 17/500, Train Loss: 1.1942, Val Loss: 1.9264
2024-12-28 14:39:03,174 - INFO - Epoch 18/500, Train Loss: 1.1619, Val Loss: 1.8611
2024-12-28 14:39:04,248 - INFO - Epoch 19/500, Train Loss: 1.1335, Val Loss: 1.8453
2024-12-28 14:39:05,363 - INFO - Epoch 20/500, Train Loss: 1.1162, Val Loss: 1.8487
2024-12-28 14:39:06,503 - INFO - Epoch 21/500, Train Loss: 1.0904, Val Loss: 1.8119
2024-12-28 14:39:07,698 - INFO - Epoch 22/500, Train Loss: 1.0716, Val Loss: 1.7952
2024-12-28 14:39:08,880 - INFO - Epoch 23/500, Train Loss: 1.0536, Val Loss: 1.8053
2024-12-28 14:39:09,863 - INFO - Epoch 24/500, Train Loss: 1.0420, Val Loss: 1.7663
2024-12-28 14:39:10,807 - INFO - Epoch 25/500, Train Loss: 1.0222, Val Loss: 1.7107
2024-12-28 14:39:11,860 - INFO - Epoch 26/500, Train Loss: 1.0179, Val Loss: 1.7264
2024-12-28 14:39:12,875 - INFO - Epoch 27/500, Train Loss: 0.9986, Val Loss: 1.7492
2024-12-28 14:39:13,893 - INFO - Epoch 28/500, Train Loss: 0.9879, Val Loss: 1.6815
2024-12-28 14:39:15,024 - INFO - Epoch 29/500, Train Loss: 0.9761, Val Loss: 1.7256
2024-12-28 14:39:16,223 - INFO - Epoch 30/500, Train Loss: 0.9741, Val Loss: 1.6776
2024-12-28 14:39:17,453 - INFO - Epoch 31/500, Train Loss: 0.9637, Val Loss: 1.7084
2024-12-28 14:39:18,655 - INFO - Epoch 32/500, Train Loss: 0.9566, Val Loss: 1.6852
2024-12-28 14:39:19,877 - INFO - Epoch 33/500, Train Loss: 0.9427, Val Loss: 1.6947
2024-12-28 14:39:21,142 - INFO - Epoch 34/500, Train Loss: 0.9487, Val Loss: 1.7153
2024-12-28 14:39:22,329 - INFO - Epoch 35/500, Train Loss: 0.9302, Val Loss: 1.6792
2024-12-28 14:39:22,329 - INFO - Early stopping triggered at epoch 35
2024-12-28 14:39:22,329 - INFO - Training completed in 39.46s
2024-12-28 14:39:22,330 - INFO - Final memory usage: CPU 1922.3 MB, GPU 104.2 MB
2024-12-28 14:39:22,330 - INFO - Model training completed in 39.46s
2024-12-28 14:39:22,414 - INFO - Prediction completed in 0.08s
2024-12-28 14:39:22,440 - INFO - Poison rate 0.1 completed in 48.67s
2024-12-28 14:39:22,440 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:39:22,447 - INFO - Total number of labels flipped: 3930
2024-12-28 14:39:22,447 - INFO - Label flipping completed in 0.01s
2024-12-28 14:39:22,447 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:39:22,447 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:39:23,647 - INFO - Feature scaling completed in 1.20s
2024-12-28 14:39:23,647 - INFO - Starting feature selection (k=50)
2024-12-28 14:39:23,678 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:39:23,679 - INFO - Starting anomaly detection
2024-12-28 14:39:31,183 - INFO - Anomaly detection completed in 7.50s
2024-12-28 14:39:31,184 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:39:31,184 - INFO - Total fit_transform time: 8.74s
2024-12-28 14:39:31,184 - INFO - Training set processing completed in 8.74s
2024-12-28 14:39:31,184 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:39:31,185 - INFO - Memory usage at start_fit: CPU 1883.7 MB, GPU 104.1 MB
2024-12-28 14:39:31,185 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:39:31,193 - INFO - Number of unique classes: 43
2024-12-28 14:39:31,349 - INFO - Fitted scaler and transformed data
2024-12-28 14:39:31,349 - INFO - Scaling time: 0.15s
2024-12-28 14:39:32,271 - INFO - Epoch 1/500, Train Loss: 13.3623, Val Loss: 9.9057
2024-12-28 14:39:33,263 - INFO - Epoch 2/500, Train Loss: 8.0563, Val Loss: 7.5291
2024-12-28 14:39:34,253 - INFO - Epoch 3/500, Train Loss: 5.9212, Val Loss: 5.9692
2024-12-28 14:39:35,405 - INFO - Epoch 4/500, Train Loss: 4.4693, Val Loss: 4.6076
2024-12-28 14:39:36,481 - INFO - Epoch 5/500, Train Loss: 3.4804, Val Loss: 3.8436
2024-12-28 14:39:37,510 - INFO - Epoch 6/500, Train Loss: 2.9100, Val Loss: 3.4015
2024-12-28 14:39:38,553 - INFO - Epoch 7/500, Train Loss: 2.5090, Val Loss: 3.1123
2024-12-28 14:39:39,676 - INFO - Epoch 8/500, Train Loss: 2.2627, Val Loss: 2.8550
2024-12-28 14:39:40,874 - INFO - Epoch 9/500, Train Loss: 2.0825, Val Loss: 2.6365
2024-12-28 14:39:42,025 - INFO - Epoch 10/500, Train Loss: 1.9438, Val Loss: 2.5491
2024-12-28 14:39:43,178 - INFO - Epoch 11/500, Train Loss: 1.8386, Val Loss: 2.5046
2024-12-28 14:39:44,324 - INFO - Epoch 12/500, Train Loss: 1.7611, Val Loss: 2.3710
2024-12-28 14:39:45,501 - INFO - Epoch 13/500, Train Loss: 1.6850, Val Loss: 2.3401
2024-12-28 14:39:46,674 - INFO - Epoch 14/500, Train Loss: 1.6232, Val Loss: 2.2765
2024-12-28 14:39:47,859 - INFO - Epoch 15/500, Train Loss: 1.5675, Val Loss: 2.1916
2024-12-28 14:39:49,028 - INFO - Epoch 16/500, Train Loss: 1.5317, Val Loss: 2.1484
2024-12-28 14:39:50,122 - INFO - Epoch 17/500, Train Loss: 1.4814, Val Loss: 2.1372
2024-12-28 14:39:51,245 - INFO - Epoch 18/500, Train Loss: 1.4441, Val Loss: 2.1295
2024-12-28 14:39:52,273 - INFO - Epoch 19/500, Train Loss: 1.4133, Val Loss: 2.0977
2024-12-28 14:39:53,339 - INFO - Epoch 20/500, Train Loss: 1.3970, Val Loss: 2.0141
2024-12-28 14:39:54,408 - INFO - Epoch 21/500, Train Loss: 1.3683, Val Loss: 1.9960
2024-12-28 14:39:55,504 - INFO - Epoch 22/500, Train Loss: 1.3529, Val Loss: 2.0209
2024-12-28 14:39:56,631 - INFO - Epoch 23/500, Train Loss: 1.3331, Val Loss: 1.9380
2024-12-28 14:39:57,730 - INFO - Epoch 24/500, Train Loss: 1.3123, Val Loss: 1.9948
2024-12-28 14:39:58,915 - INFO - Epoch 25/500, Train Loss: 1.2977, Val Loss: 1.9410
2024-12-28 14:40:00,002 - INFO - Epoch 26/500, Train Loss: 1.2853, Val Loss: 1.9377
2024-12-28 14:40:01,011 - INFO - Epoch 27/500, Train Loss: 1.2601, Val Loss: 1.8927
2024-12-28 14:40:02,086 - INFO - Epoch 28/500, Train Loss: 1.2485, Val Loss: 1.8931
2024-12-28 14:40:03,272 - INFO - Epoch 29/500, Train Loss: 1.2446, Val Loss: 1.9185
2024-12-28 14:40:04,517 - INFO - Epoch 30/500, Train Loss: 1.2311, Val Loss: 1.9002
2024-12-28 14:40:05,619 - INFO - Epoch 31/500, Train Loss: 1.2189, Val Loss: 1.8850
2024-12-28 14:40:06,704 - INFO - Epoch 32/500, Train Loss: 1.2084, Val Loss: 1.8701
2024-12-28 14:40:07,793 - INFO - Epoch 33/500, Train Loss: 1.2022, Val Loss: 1.8956
2024-12-28 14:40:08,864 - INFO - Epoch 34/500, Train Loss: 1.1960, Val Loss: 1.8564
2024-12-28 14:40:09,891 - INFO - Epoch 35/500, Train Loss: 1.1913, Val Loss: 1.8264
2024-12-28 14:40:10,977 - INFO - Epoch 36/500, Train Loss: 1.1845, Val Loss: 1.8130
2024-12-28 14:40:12,035 - INFO - Epoch 37/500, Train Loss: 1.1751, Val Loss: 1.8132
2024-12-28 14:40:13,079 - INFO - Epoch 38/500, Train Loss: 1.1733, Val Loss: 1.8060
2024-12-28 14:40:14,194 - INFO - Epoch 39/500, Train Loss: 1.1615, Val Loss: 1.8066
2024-12-28 14:40:15,383 - INFO - Epoch 40/500, Train Loss: 1.1499, Val Loss: 1.8011
2024-12-28 14:40:16,444 - INFO - Epoch 41/500, Train Loss: 1.1492, Val Loss: 1.7948
2024-12-28 14:40:17,484 - INFO - Epoch 42/500, Train Loss: 1.1443, Val Loss: 1.8039
2024-12-28 14:40:18,602 - INFO - Epoch 43/500, Train Loss: 1.1355, Val Loss: 1.7744
2024-12-28 14:40:19,793 - INFO - Epoch 44/500, Train Loss: 1.1381, Val Loss: 1.7233
2024-12-28 14:40:21,007 - INFO - Epoch 45/500, Train Loss: 1.1318, Val Loss: 1.7519
2024-12-28 14:40:22,090 - INFO - Epoch 46/500, Train Loss: 1.1350, Val Loss: 1.7233
2024-12-28 14:40:23,158 - INFO - Epoch 47/500, Train Loss: 1.1205, Val Loss: 1.7718
2024-12-28 14:40:24,296 - INFO - Epoch 48/500, Train Loss: 1.1119, Val Loss: 1.7939
2024-12-28 14:40:25,492 - INFO - Epoch 49/500, Train Loss: 1.1128, Val Loss: 1.7839
2024-12-28 14:40:25,493 - INFO - Early stopping triggered at epoch 49
2024-12-28 14:40:25,493 - INFO - Training completed in 54.31s
2024-12-28 14:40:25,494 - INFO - Final memory usage: CPU 1924.0 MB, GPU 104.2 MB
2024-12-28 14:40:25,494 - INFO - Model training completed in 54.31s
2024-12-28 14:40:25,556 - INFO - Prediction completed in 0.06s
2024-12-28 14:40:25,567 - INFO - Poison rate 0.2 completed in 63.13s
2024-12-28 14:40:25,569 - INFO - Loaded 63 existing results
2024-12-28 14:40:25,569 - INFO - Total results to save: 70
2024-12-28 14:40:25,570 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:40:25,583 - INFO - Saved 70 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:40:25,584 - INFO - Total evaluation time: 434.16s
2024-12-28 14:40:25,596 - INFO - 
Progress: 11.5% - Evaluating GTSRB with LogisticRegression (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:40:25,833 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:40:25,833 - INFO - Dataset type: image
2024-12-28 14:40:25,833 - INFO - Sample size: 39209
2024-12-28 14:40:25,833 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:40:25,834 - INFO - Loading datasets...
2024-12-28 14:40:44,440 - INFO - Dataset loading completed in 18.61s
2024-12-28 14:40:44,440 - INFO - Extracting validation features...
2024-12-28 14:40:44,440 - INFO - Extracting features from 4435 samples...
2024-12-28 14:40:45,231 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:40:45,234 - INFO - Validation feature extraction completed in 0.79s
2024-12-28 14:40:45,235 - INFO - Extracting training features...
2024-12-28 14:40:45,235 - INFO - Extracting features from 19755 samples...
2024-12-28 14:40:47,988 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:40:48,003 - INFO - Training feature extraction completed in 2.77s
2024-12-28 14:40:48,003 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 14:40:48,004 - INFO - Using device: cuda
2024-12-28 14:40:48,004 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:40:48,005 - INFO - Training set processing completed in 0.00s
2024-12-28 14:40:48,005 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:40:48,006 - INFO - Memory usage at start_fit: CPU 1866.6 MB, GPU 103.1 MB
2024-12-28 14:40:48,007 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:40:48,015 - INFO - Number of unique classes: 43
2024-12-28 14:40:48,166 - INFO - Fitted scaler and transformed data
2024-12-28 14:40:48,166 - INFO - Scaling time: 0.15s
2024-12-28 14:40:48,978 - INFO - Epoch 1/1000, Train Loss: 1.8988, Val Loss: 1.3682
2024-12-28 14:40:49,602 - INFO - Epoch 2/1000, Train Loss: 1.2011, Val Loss: 1.1197
2024-12-28 14:40:50,168 - INFO - Epoch 3/1000, Train Loss: 1.0140, Val Loss: 1.0060
2024-12-28 14:40:50,675 - INFO - Epoch 4/1000, Train Loss: 0.9179, Val Loss: 0.9491
2024-12-28 14:40:51,238 - INFO - Epoch 5/1000, Train Loss: 0.8601, Val Loss: 0.9082
2024-12-28 14:40:51,824 - INFO - Epoch 6/1000, Train Loss: 0.8215, Val Loss: 0.8843
2024-12-28 14:40:52,463 - INFO - Epoch 7/1000, Train Loss: 0.7944, Val Loss: 0.8650
2024-12-28 14:40:53,058 - INFO - Epoch 8/1000, Train Loss: 0.7761, Val Loss: 0.8472
2024-12-28 14:40:53,588 - INFO - Epoch 9/1000, Train Loss: 0.7635, Val Loss: 0.8302
2024-12-28 14:40:54,151 - INFO - Epoch 10/1000, Train Loss: 0.7511, Val Loss: 0.8257
2024-12-28 14:40:54,867 - INFO - Epoch 11/1000, Train Loss: 0.7444, Val Loss: 0.8202
2024-12-28 14:40:55,632 - INFO - Epoch 12/1000, Train Loss: 0.7362, Val Loss: 0.8215
2024-12-28 14:40:56,498 - INFO - Epoch 13/1000, Train Loss: 0.7308, Val Loss: 0.8133
2024-12-28 14:40:57,260 - INFO - Epoch 14/1000, Train Loss: 0.7294, Val Loss: 0.8094
2024-12-28 14:40:58,103 - INFO - Epoch 15/1000, Train Loss: 0.7255, Val Loss: 0.8191
2024-12-28 14:40:58,913 - INFO - Epoch 16/1000, Train Loss: 0.7221, Val Loss: 0.8107
2024-12-28 14:40:59,635 - INFO - Epoch 17/1000, Train Loss: 0.7200, Val Loss: 0.8077
2024-12-28 14:41:00,200 - INFO - Epoch 18/1000, Train Loss: 0.7191, Val Loss: 0.8063
2024-12-28 14:41:00,834 - INFO - Epoch 19/1000, Train Loss: 0.7190, Val Loss: 0.8034
2024-12-28 14:41:01,478 - INFO - Epoch 20/1000, Train Loss: 0.7160, Val Loss: 0.8056
2024-12-28 14:41:02,074 - INFO - Epoch 21/1000, Train Loss: 0.7148, Val Loss: 0.7998
2024-12-28 14:41:02,675 - INFO - Epoch 22/1000, Train Loss: 0.7133, Val Loss: 0.7973
2024-12-28 14:41:03,224 - INFO - Epoch 23/1000, Train Loss: 0.7139, Val Loss: 0.8109
2024-12-28 14:41:03,753 - INFO - Epoch 24/1000, Train Loss: 0.7142, Val Loss: 0.8003
2024-12-28 14:41:04,400 - INFO - Epoch 25/1000, Train Loss: 0.7107, Val Loss: 0.8039
2024-12-28 14:41:05,026 - INFO - Epoch 26/1000, Train Loss: 0.7109, Val Loss: 0.8050
2024-12-28 14:41:05,607 - INFO - Epoch 27/1000, Train Loss: 0.7128, Val Loss: 0.7969
2024-12-28 14:41:05,607 - INFO - Early stopping triggered at epoch 27
2024-12-28 14:41:05,607 - INFO - Training completed in 17.60s
2024-12-28 14:41:05,608 - INFO - Final memory usage: CPU 1914.7 MB, GPU 103.6 MB
2024-12-28 14:41:05,608 - INFO - Model training completed in 17.60s
2024-12-28 14:41:05,673 - INFO - Prediction completed in 0.06s
2024-12-28 14:41:05,684 - INFO - Poison rate 0.0 completed in 17.68s
2024-12-28 14:41:05,685 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:41:05,686 - INFO - Total number of labels flipped: 196
2024-12-28 14:41:05,687 - INFO - Label flipping completed in 0.00s
2024-12-28 14:41:05,687 - INFO - Training set processing completed in 0.00s
2024-12-28 14:41:05,687 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:41:05,688 - INFO - Memory usage at start_fit: CPU 1876.1 MB, GPU 103.5 MB
2024-12-28 14:41:05,688 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:41:05,693 - INFO - Number of unique classes: 43
2024-12-28 14:41:05,842 - INFO - Fitted scaler and transformed data
2024-12-28 14:41:05,842 - INFO - Scaling time: 0.15s
2024-12-28 14:41:06,698 - INFO - Epoch 1/1000, Train Loss: 1.9360, Val Loss: 1.4939
2024-12-28 14:41:07,300 - INFO - Epoch 2/1000, Train Loss: 1.2530, Val Loss: 1.2600
2024-12-28 14:41:07,861 - INFO - Epoch 3/1000, Train Loss: 1.0724, Val Loss: 1.1428
2024-12-28 14:41:08,419 - INFO - Epoch 4/1000, Train Loss: 0.9790, Val Loss: 1.0732
2024-12-28 14:41:08,965 - INFO - Epoch 5/1000, Train Loss: 0.9230, Val Loss: 1.0440
2024-12-28 14:41:09,549 - INFO - Epoch 6/1000, Train Loss: 0.8863, Val Loss: 1.0005
2024-12-28 14:41:10,134 - INFO - Epoch 7/1000, Train Loss: 0.8607, Val Loss: 0.9923
2024-12-28 14:41:10,754 - INFO - Epoch 8/1000, Train Loss: 0.8401, Val Loss: 0.9728
2024-12-28 14:41:11,322 - INFO - Epoch 9/1000, Train Loss: 0.8260, Val Loss: 0.9612
2024-12-28 14:41:12,008 - INFO - Epoch 10/1000, Train Loss: 0.8148, Val Loss: 0.9571
2024-12-28 14:41:12,774 - INFO - Epoch 11/1000, Train Loss: 0.8072, Val Loss: 0.9436
2024-12-28 14:41:13,619 - INFO - Epoch 12/1000, Train Loss: 0.8006, Val Loss: 0.9427
2024-12-28 14:41:14,494 - INFO - Epoch 13/1000, Train Loss: 0.7967, Val Loss: 0.9358
2024-12-28 14:41:15,330 - INFO - Epoch 14/1000, Train Loss: 0.7920, Val Loss: 0.9285
2024-12-28 14:41:15,950 - INFO - Epoch 15/1000, Train Loss: 0.7898, Val Loss: 0.9257
2024-12-28 14:41:16,564 - INFO - Epoch 16/1000, Train Loss: 0.7871, Val Loss: 0.9215
2024-12-28 14:41:17,196 - INFO - Epoch 17/1000, Train Loss: 0.7828, Val Loss: 0.9235
2024-12-28 14:41:17,804 - INFO - Epoch 18/1000, Train Loss: 0.7847, Val Loss: 0.9173
2024-12-28 14:41:18,515 - INFO - Epoch 19/1000, Train Loss: 0.7824, Val Loss: 0.9182
2024-12-28 14:41:19,264 - INFO - Epoch 20/1000, Train Loss: 0.7793, Val Loss: 0.9249
2024-12-28 14:41:19,892 - INFO - Epoch 21/1000, Train Loss: 0.7784, Val Loss: 0.9189
2024-12-28 14:41:20,475 - INFO - Epoch 22/1000, Train Loss: 0.7806, Val Loss: 0.9190
2024-12-28 14:41:21,269 - INFO - Epoch 23/1000, Train Loss: 0.7783, Val Loss: 0.9169
2024-12-28 14:41:21,269 - INFO - Early stopping triggered at epoch 23
2024-12-28 14:41:21,269 - INFO - Training completed in 15.58s
2024-12-28 14:41:21,270 - INFO - Final memory usage: CPU 1914.7 MB, GPU 103.6 MB
2024-12-28 14:41:21,271 - INFO - Model training completed in 15.58s
2024-12-28 14:41:21,353 - INFO - Prediction completed in 0.08s
2024-12-28 14:41:21,381 - INFO - Poison rate 0.01 completed in 15.70s
2024-12-28 14:41:21,381 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:41:21,385 - INFO - Total number of labels flipped: 587
2024-12-28 14:41:21,385 - INFO - Label flipping completed in 0.00s
2024-12-28 14:41:21,385 - INFO - Training set processing completed in 0.00s
2024-12-28 14:41:21,385 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:41:21,387 - INFO - Memory usage at start_fit: CPU 1876.1 MB, GPU 103.5 MB
2024-12-28 14:41:21,387 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:41:21,395 - INFO - Number of unique classes: 43
2024-12-28 14:41:21,541 - INFO - Fitted scaler and transformed data
2024-12-28 14:41:21,541 - INFO - Scaling time: 0.14s
2024-12-28 14:41:22,256 - INFO - Epoch 1/1000, Train Loss: 2.0280, Val Loss: 1.5126
2024-12-28 14:41:23,145 - INFO - Epoch 2/1000, Train Loss: 1.3753, Val Loss: 1.2759
2024-12-28 14:41:24,042 - INFO - Epoch 3/1000, Train Loss: 1.2038, Val Loss: 1.1675
2024-12-28 14:41:24,756 - INFO - Epoch 4/1000, Train Loss: 1.1079, Val Loss: 1.1026
2024-12-28 14:41:25,491 - INFO - Epoch 5/1000, Train Loss: 1.0513, Val Loss: 1.0694
2024-12-28 14:41:26,255 - INFO - Epoch 6/1000, Train Loss: 1.0114, Val Loss: 1.0506
2024-12-28 14:41:27,092 - INFO - Epoch 7/1000, Train Loss: 0.9856, Val Loss: 1.0250
2024-12-28 14:41:27,951 - INFO - Epoch 8/1000, Train Loss: 0.9671, Val Loss: 1.0122
2024-12-28 14:41:28,813 - INFO - Epoch 9/1000, Train Loss: 0.9500, Val Loss: 0.9974
2024-12-28 14:41:29,672 - INFO - Epoch 10/1000, Train Loss: 0.9358, Val Loss: 0.9836
2024-12-28 14:41:30,472 - INFO - Epoch 11/1000, Train Loss: 0.9251, Val Loss: 0.9780
2024-12-28 14:41:31,253 - INFO - Epoch 12/1000, Train Loss: 0.9171, Val Loss: 0.9729
2024-12-28 14:41:32,047 - INFO - Epoch 13/1000, Train Loss: 0.9126, Val Loss: 0.9589
2024-12-28 14:41:32,752 - INFO - Epoch 14/1000, Train Loss: 0.9038, Val Loss: 0.9573
2024-12-28 14:41:33,204 - INFO - Epoch 15/1000, Train Loss: 0.8998, Val Loss: 0.9629
2024-12-28 14:41:33,649 - INFO - Epoch 16/1000, Train Loss: 0.8974, Val Loss: 0.9530
2024-12-28 14:41:34,104 - INFO - Epoch 17/1000, Train Loss: 0.8932, Val Loss: 0.9556
2024-12-28 14:41:34,546 - INFO - Epoch 18/1000, Train Loss: 0.8906, Val Loss: 0.9564
2024-12-28 14:41:34,991 - INFO - Epoch 19/1000, Train Loss: 0.8887, Val Loss: 0.9515
2024-12-28 14:41:35,452 - INFO - Epoch 20/1000, Train Loss: 0.8871, Val Loss: 0.9493
2024-12-28 14:41:35,922 - INFO - Epoch 21/1000, Train Loss: 0.8851, Val Loss: 0.9454
2024-12-28 14:41:36,406 - INFO - Epoch 22/1000, Train Loss: 0.8838, Val Loss: 0.9370
2024-12-28 14:41:36,861 - INFO - Epoch 23/1000, Train Loss: 0.8809, Val Loss: 0.9426
2024-12-28 14:41:37,343 - INFO - Epoch 24/1000, Train Loss: 0.8803, Val Loss: 0.9382
2024-12-28 14:41:37,779 - INFO - Epoch 25/1000, Train Loss: 0.8791, Val Loss: 0.9470
2024-12-28 14:41:38,226 - INFO - Epoch 26/1000, Train Loss: 0.8790, Val Loss: 0.9380
2024-12-28 14:41:38,671 - INFO - Epoch 27/1000, Train Loss: 0.8766, Val Loss: 0.9461
2024-12-28 14:41:38,671 - INFO - Early stopping triggered at epoch 27
2024-12-28 14:41:38,671 - INFO - Training completed in 17.28s
2024-12-28 14:41:38,672 - INFO - Final memory usage: CPU 1914.7 MB, GPU 103.6 MB
2024-12-28 14:41:38,672 - INFO - Model training completed in 17.29s
2024-12-28 14:41:38,735 - INFO - Prediction completed in 0.06s
2024-12-28 14:41:38,746 - INFO - Poison rate 0.03 completed in 17.36s
2024-12-28 14:41:38,746 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:41:38,748 - INFO - Total number of labels flipped: 980
2024-12-28 14:41:38,748 - INFO - Label flipping completed in 0.00s
2024-12-28 14:41:38,748 - INFO - Training set processing completed in 0.00s
2024-12-28 14:41:38,748 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:41:38,749 - INFO - Memory usage at start_fit: CPU 1876.1 MB, GPU 103.5 MB
2024-12-28 14:41:38,750 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:41:38,759 - INFO - Number of unique classes: 43
2024-12-28 14:41:38,932 - INFO - Fitted scaler and transformed data
2024-12-28 14:41:38,933 - INFO - Scaling time: 0.17s
2024-12-28 14:41:39,396 - INFO - Epoch 1/1000, Train Loss: 2.1011, Val Loss: 1.6192
2024-12-28 14:41:39,864 - INFO - Epoch 2/1000, Train Loss: 1.4744, Val Loss: 1.3835
2024-12-28 14:41:40,301 - INFO - Epoch 3/1000, Train Loss: 1.3021, Val Loss: 1.2826
2024-12-28 14:41:40,802 - INFO - Epoch 4/1000, Train Loss: 1.2143, Val Loss: 1.2106
2024-12-28 14:41:41,245 - INFO - Epoch 5/1000, Train Loss: 1.1533, Val Loss: 1.1775
2024-12-28 14:41:41,721 - INFO - Epoch 6/1000, Train Loss: 1.1142, Val Loss: 1.1454
2024-12-28 14:41:42,173 - INFO - Epoch 7/1000, Train Loss: 1.0839, Val Loss: 1.1184
2024-12-28 14:41:42,634 - INFO - Epoch 8/1000, Train Loss: 1.0586, Val Loss: 1.1064
2024-12-28 14:41:43,080 - INFO - Epoch 9/1000, Train Loss: 1.0426, Val Loss: 1.0847
2024-12-28 14:41:43,573 - INFO - Epoch 10/1000, Train Loss: 1.0291, Val Loss: 1.0705
2024-12-28 14:41:44,023 - INFO - Epoch 11/1000, Train Loss: 1.0142, Val Loss: 1.0736
2024-12-28 14:41:44,487 - INFO - Epoch 12/1000, Train Loss: 1.0072, Val Loss: 1.0653
2024-12-28 14:41:44,949 - INFO - Epoch 13/1000, Train Loss: 0.9988, Val Loss: 1.0540
2024-12-28 14:41:45,403 - INFO - Epoch 14/1000, Train Loss: 0.9884, Val Loss: 1.0600
2024-12-28 14:41:45,896 - INFO - Epoch 15/1000, Train Loss: 0.9825, Val Loss: 1.0494
2024-12-28 14:41:46,336 - INFO - Epoch 16/1000, Train Loss: 0.9780, Val Loss: 1.0400
2024-12-28 14:41:46,784 - INFO - Epoch 17/1000, Train Loss: 0.9725, Val Loss: 1.0360
2024-12-28 14:41:47,241 - INFO - Epoch 18/1000, Train Loss: 0.9674, Val Loss: 1.0315
2024-12-28 14:41:47,692 - INFO - Epoch 19/1000, Train Loss: 0.9654, Val Loss: 1.0338
2024-12-28 14:41:48,161 - INFO - Epoch 20/1000, Train Loss: 0.9614, Val Loss: 1.0290
2024-12-28 14:41:48,597 - INFO - Epoch 21/1000, Train Loss: 0.9578, Val Loss: 1.0218
2024-12-28 14:41:49,042 - INFO - Epoch 22/1000, Train Loss: 0.9550, Val Loss: 1.0123
2024-12-28 14:41:49,502 - INFO - Epoch 23/1000, Train Loss: 0.9515, Val Loss: 1.0138
2024-12-28 14:41:49,944 - INFO - Epoch 24/1000, Train Loss: 0.9499, Val Loss: 1.0167
2024-12-28 14:41:50,437 - INFO - Epoch 25/1000, Train Loss: 0.9474, Val Loss: 1.0062
2024-12-28 14:41:50,903 - INFO - Epoch 26/1000, Train Loss: 0.9479, Val Loss: 1.0143
2024-12-28 14:41:51,384 - INFO - Epoch 27/1000, Train Loss: 0.9451, Val Loss: 1.0169
2024-12-28 14:41:51,855 - INFO - Epoch 28/1000, Train Loss: 0.9418, Val Loss: 1.0039
2024-12-28 14:41:52,329 - INFO - Epoch 29/1000, Train Loss: 0.9428, Val Loss: 1.0082
2024-12-28 14:41:52,817 - INFO - Epoch 30/1000, Train Loss: 0.9393, Val Loss: 1.0062
2024-12-28 14:41:53,463 - INFO - Epoch 31/1000, Train Loss: 0.9387, Val Loss: 0.9988
2024-12-28 14:41:54,322 - INFO - Epoch 32/1000, Train Loss: 0.9383, Val Loss: 0.9998
2024-12-28 14:41:55,191 - INFO - Epoch 33/1000, Train Loss: 0.9377, Val Loss: 1.0053
2024-12-28 14:41:55,900 - INFO - Epoch 34/1000, Train Loss: 0.9369, Val Loss: 1.0089
2024-12-28 14:41:56,580 - INFO - Epoch 35/1000, Train Loss: 0.9373, Val Loss: 0.9992
2024-12-28 14:41:57,444 - INFO - Epoch 36/1000, Train Loss: 0.9346, Val Loss: 0.9975
2024-12-28 14:41:58,091 - INFO - Epoch 37/1000, Train Loss: 0.9363, Val Loss: 0.9913
2024-12-28 14:41:58,698 - INFO - Epoch 38/1000, Train Loss: 0.9336, Val Loss: 0.9966
2024-12-28 14:41:59,522 - INFO - Epoch 39/1000, Train Loss: 0.9343, Val Loss: 1.0069
2024-12-28 14:42:00,230 - INFO - Epoch 40/1000, Train Loss: 0.9344, Val Loss: 0.9994
2024-12-28 14:42:01,107 - INFO - Epoch 41/1000, Train Loss: 0.9323, Val Loss: 0.9964
2024-12-28 14:42:01,930 - INFO - Epoch 42/1000, Train Loss: 0.9335, Val Loss: 0.9982
2024-12-28 14:42:01,930 - INFO - Early stopping triggered at epoch 42
2024-12-28 14:42:01,930 - INFO - Training completed in 23.18s
2024-12-28 14:42:01,930 - INFO - Final memory usage: CPU 1914.7 MB, GPU 103.6 MB
2024-12-28 14:42:01,931 - INFO - Model training completed in 23.18s
2024-12-28 14:42:01,994 - INFO - Prediction completed in 0.06s
2024-12-28 14:42:02,005 - INFO - Poison rate 0.05 completed in 23.26s
2024-12-28 14:42:02,005 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:42:02,007 - INFO - Total number of labels flipped: 1372
2024-12-28 14:42:02,007 - INFO - Label flipping completed in 0.00s
2024-12-28 14:42:02,007 - INFO - Training set processing completed in 0.00s
2024-12-28 14:42:02,007 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:42:02,008 - INFO - Memory usage at start_fit: CPU 1876.1 MB, GPU 103.5 MB
2024-12-28 14:42:02,008 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:42:02,014 - INFO - Number of unique classes: 43
2024-12-28 14:42:02,151 - INFO - Fitted scaler and transformed data
2024-12-28 14:42:02,152 - INFO - Scaling time: 0.13s
2024-12-28 14:42:03,008 - INFO - Epoch 1/1000, Train Loss: 2.1730, Val Loss: 1.6928
2024-12-28 14:42:03,860 - INFO - Epoch 2/1000, Train Loss: 1.5746, Val Loss: 1.4811
2024-12-28 14:42:04,392 - INFO - Epoch 3/1000, Train Loss: 1.4104, Val Loss: 1.3834
2024-12-28 14:42:04,809 - INFO - Epoch 4/1000, Train Loss: 1.3213, Val Loss: 1.3136
2024-12-28 14:42:05,252 - INFO - Epoch 5/1000, Train Loss: 1.2593, Val Loss: 1.2784
2024-12-28 14:42:05,681 - INFO - Epoch 6/1000, Train Loss: 1.2163, Val Loss: 1.2491
2024-12-28 14:42:06,132 - INFO - Epoch 7/1000, Train Loss: 1.1836, Val Loss: 1.2128
2024-12-28 14:42:06,575 - INFO - Epoch 8/1000, Train Loss: 1.1592, Val Loss: 1.1996
2024-12-28 14:42:07,022 - INFO - Epoch 9/1000, Train Loss: 1.1381, Val Loss: 1.1865
2024-12-28 14:42:07,444 - INFO - Epoch 10/1000, Train Loss: 1.1212, Val Loss: 1.1772
2024-12-28 14:42:07,875 - INFO - Epoch 11/1000, Train Loss: 1.1077, Val Loss: 1.1755
2024-12-28 14:42:08,323 - INFO - Epoch 12/1000, Train Loss: 1.0937, Val Loss: 1.1575
2024-12-28 14:42:08,729 - INFO - Epoch 13/1000, Train Loss: 1.0827, Val Loss: 1.1438
2024-12-28 14:42:09,141 - INFO - Epoch 14/1000, Train Loss: 1.0719, Val Loss: 1.1359
2024-12-28 14:42:09,595 - INFO - Epoch 15/1000, Train Loss: 1.0631, Val Loss: 1.1364
2024-12-28 14:42:10,042 - INFO - Epoch 16/1000, Train Loss: 1.0579, Val Loss: 1.1320
2024-12-28 14:42:10,496 - INFO - Epoch 17/1000, Train Loss: 1.0493, Val Loss: 1.1270
2024-12-28 14:42:10,919 - INFO - Epoch 18/1000, Train Loss: 1.0450, Val Loss: 1.1174
2024-12-28 14:42:11,369 - INFO - Epoch 19/1000, Train Loss: 1.0378, Val Loss: 1.1091
2024-12-28 14:42:11,807 - INFO - Epoch 20/1000, Train Loss: 1.0333, Val Loss: 1.1077
2024-12-28 14:42:12,252 - INFO - Epoch 21/1000, Train Loss: 1.0269, Val Loss: 1.1021
2024-12-28 14:42:12,693 - INFO - Epoch 22/1000, Train Loss: 1.0240, Val Loss: 1.0995
2024-12-28 14:42:13,122 - INFO - Epoch 23/1000, Train Loss: 1.0190, Val Loss: 1.0909
2024-12-28 14:42:13,589 - INFO - Epoch 24/1000, Train Loss: 1.0154, Val Loss: 1.0856
2024-12-28 14:42:14,075 - INFO - Epoch 25/1000, Train Loss: 1.0099, Val Loss: 1.0983
2024-12-28 14:42:14,510 - INFO - Epoch 26/1000, Train Loss: 1.0100, Val Loss: 1.0846
2024-12-28 14:42:14,937 - INFO - Epoch 27/1000, Train Loss: 1.0052, Val Loss: 1.0870
2024-12-28 14:42:15,379 - INFO - Epoch 28/1000, Train Loss: 1.0026, Val Loss: 1.0860
2024-12-28 14:42:15,834 - INFO - Epoch 29/1000, Train Loss: 1.0015, Val Loss: 1.0802
2024-12-28 14:42:16,282 - INFO - Epoch 30/1000, Train Loss: 1.0000, Val Loss: 1.0866
2024-12-28 14:42:16,941 - INFO - Epoch 31/1000, Train Loss: 0.9985, Val Loss: 1.0666
2024-12-28 14:42:17,379 - INFO - Epoch 32/1000, Train Loss: 0.9961, Val Loss: 1.0764
2024-12-28 14:42:17,826 - INFO - Epoch 33/1000, Train Loss: 0.9947, Val Loss: 1.0737
2024-12-28 14:42:18,259 - INFO - Epoch 34/1000, Train Loss: 0.9929, Val Loss: 1.0786
2024-12-28 14:42:18,724 - INFO - Epoch 35/1000, Train Loss: 0.9921, Val Loss: 1.0707
2024-12-28 14:42:19,182 - INFO - Epoch 36/1000, Train Loss: 0.9908, Val Loss: 1.0731
2024-12-28 14:42:19,183 - INFO - Early stopping triggered at epoch 36
2024-12-28 14:42:19,183 - INFO - Training completed in 17.17s
2024-12-28 14:42:19,183 - INFO - Final memory usage: CPU 1914.7 MB, GPU 103.6 MB
2024-12-28 14:42:19,185 - INFO - Model training completed in 17.18s
2024-12-28 14:42:19,262 - INFO - Prediction completed in 0.08s
2024-12-28 14:42:19,273 - INFO - Poison rate 0.07 completed in 17.27s
2024-12-28 14:42:19,273 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:42:19,276 - INFO - Total number of labels flipped: 1956
2024-12-28 14:42:19,276 - INFO - Label flipping completed in 0.00s
2024-12-28 14:42:19,276 - INFO - Training set processing completed in 0.00s
2024-12-28 14:42:19,276 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:42:19,277 - INFO - Memory usage at start_fit: CPU 1876.1 MB, GPU 103.5 MB
2024-12-28 14:42:19,277 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:42:19,286 - INFO - Number of unique classes: 43
2024-12-28 14:42:19,461 - INFO - Fitted scaler and transformed data
2024-12-28 14:42:19,461 - INFO - Scaling time: 0.17s
2024-12-28 14:42:19,896 - INFO - Epoch 1/1000, Train Loss: 2.2621, Val Loss: 1.9150
2024-12-28 14:42:20,331 - INFO - Epoch 2/1000, Train Loss: 1.6914, Val Loss: 1.7269
2024-12-28 14:42:20,751 - INFO - Epoch 3/1000, Train Loss: 1.5296, Val Loss: 1.6283
2024-12-28 14:42:21,171 - INFO - Epoch 4/1000, Train Loss: 1.4363, Val Loss: 1.5706
2024-12-28 14:42:21,641 - INFO - Epoch 5/1000, Train Loss: 1.3762, Val Loss: 1.5279
2024-12-28 14:42:22,080 - INFO - Epoch 6/1000, Train Loss: 1.3316, Val Loss: 1.4900
2024-12-28 14:42:22,542 - INFO - Epoch 7/1000, Train Loss: 1.2962, Val Loss: 1.4618
2024-12-28 14:42:22,991 - INFO - Epoch 8/1000, Train Loss: 1.2684, Val Loss: 1.4475
2024-12-28 14:42:23,495 - INFO - Epoch 9/1000, Train Loss: 1.2462, Val Loss: 1.4196
2024-12-28 14:42:23,958 - INFO - Epoch 10/1000, Train Loss: 1.2251, Val Loss: 1.3939
2024-12-28 14:42:24,449 - INFO - Epoch 11/1000, Train Loss: 1.2073, Val Loss: 1.3838
2024-12-28 14:42:24,916 - INFO - Epoch 12/1000, Train Loss: 1.1924, Val Loss: 1.3756
2024-12-28 14:42:25,383 - INFO - Epoch 13/1000, Train Loss: 1.1804, Val Loss: 1.3579
2024-12-28 14:42:25,850 - INFO - Epoch 14/1000, Train Loss: 1.1685, Val Loss: 1.3436
2024-12-28 14:42:26,300 - INFO - Epoch 15/1000, Train Loss: 1.1595, Val Loss: 1.3240
2024-12-28 14:42:26,762 - INFO - Epoch 16/1000, Train Loss: 1.1471, Val Loss: 1.3206
2024-12-28 14:42:27,191 - INFO - Epoch 17/1000, Train Loss: 1.1380, Val Loss: 1.3040
2024-12-28 14:42:27,636 - INFO - Epoch 18/1000, Train Loss: 1.1296, Val Loss: 1.2942
2024-12-28 14:42:28,133 - INFO - Epoch 19/1000, Train Loss: 1.1217, Val Loss: 1.2912
2024-12-28 14:42:28,572 - INFO - Epoch 20/1000, Train Loss: 1.1153, Val Loss: 1.2745
2024-12-28 14:42:29,027 - INFO - Epoch 21/1000, Train Loss: 1.1089, Val Loss: 1.2671
2024-12-28 14:42:29,492 - INFO - Epoch 22/1000, Train Loss: 1.1023, Val Loss: 1.2538
2024-12-28 14:42:29,939 - INFO - Epoch 23/1000, Train Loss: 1.0973, Val Loss: 1.2448
2024-12-28 14:42:30,417 - INFO - Epoch 24/1000, Train Loss: 1.0896, Val Loss: 1.2462
2024-12-28 14:42:30,886 - INFO - Epoch 25/1000, Train Loss: 1.0885, Val Loss: 1.2295
2024-12-28 14:42:31,363 - INFO - Epoch 26/1000, Train Loss: 1.0832, Val Loss: 1.2292
2024-12-28 14:42:31,800 - INFO - Epoch 27/1000, Train Loss: 1.0770, Val Loss: 1.2232
2024-12-28 14:42:32,255 - INFO - Epoch 28/1000, Train Loss: 1.0738, Val Loss: 1.2178
2024-12-28 14:42:32,712 - INFO - Epoch 29/1000, Train Loss: 1.0704, Val Loss: 1.2163
2024-12-28 14:42:33,166 - INFO - Epoch 30/1000, Train Loss: 1.0660, Val Loss: 1.2019
2024-12-28 14:42:33,635 - INFO - Epoch 31/1000, Train Loss: 1.0621, Val Loss: 1.2068
2024-12-28 14:42:34,119 - INFO - Epoch 32/1000, Train Loss: 1.0615, Val Loss: 1.2090
2024-12-28 14:42:34,575 - INFO - Epoch 33/1000, Train Loss: 1.0567, Val Loss: 1.1882
2024-12-28 14:42:35,112 - INFO - Epoch 34/1000, Train Loss: 1.0558, Val Loss: 1.1901
2024-12-28 14:42:35,613 - INFO - Epoch 35/1000, Train Loss: 1.0552, Val Loss: 1.1859
2024-12-28 14:42:36,143 - INFO - Epoch 36/1000, Train Loss: 1.0519, Val Loss: 1.1861
2024-12-28 14:42:36,631 - INFO - Epoch 37/1000, Train Loss: 1.0500, Val Loss: 1.1913
2024-12-28 14:42:37,074 - INFO - Epoch 38/1000, Train Loss: 1.0470, Val Loss: 1.1823
2024-12-28 14:42:37,550 - INFO - Epoch 39/1000, Train Loss: 1.0497, Val Loss: 1.1798
2024-12-28 14:42:37,998 - INFO - Epoch 40/1000, Train Loss: 1.0479, Val Loss: 1.1790
2024-12-28 14:42:38,433 - INFO - Epoch 41/1000, Train Loss: 1.0468, Val Loss: 1.1788
2024-12-28 14:42:38,873 - INFO - Epoch 42/1000, Train Loss: 1.0450, Val Loss: 1.1674
2024-12-28 14:42:39,307 - INFO - Epoch 43/1000, Train Loss: 1.0451, Val Loss: 1.1783
2024-12-28 14:42:39,735 - INFO - Epoch 44/1000, Train Loss: 1.0442, Val Loss: 1.1739
2024-12-28 14:42:40,174 - INFO - Epoch 45/1000, Train Loss: 1.0416, Val Loss: 1.1787
2024-12-28 14:42:40,604 - INFO - Epoch 46/1000, Train Loss: 1.0438, Val Loss: 1.1758
2024-12-28 14:42:41,107 - INFO - Epoch 47/1000, Train Loss: 1.0410, Val Loss: 1.1751
2024-12-28 14:42:41,108 - INFO - Early stopping triggered at epoch 47
2024-12-28 14:42:41,108 - INFO - Training completed in 21.83s
2024-12-28 14:42:41,109 - INFO - Final memory usage: CPU 1914.7 MB, GPU 103.6 MB
2024-12-28 14:42:41,110 - INFO - Model training completed in 21.83s
2024-12-28 14:42:41,187 - INFO - Prediction completed in 0.08s
2024-12-28 14:42:41,198 - INFO - Poison rate 0.1 completed in 21.92s
2024-12-28 14:42:41,198 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:42:41,201 - INFO - Total number of labels flipped: 3933
2024-12-28 14:42:41,201 - INFO - Label flipping completed in 0.00s
2024-12-28 14:42:41,202 - INFO - Training set processing completed in 0.00s
2024-12-28 14:42:41,202 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:42:41,202 - INFO - Memory usage at start_fit: CPU 1876.1 MB, GPU 103.5 MB
2024-12-28 14:42:41,202 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:42:41,209 - INFO - Number of unique classes: 43
2024-12-28 14:42:41,349 - INFO - Fitted scaler and transformed data
2024-12-28 14:42:41,349 - INFO - Scaling time: 0.14s
2024-12-28 14:42:41,815 - INFO - Epoch 1/1000, Train Loss: 2.5869, Val Loss: 2.2126
2024-12-28 14:42:42,288 - INFO - Epoch 2/1000, Train Loss: 2.1060, Val Loss: 2.0414
2024-12-28 14:42:42,746 - INFO - Epoch 3/1000, Train Loss: 1.9516, Val Loss: 1.9446
2024-12-28 14:42:43,182 - INFO - Epoch 4/1000, Train Loss: 1.8537, Val Loss: 1.8808
2024-12-28 14:42:43,643 - INFO - Epoch 5/1000, Train Loss: 1.7773, Val Loss: 1.8236
2024-12-28 14:42:44,138 - INFO - Epoch 6/1000, Train Loss: 1.7218, Val Loss: 1.7797
2024-12-28 14:42:44,622 - INFO - Epoch 7/1000, Train Loss: 1.6734, Val Loss: 1.7518
2024-12-28 14:42:45,078 - INFO - Epoch 8/1000, Train Loss: 1.6289, Val Loss: 1.7194
2024-12-28 14:42:45,511 - INFO - Epoch 9/1000, Train Loss: 1.5935, Val Loss: 1.6833
2024-12-28 14:42:45,958 - INFO - Epoch 10/1000, Train Loss: 1.5617, Val Loss: 1.6548
2024-12-28 14:42:46,405 - INFO - Epoch 11/1000, Train Loss: 1.5307, Val Loss: 1.6360
2024-12-28 14:42:46,831 - INFO - Epoch 12/1000, Train Loss: 1.5040, Val Loss: 1.6055
2024-12-28 14:42:47,250 - INFO - Epoch 13/1000, Train Loss: 1.4797, Val Loss: 1.5849
2024-12-28 14:42:47,708 - INFO - Epoch 14/1000, Train Loss: 1.4590, Val Loss: 1.5540
2024-12-28 14:42:48,181 - INFO - Epoch 15/1000, Train Loss: 1.4345, Val Loss: 1.5443
2024-12-28 14:42:48,656 - INFO - Epoch 16/1000, Train Loss: 1.4168, Val Loss: 1.5275
2024-12-28 14:42:49,097 - INFO - Epoch 17/1000, Train Loss: 1.4004, Val Loss: 1.5115
2024-12-28 14:42:49,512 - INFO - Epoch 18/1000, Train Loss: 1.3832, Val Loss: 1.4919
2024-12-28 14:42:49,990 - INFO - Epoch 19/1000, Train Loss: 1.3674, Val Loss: 1.4762
2024-12-28 14:42:50,419 - INFO - Epoch 20/1000, Train Loss: 1.3531, Val Loss: 1.4594
2024-12-28 14:42:50,900 - INFO - Epoch 21/1000, Train Loss: 1.3376, Val Loss: 1.4531
2024-12-28 14:42:51,318 - INFO - Epoch 22/1000, Train Loss: 1.3263, Val Loss: 1.4362
2024-12-28 14:42:51,730 - INFO - Epoch 23/1000, Train Loss: 1.3147, Val Loss: 1.4249
2024-12-28 14:42:52,183 - INFO - Epoch 24/1000, Train Loss: 1.3034, Val Loss: 1.4104
2024-12-28 14:42:52,654 - INFO - Epoch 25/1000, Train Loss: 1.2923, Val Loss: 1.4037
2024-12-28 14:42:53,105 - INFO - Epoch 26/1000, Train Loss: 1.2810, Val Loss: 1.3996
2024-12-28 14:42:53,538 - INFO - Epoch 27/1000, Train Loss: 1.2739, Val Loss: 1.3860
2024-12-28 14:42:53,976 - INFO - Epoch 28/1000, Train Loss: 1.2658, Val Loss: 1.3832
2024-12-28 14:42:54,447 - INFO - Epoch 29/1000, Train Loss: 1.2582, Val Loss: 1.3631
2024-12-28 14:42:54,923 - INFO - Epoch 30/1000, Train Loss: 1.2505, Val Loss: 1.3648
2024-12-28 14:42:55,337 - INFO - Epoch 31/1000, Train Loss: 1.2428, Val Loss: 1.3502
2024-12-28 14:42:55,793 - INFO - Epoch 32/1000, Train Loss: 1.2380, Val Loss: 1.3542
2024-12-28 14:42:56,240 - INFO - Epoch 33/1000, Train Loss: 1.2325, Val Loss: 1.3407
2024-12-28 14:42:56,675 - INFO - Epoch 34/1000, Train Loss: 1.2254, Val Loss: 1.3420
2024-12-28 14:42:57,128 - INFO - Epoch 35/1000, Train Loss: 1.2228, Val Loss: 1.3317
2024-12-28 14:42:57,545 - INFO - Epoch 36/1000, Train Loss: 1.2166, Val Loss: 1.3307
2024-12-28 14:42:58,160 - INFO - Epoch 37/1000, Train Loss: 1.2138, Val Loss: 1.3404
2024-12-28 14:42:58,719 - INFO - Epoch 38/1000, Train Loss: 1.2108, Val Loss: 1.3154
2024-12-28 14:42:59,306 - INFO - Epoch 39/1000, Train Loss: 1.2080, Val Loss: 1.3203
2024-12-28 14:42:59,974 - INFO - Epoch 40/1000, Train Loss: 1.2056, Val Loss: 1.3095
2024-12-28 14:43:00,606 - INFO - Epoch 41/1000, Train Loss: 1.1998, Val Loss: 1.3073
2024-12-28 14:43:01,233 - INFO - Epoch 42/1000, Train Loss: 1.1971, Val Loss: 1.3142
2024-12-28 14:43:01,835 - INFO - Epoch 43/1000, Train Loss: 1.1963, Val Loss: 1.3105
2024-12-28 14:43:02,397 - INFO - Epoch 44/1000, Train Loss: 1.1951, Val Loss: 1.3072
2024-12-28 14:43:03,027 - INFO - Epoch 45/1000, Train Loss: 1.1911, Val Loss: 1.3061
2024-12-28 14:43:03,704 - INFO - Epoch 46/1000, Train Loss: 1.1908, Val Loss: 1.2992
2024-12-28 14:43:04,542 - INFO - Epoch 47/1000, Train Loss: 1.1898, Val Loss: 1.3035
2024-12-28 14:43:05,404 - INFO - Epoch 48/1000, Train Loss: 1.1862, Val Loss: 1.3101
2024-12-28 14:43:06,231 - INFO - Epoch 49/1000, Train Loss: 1.1869, Val Loss: 1.3006
2024-12-28 14:43:06,929 - INFO - Epoch 50/1000, Train Loss: 1.1851, Val Loss: 1.3075
2024-12-28 14:43:07,639 - INFO - Epoch 51/1000, Train Loss: 1.1853, Val Loss: 1.2900
2024-12-28 14:43:08,215 - INFO - Epoch 52/1000, Train Loss: 1.1850, Val Loss: 1.2956
2024-12-28 14:43:08,808 - INFO - Epoch 53/1000, Train Loss: 1.1836, Val Loss: 1.2950
2024-12-28 14:43:09,431 - INFO - Epoch 54/1000, Train Loss: 1.1824, Val Loss: 1.2932
2024-12-28 14:43:09,991 - INFO - Epoch 55/1000, Train Loss: 1.1827, Val Loss: 1.3020
2024-12-28 14:43:10,628 - INFO - Epoch 56/1000, Train Loss: 1.1832, Val Loss: 1.3001
2024-12-28 14:43:10,628 - INFO - Early stopping triggered at epoch 56
2024-12-28 14:43:10,629 - INFO - Training completed in 29.43s
2024-12-28 14:43:10,629 - INFO - Final memory usage: CPU 1914.7 MB, GPU 103.6 MB
2024-12-28 14:43:10,630 - INFO - Model training completed in 29.43s
2024-12-28 14:43:10,695 - INFO - Prediction completed in 0.06s
2024-12-28 14:43:10,706 - INFO - Poison rate 0.2 completed in 29.51s
2024-12-28 14:43:10,709 - INFO - Loaded 70 existing results
2024-12-28 14:43:10,709 - INFO - Total results to save: 77
2024-12-28 14:43:10,710 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:43:10,715 - INFO - Saved 77 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:43:10,715 - INFO - Total evaluation time: 164.88s
2024-12-28 14:43:10,720 - INFO - 
Progress: 12.5% - Evaluating GTSRB with LogisticRegression (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:43:10,900 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:43:10,900 - INFO - Dataset type: image
2024-12-28 14:43:10,900 - INFO - Sample size: 39209
2024-12-28 14:43:10,900 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:43:10,901 - INFO - Loading datasets...
2024-12-28 14:43:30,193 - INFO - Dataset loading completed in 19.29s
2024-12-28 14:43:30,193 - INFO - Extracting validation features...
2024-12-28 14:43:30,194 - INFO - Extracting features from 4435 samples...
2024-12-28 14:43:30,946 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:43:30,948 - INFO - Validation feature extraction completed in 0.75s
2024-12-28 14:43:30,949 - INFO - Extracting training features...
2024-12-28 14:43:30,949 - INFO - Extracting features from 19755 samples...
2024-12-28 14:43:33,668 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:43:33,675 - INFO - Training feature extraction completed in 2.73s
2024-12-28 14:43:33,675 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 14:43:33,675 - INFO - Using device: cuda
2024-12-28 14:43:33,675 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:43:33,675 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:43:33,676 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:43:34,853 - INFO - Feature scaling completed in 1.18s
2024-12-28 14:43:34,853 - INFO - Starting feature selection (k=50)
2024-12-28 14:43:34,882 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:43:34,883 - INFO - Starting anomaly detection
2024-12-28 14:43:42,981 - INFO - Anomaly detection completed in 8.10s
2024-12-28 14:43:42,981 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:43:42,981 - INFO - Total fit_transform time: 9.31s
2024-12-28 14:43:42,981 - INFO - Training set processing completed in 9.31s
2024-12-28 14:43:42,982 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:43:42,983 - INFO - Memory usage at start_fit: CPU 1885.8 MB, GPU 104.6 MB
2024-12-28 14:43:42,983 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:43:42,990 - INFO - Number of unique classes: 43
2024-12-28 14:43:43,129 - INFO - Fitted scaler and transformed data
2024-12-28 14:43:43,129 - INFO - Scaling time: 0.14s
2024-12-28 14:43:43,709 - INFO - Epoch 1/1000, Train Loss: 1.9251, Val Loss: 1.4283
2024-12-28 14:43:44,422 - INFO - Epoch 2/1000, Train Loss: 1.2044, Val Loss: 1.1744
2024-12-28 14:43:45,159 - INFO - Epoch 3/1000, Train Loss: 1.0151, Val Loss: 1.0487
2024-12-28 14:43:45,809 - INFO - Epoch 4/1000, Train Loss: 0.9182, Val Loss: 0.9998
2024-12-28 14:43:46,369 - INFO - Epoch 5/1000, Train Loss: 0.8603, Val Loss: 0.9444
2024-12-28 14:43:47,111 - INFO - Epoch 6/1000, Train Loss: 0.8229, Val Loss: 0.9151
2024-12-28 14:43:47,991 - INFO - Epoch 7/1000, Train Loss: 0.7955, Val Loss: 0.8904
2024-12-28 14:43:48,630 - INFO - Epoch 8/1000, Train Loss: 0.7760, Val Loss: 0.8861
2024-12-28 14:43:49,094 - INFO - Epoch 9/1000, Train Loss: 0.7626, Val Loss: 0.8766
2024-12-28 14:43:49,548 - INFO - Epoch 10/1000, Train Loss: 0.7499, Val Loss: 0.8680
2024-12-28 14:43:50,023 - INFO - Epoch 11/1000, Train Loss: 0.7432, Val Loss: 0.8600
2024-12-28 14:43:50,486 - INFO - Epoch 12/1000, Train Loss: 0.7374, Val Loss: 0.8454
2024-12-28 14:43:50,927 - INFO - Epoch 13/1000, Train Loss: 0.7328, Val Loss: 0.8477
2024-12-28 14:43:51,390 - INFO - Epoch 14/1000, Train Loss: 0.7274, Val Loss: 0.8473
2024-12-28 14:43:51,822 - INFO - Epoch 15/1000, Train Loss: 0.7252, Val Loss: 0.8421
2024-12-28 14:43:52,256 - INFO - Epoch 16/1000, Train Loss: 0.7223, Val Loss: 0.8443
2024-12-28 14:43:52,739 - INFO - Epoch 17/1000, Train Loss: 0.7214, Val Loss: 0.8395
2024-12-28 14:43:53,214 - INFO - Epoch 18/1000, Train Loss: 0.7199, Val Loss: 0.8330
2024-12-28 14:43:53,683 - INFO - Epoch 19/1000, Train Loss: 0.7170, Val Loss: 0.8319
2024-12-28 14:43:54,138 - INFO - Epoch 20/1000, Train Loss: 0.7149, Val Loss: 0.8416
2024-12-28 14:43:54,605 - INFO - Epoch 21/1000, Train Loss: 0.7163, Val Loss: 0.8370
2024-12-28 14:43:55,048 - INFO - Epoch 22/1000, Train Loss: 0.7140, Val Loss: 0.8387
2024-12-28 14:43:55,507 - INFO - Epoch 23/1000, Train Loss: 0.7118, Val Loss: 0.8382
2024-12-28 14:43:55,969 - INFO - Epoch 24/1000, Train Loss: 0.7136, Val Loss: 0.8282
2024-12-28 14:43:56,424 - INFO - Epoch 25/1000, Train Loss: 0.7145, Val Loss: 0.8308
2024-12-28 14:43:56,861 - INFO - Epoch 26/1000, Train Loss: 0.7111, Val Loss: 0.8340
2024-12-28 14:43:57,330 - INFO - Epoch 27/1000, Train Loss: 0.7130, Val Loss: 0.8306
2024-12-28 14:43:57,768 - INFO - Epoch 28/1000, Train Loss: 0.7135, Val Loss: 0.8373
2024-12-28 14:43:58,229 - INFO - Epoch 29/1000, Train Loss: 0.7135, Val Loss: 0.8319
2024-12-28 14:43:58,229 - INFO - Early stopping triggered at epoch 29
2024-12-28 14:43:58,229 - INFO - Training completed in 15.25s
2024-12-28 14:43:58,230 - INFO - Final memory usage: CPU 1924.4 MB, GPU 105.1 MB
2024-12-28 14:43:58,230 - INFO - Model training completed in 15.25s
2024-12-28 14:43:58,318 - INFO - Prediction completed in 0.09s
2024-12-28 14:43:58,329 - INFO - Poison rate 0.0 completed in 24.65s
2024-12-28 14:43:58,329 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:43:58,331 - INFO - Total number of labels flipped: 195
2024-12-28 14:43:58,331 - INFO - Label flipping completed in 0.00s
2024-12-28 14:43:58,331 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:43:58,331 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:43:59,650 - INFO - Feature scaling completed in 1.32s
2024-12-28 14:43:59,650 - INFO - Starting feature selection (k=50)
2024-12-28 14:43:59,679 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:43:59,680 - INFO - Starting anomaly detection
2024-12-28 14:44:06,965 - INFO - Anomaly detection completed in 7.28s
2024-12-28 14:44:06,965 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:44:06,965 - INFO - Total fit_transform time: 8.63s
2024-12-28 14:44:06,965 - INFO - Training set processing completed in 8.63s
2024-12-28 14:44:06,965 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:44:06,966 - INFO - Memory usage at start_fit: CPU 1885.8 MB, GPU 105.0 MB
2024-12-28 14:44:06,966 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:44:06,972 - INFO - Number of unique classes: 43
2024-12-28 14:44:07,110 - INFO - Fitted scaler and transformed data
2024-12-28 14:44:07,111 - INFO - Scaling time: 0.14s
2024-12-28 14:44:07,596 - INFO - Epoch 1/1000, Train Loss: 1.9534, Val Loss: 1.4341
2024-12-28 14:44:07,993 - INFO - Epoch 2/1000, Train Loss: 1.2657, Val Loss: 1.1787
2024-12-28 14:44:08,417 - INFO - Epoch 3/1000, Train Loss: 1.0825, Val Loss: 1.0667
2024-12-28 14:44:08,843 - INFO - Epoch 4/1000, Train Loss: 0.9881, Val Loss: 0.9967
2024-12-28 14:44:09,332 - INFO - Epoch 5/1000, Train Loss: 0.9298, Val Loss: 0.9573
2024-12-28 14:44:09,767 - INFO - Epoch 6/1000, Train Loss: 0.8944, Val Loss: 0.9445
2024-12-28 14:44:10,199 - INFO - Epoch 7/1000, Train Loss: 0.8670, Val Loss: 0.9111
2024-12-28 14:44:10,635 - INFO - Epoch 8/1000, Train Loss: 0.8470, Val Loss: 0.9003
2024-12-28 14:44:11,100 - INFO - Epoch 9/1000, Train Loss: 0.8338, Val Loss: 0.8850
2024-12-28 14:44:11,528 - INFO - Epoch 10/1000, Train Loss: 0.8226, Val Loss: 0.8854
2024-12-28 14:44:11,989 - INFO - Epoch 11/1000, Train Loss: 0.8134, Val Loss: 0.8714
2024-12-28 14:44:12,416 - INFO - Epoch 12/1000, Train Loss: 0.8089, Val Loss: 0.8663
2024-12-28 14:44:12,876 - INFO - Epoch 13/1000, Train Loss: 0.8028, Val Loss: 0.8662
2024-12-28 14:44:13,293 - INFO - Epoch 14/1000, Train Loss: 0.7998, Val Loss: 0.8548
2024-12-28 14:44:13,711 - INFO - Epoch 15/1000, Train Loss: 0.7953, Val Loss: 0.8567
2024-12-28 14:44:14,141 - INFO - Epoch 16/1000, Train Loss: 0.7942, Val Loss: 0.8583
2024-12-28 14:44:14,597 - INFO - Epoch 17/1000, Train Loss: 0.7893, Val Loss: 0.8566
2024-12-28 14:44:15,058 - INFO - Epoch 18/1000, Train Loss: 0.7892, Val Loss: 0.8502
2024-12-28 14:44:15,535 - INFO - Epoch 19/1000, Train Loss: 0.7851, Val Loss: 0.8544
2024-12-28 14:44:15,987 - INFO - Epoch 20/1000, Train Loss: 0.7874, Val Loss: 0.8525
2024-12-28 14:44:16,436 - INFO - Epoch 21/1000, Train Loss: 0.7845, Val Loss: 0.8447
2024-12-28 14:44:16,873 - INFO - Epoch 22/1000, Train Loss: 0.7838, Val Loss: 0.8502
2024-12-28 14:44:17,344 - INFO - Epoch 23/1000, Train Loss: 0.7829, Val Loss: 0.8486
2024-12-28 14:44:17,794 - INFO - Epoch 24/1000, Train Loss: 0.7842, Val Loss: 0.8533
2024-12-28 14:44:18,223 - INFO - Epoch 25/1000, Train Loss: 0.7827, Val Loss: 0.8411
2024-12-28 14:44:18,656 - INFO - Epoch 26/1000, Train Loss: 0.7815, Val Loss: 0.8438
2024-12-28 14:44:19,122 - INFO - Epoch 27/1000, Train Loss: 0.7809, Val Loss: 0.8434
2024-12-28 14:44:19,583 - INFO - Epoch 28/1000, Train Loss: 0.7828, Val Loss: 0.8467
2024-12-28 14:44:20,025 - INFO - Epoch 29/1000, Train Loss: 0.7813, Val Loss: 0.8367
2024-12-28 14:44:20,501 - INFO - Epoch 30/1000, Train Loss: 0.7822, Val Loss: 0.8451
2024-12-28 14:44:20,916 - INFO - Epoch 31/1000, Train Loss: 0.7812, Val Loss: 0.8405
2024-12-28 14:44:21,427 - INFO - Epoch 32/1000, Train Loss: 0.7809, Val Loss: 0.8509
2024-12-28 14:44:21,912 - INFO - Epoch 33/1000, Train Loss: 0.7799, Val Loss: 0.8441
2024-12-28 14:44:22,383 - INFO - Epoch 34/1000, Train Loss: 0.7803, Val Loss: 0.8418
2024-12-28 14:44:22,383 - INFO - Early stopping triggered at epoch 34
2024-12-28 14:44:22,383 - INFO - Training completed in 15.42s
2024-12-28 14:44:22,383 - INFO - Final memory usage: CPU 1924.4 MB, GPU 105.1 MB
2024-12-28 14:44:22,384 - INFO - Model training completed in 15.42s
2024-12-28 14:44:22,450 - INFO - Prediction completed in 0.07s
2024-12-28 14:44:22,461 - INFO - Poison rate 0.01 completed in 24.13s
2024-12-28 14:44:22,461 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:44:22,463 - INFO - Total number of labels flipped: 591
2024-12-28 14:44:22,463 - INFO - Label flipping completed in 0.00s
2024-12-28 14:44:22,463 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:44:22,463 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:44:23,739 - INFO - Feature scaling completed in 1.28s
2024-12-28 14:44:23,739 - INFO - Starting feature selection (k=50)
2024-12-28 14:44:23,766 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:44:23,767 - INFO - Starting anomaly detection
2024-12-28 14:44:32,115 - INFO - Anomaly detection completed in 8.35s
2024-12-28 14:44:32,115 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:44:32,115 - INFO - Total fit_transform time: 9.65s
2024-12-28 14:44:32,115 - INFO - Training set processing completed in 9.65s
2024-12-28 14:44:32,115 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:44:32,116 - INFO - Memory usage at start_fit: CPU 1885.8 MB, GPU 105.0 MB
2024-12-28 14:44:32,116 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:44:32,123 - INFO - Number of unique classes: 43
2024-12-28 14:44:32,273 - INFO - Fitted scaler and transformed data
2024-12-28 14:44:32,274 - INFO - Scaling time: 0.15s
2024-12-28 14:44:32,691 - INFO - Epoch 1/1000, Train Loss: 2.0168, Val Loss: 1.5967
2024-12-28 14:44:33,141 - INFO - Epoch 2/1000, Train Loss: 1.3720, Val Loss: 1.3609
2024-12-28 14:44:33,589 - INFO - Epoch 3/1000, Train Loss: 1.2005, Val Loss: 1.2533
2024-12-28 14:44:34,053 - INFO - Epoch 4/1000, Train Loss: 1.1078, Val Loss: 1.1822
2024-12-28 14:44:34,542 - INFO - Epoch 5/1000, Train Loss: 1.0498, Val Loss: 1.1572
2024-12-28 14:44:35,004 - INFO - Epoch 6/1000, Train Loss: 1.0097, Val Loss: 1.1151
2024-12-28 14:44:35,447 - INFO - Epoch 7/1000, Train Loss: 0.9837, Val Loss: 1.1010
2024-12-28 14:44:35,911 - INFO - Epoch 8/1000, Train Loss: 0.9632, Val Loss: 1.0794
2024-12-28 14:44:36,357 - INFO - Epoch 9/1000, Train Loss: 0.9446, Val Loss: 1.0678
2024-12-28 14:44:36,783 - INFO - Epoch 10/1000, Train Loss: 0.9319, Val Loss: 1.0578
2024-12-28 14:44:37,257 - INFO - Epoch 11/1000, Train Loss: 0.9212, Val Loss: 1.0524
2024-12-28 14:44:37,697 - INFO - Epoch 12/1000, Train Loss: 0.9136, Val Loss: 1.0407
2024-12-28 14:44:38,122 - INFO - Epoch 13/1000, Train Loss: 0.9053, Val Loss: 1.0356
2024-12-28 14:44:38,560 - INFO - Epoch 14/1000, Train Loss: 0.9012, Val Loss: 1.0240
2024-12-28 14:44:39,005 - INFO - Epoch 15/1000, Train Loss: 0.8954, Val Loss: 1.0296
2024-12-28 14:44:39,439 - INFO - Epoch 16/1000, Train Loss: 0.8940, Val Loss: 1.0224
2024-12-28 14:44:39,893 - INFO - Epoch 17/1000, Train Loss: 0.8898, Val Loss: 1.0213
2024-12-28 14:44:40,347 - INFO - Epoch 18/1000, Train Loss: 0.8878, Val Loss: 1.0307
2024-12-28 14:44:40,790 - INFO - Epoch 19/1000, Train Loss: 0.8848, Val Loss: 1.0093
2024-12-28 14:44:41,227 - INFO - Epoch 20/1000, Train Loss: 0.8827, Val Loss: 1.0165
2024-12-28 14:44:41,705 - INFO - Epoch 21/1000, Train Loss: 0.8792, Val Loss: 1.0119
2024-12-28 14:44:42,171 - INFO - Epoch 22/1000, Train Loss: 0.8797, Val Loss: 1.0205
2024-12-28 14:44:42,593 - INFO - Epoch 23/1000, Train Loss: 0.8761, Val Loss: 1.0132
2024-12-28 14:44:43,040 - INFO - Epoch 24/1000, Train Loss: 0.8739, Val Loss: 1.0063
2024-12-28 14:44:43,495 - INFO - Epoch 25/1000, Train Loss: 0.8762, Val Loss: 1.0000
2024-12-28 14:44:43,939 - INFO - Epoch 26/1000, Train Loss: 0.8733, Val Loss: 1.0105
2024-12-28 14:44:44,380 - INFO - Epoch 27/1000, Train Loss: 0.8734, Val Loss: 0.9994
2024-12-28 14:44:44,885 - INFO - Epoch 28/1000, Train Loss: 0.8719, Val Loss: 1.0034
2024-12-28 14:44:45,328 - INFO - Epoch 29/1000, Train Loss: 0.8725, Val Loss: 0.9983
2024-12-28 14:44:45,769 - INFO - Epoch 30/1000, Train Loss: 0.8720, Val Loss: 1.0105
2024-12-28 14:44:46,235 - INFO - Epoch 31/1000, Train Loss: 0.8716, Val Loss: 1.0130
2024-12-28 14:44:46,674 - INFO - Epoch 32/1000, Train Loss: 0.8711, Val Loss: 1.0059
2024-12-28 14:44:47,109 - INFO - Epoch 33/1000, Train Loss: 0.8714, Val Loss: 0.9926
2024-12-28 14:44:47,556 - INFO - Epoch 34/1000, Train Loss: 0.8698, Val Loss: 1.0032
2024-12-28 14:44:48,037 - INFO - Epoch 35/1000, Train Loss: 0.8706, Val Loss: 1.0128
2024-12-28 14:44:48,493 - INFO - Epoch 36/1000, Train Loss: 0.8685, Val Loss: 0.9959
2024-12-28 14:44:48,925 - INFO - Epoch 37/1000, Train Loss: 0.8694, Val Loss: 1.0000
2024-12-28 14:44:49,386 - INFO - Epoch 38/1000, Train Loss: 0.8691, Val Loss: 0.9977
2024-12-28 14:44:49,386 - INFO - Early stopping triggered at epoch 38
2024-12-28 14:44:49,386 - INFO - Training completed in 17.27s
2024-12-28 14:44:49,387 - INFO - Final memory usage: CPU 1924.4 MB, GPU 105.1 MB
2024-12-28 14:44:49,388 - INFO - Model training completed in 17.27s
2024-12-28 14:44:49,452 - INFO - Prediction completed in 0.06s
2024-12-28 14:44:49,463 - INFO - Poison rate 0.03 completed in 27.00s
2024-12-28 14:44:49,463 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:44:49,465 - INFO - Total number of labels flipped: 983
2024-12-28 14:44:49,466 - INFO - Label flipping completed in 0.00s
2024-12-28 14:44:49,466 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:44:49,466 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:44:50,694 - INFO - Feature scaling completed in 1.23s
2024-12-28 14:44:50,695 - INFO - Starting feature selection (k=50)
2024-12-28 14:44:50,726 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:44:50,726 - INFO - Starting anomaly detection
2024-12-28 14:44:58,749 - INFO - Anomaly detection completed in 8.02s
2024-12-28 14:44:58,750 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:44:58,750 - INFO - Total fit_transform time: 9.28s
2024-12-28 14:44:58,750 - INFO - Training set processing completed in 9.28s
2024-12-28 14:44:58,750 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:44:58,751 - INFO - Memory usage at start_fit: CPU 1885.8 MB, GPU 105.0 MB
2024-12-28 14:44:58,751 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:44:58,758 - INFO - Number of unique classes: 43
2024-12-28 14:44:58,913 - INFO - Fitted scaler and transformed data
2024-12-28 14:44:58,913 - INFO - Scaling time: 0.15s
2024-12-28 14:44:59,338 - INFO - Epoch 1/1000, Train Loss: 2.1057, Val Loss: 1.6610
2024-12-28 14:44:59,758 - INFO - Epoch 2/1000, Train Loss: 1.4733, Val Loss: 1.4213
2024-12-28 14:45:00,184 - INFO - Epoch 3/1000, Train Loss: 1.3081, Val Loss: 1.3325
2024-12-28 14:45:00,626 - INFO - Epoch 4/1000, Train Loss: 1.2129, Val Loss: 1.2681
2024-12-28 14:45:01,068 - INFO - Epoch 5/1000, Train Loss: 1.1571, Val Loss: 1.2199
2024-12-28 14:45:01,547 - INFO - Epoch 6/1000, Train Loss: 1.1160, Val Loss: 1.1906
2024-12-28 14:45:01,969 - INFO - Epoch 7/1000, Train Loss: 1.0860, Val Loss: 1.1731
2024-12-28 14:45:02,419 - INFO - Epoch 8/1000, Train Loss: 1.0612, Val Loss: 1.1550
2024-12-28 14:45:02,841 - INFO - Epoch 9/1000, Train Loss: 1.0431, Val Loss: 1.1347
2024-12-28 14:45:03,271 - INFO - Epoch 10/1000, Train Loss: 1.0281, Val Loss: 1.1222
2024-12-28 14:45:03,747 - INFO - Epoch 11/1000, Train Loss: 1.0159, Val Loss: 1.1299
2024-12-28 14:45:04,210 - INFO - Epoch 12/1000, Train Loss: 1.0065, Val Loss: 1.1149
2024-12-28 14:45:04,651 - INFO - Epoch 13/1000, Train Loss: 0.9985, Val Loss: 1.1018
2024-12-28 14:45:05,085 - INFO - Epoch 14/1000, Train Loss: 0.9897, Val Loss: 1.1060
2024-12-28 14:45:05,545 - INFO - Epoch 15/1000, Train Loss: 0.9835, Val Loss: 1.0915
2024-12-28 14:45:05,964 - INFO - Epoch 16/1000, Train Loss: 0.9771, Val Loss: 1.0905
2024-12-28 14:45:06,405 - INFO - Epoch 17/1000, Train Loss: 0.9708, Val Loss: 1.0793
2024-12-28 14:45:06,828 - INFO - Epoch 18/1000, Train Loss: 0.9676, Val Loss: 1.0758
2024-12-28 14:45:07,252 - INFO - Epoch 19/1000, Train Loss: 0.9637, Val Loss: 1.0799
2024-12-28 14:45:07,666 - INFO - Epoch 20/1000, Train Loss: 0.9629, Val Loss: 1.0677
2024-12-28 14:45:08,106 - INFO - Epoch 21/1000, Train Loss: 0.9563, Val Loss: 1.0649
2024-12-28 14:45:08,539 - INFO - Epoch 22/1000, Train Loss: 0.9517, Val Loss: 1.0625
2024-12-28 14:45:08,979 - INFO - Epoch 23/1000, Train Loss: 0.9524, Val Loss: 1.0709
2024-12-28 14:45:09,416 - INFO - Epoch 24/1000, Train Loss: 0.9486, Val Loss: 1.0636
2024-12-28 14:45:09,901 - INFO - Epoch 25/1000, Train Loss: 0.9463, Val Loss: 1.0599
2024-12-28 14:45:10,335 - INFO - Epoch 26/1000, Train Loss: 0.9432, Val Loss: 1.0534
2024-12-28 14:45:10,817 - INFO - Epoch 27/1000, Train Loss: 0.9428, Val Loss: 1.0530
2024-12-28 14:45:11,267 - INFO - Epoch 28/1000, Train Loss: 0.9426, Val Loss: 1.0513
2024-12-28 14:45:11,717 - INFO - Epoch 29/1000, Train Loss: 0.9399, Val Loss: 1.0525
2024-12-28 14:45:12,181 - INFO - Epoch 30/1000, Train Loss: 0.9391, Val Loss: 1.0542
2024-12-28 14:45:12,637 - INFO - Epoch 31/1000, Train Loss: 0.9384, Val Loss: 1.0529
2024-12-28 14:45:13,088 - INFO - Epoch 32/1000, Train Loss: 0.9371, Val Loss: 1.0516
2024-12-28 14:45:13,544 - INFO - Epoch 33/1000, Train Loss: 0.9381, Val Loss: 1.0558
2024-12-28 14:45:13,544 - INFO - Early stopping triggered at epoch 33
2024-12-28 14:45:13,544 - INFO - Training completed in 14.79s
2024-12-28 14:45:13,544 - INFO - Final memory usage: CPU 1924.4 MB, GPU 105.1 MB
2024-12-28 14:45:13,545 - INFO - Model training completed in 14.79s
2024-12-28 14:45:13,609 - INFO - Prediction completed in 0.06s
2024-12-28 14:45:13,622 - INFO - Poison rate 0.05 completed in 24.16s
2024-12-28 14:45:13,623 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:45:13,627 - INFO - Total number of labels flipped: 1374
2024-12-28 14:45:13,628 - INFO - Label flipping completed in 0.00s
2024-12-28 14:45:13,628 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:45:13,628 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:45:14,855 - INFO - Feature scaling completed in 1.23s
2024-12-28 14:45:14,855 - INFO - Starting feature selection (k=50)
2024-12-28 14:45:14,883 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:45:14,884 - INFO - Starting anomaly detection
2024-12-28 14:45:22,010 - INFO - Anomaly detection completed in 7.13s
2024-12-28 14:45:22,010 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:45:22,010 - INFO - Total fit_transform time: 8.38s
2024-12-28 14:45:22,010 - INFO - Training set processing completed in 8.38s
2024-12-28 14:45:22,010 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:45:22,011 - INFO - Memory usage at start_fit: CPU 1885.8 MB, GPU 105.0 MB
2024-12-28 14:45:22,012 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:45:22,020 - INFO - Number of unique classes: 43
2024-12-28 14:45:22,174 - INFO - Fitted scaler and transformed data
2024-12-28 14:45:22,174 - INFO - Scaling time: 0.15s
2024-12-28 14:45:22,601 - INFO - Epoch 1/1000, Train Loss: 2.1607, Val Loss: 1.7530
2024-12-28 14:45:23,018 - INFO - Epoch 2/1000, Train Loss: 1.5747, Val Loss: 1.5340
2024-12-28 14:45:23,480 - INFO - Epoch 3/1000, Train Loss: 1.4129, Val Loss: 1.4254
2024-12-28 14:45:23,922 - INFO - Epoch 4/1000, Train Loss: 1.3197, Val Loss: 1.3746
2024-12-28 14:45:24,347 - INFO - Epoch 5/1000, Train Loss: 1.2594, Val Loss: 1.3358
2024-12-28 14:45:24,759 - INFO - Epoch 6/1000, Train Loss: 1.2156, Val Loss: 1.3099
2024-12-28 14:45:25,214 - INFO - Epoch 7/1000, Train Loss: 1.1842, Val Loss: 1.2874
2024-12-28 14:45:25,633 - INFO - Epoch 8/1000, Train Loss: 1.1593, Val Loss: 1.2555
2024-12-28 14:45:26,057 - INFO - Epoch 9/1000, Train Loss: 1.1377, Val Loss: 1.2406
2024-12-28 14:45:26,506 - INFO - Epoch 10/1000, Train Loss: 1.1208, Val Loss: 1.2277
2024-12-28 14:45:26,970 - INFO - Epoch 11/1000, Train Loss: 1.1066, Val Loss: 1.2136
2024-12-28 14:45:27,430 - INFO - Epoch 12/1000, Train Loss: 1.0930, Val Loss: 1.2080
2024-12-28 14:45:27,882 - INFO - Epoch 13/1000, Train Loss: 1.0827, Val Loss: 1.1983
2024-12-28 14:45:28,324 - INFO - Epoch 14/1000, Train Loss: 1.0720, Val Loss: 1.1950
2024-12-28 14:45:28,755 - INFO - Epoch 15/1000, Train Loss: 1.0633, Val Loss: 1.1836
2024-12-28 14:45:29,162 - INFO - Epoch 16/1000, Train Loss: 1.0566, Val Loss: 1.1685
2024-12-28 14:45:29,585 - INFO - Epoch 17/1000, Train Loss: 1.0485, Val Loss: 1.1632
2024-12-28 14:45:30,027 - INFO - Epoch 18/1000, Train Loss: 1.0422, Val Loss: 1.1571
2024-12-28 14:45:30,442 - INFO - Epoch 19/1000, Train Loss: 1.0368, Val Loss: 1.1500
2024-12-28 14:45:30,922 - INFO - Epoch 20/1000, Train Loss: 1.0330, Val Loss: 1.1474
2024-12-28 14:45:31,347 - INFO - Epoch 21/1000, Train Loss: 1.0272, Val Loss: 1.1521
2024-12-28 14:45:31,801 - INFO - Epoch 22/1000, Train Loss: 1.0238, Val Loss: 1.1318
2024-12-28 14:45:32,237 - INFO - Epoch 23/1000, Train Loss: 1.0178, Val Loss: 1.1325
2024-12-28 14:45:32,660 - INFO - Epoch 24/1000, Train Loss: 1.0154, Val Loss: 1.1296
2024-12-28 14:45:33,107 - INFO - Epoch 25/1000, Train Loss: 1.0117, Val Loss: 1.1309
2024-12-28 14:45:33,559 - INFO - Epoch 26/1000, Train Loss: 1.0066, Val Loss: 1.1217
2024-12-28 14:45:33,980 - INFO - Epoch 27/1000, Train Loss: 1.0044, Val Loss: 1.1197
2024-12-28 14:45:34,451 - INFO - Epoch 28/1000, Train Loss: 1.0016, Val Loss: 1.1168
2024-12-28 14:45:34,899 - INFO - Epoch 29/1000, Train Loss: 1.0007, Val Loss: 1.1155
2024-12-28 14:45:35,344 - INFO - Epoch 30/1000, Train Loss: 0.9988, Val Loss: 1.1139
2024-12-28 14:45:35,829 - INFO - Epoch 31/1000, Train Loss: 0.9971, Val Loss: 1.1070
2024-12-28 14:45:36,278 - INFO - Epoch 32/1000, Train Loss: 0.9942, Val Loss: 1.1125
2024-12-28 14:45:36,736 - INFO - Epoch 33/1000, Train Loss: 0.9926, Val Loss: 1.1045
2024-12-28 14:45:37,205 - INFO - Epoch 34/1000, Train Loss: 0.9932, Val Loss: 1.1030
2024-12-28 14:45:37,668 - INFO - Epoch 35/1000, Train Loss: 0.9927, Val Loss: 1.0995
2024-12-28 14:45:38,152 - INFO - Epoch 36/1000, Train Loss: 0.9911, Val Loss: 1.1009
2024-12-28 14:45:38,644 - INFO - Epoch 37/1000, Train Loss: 0.9907, Val Loss: 1.1054
2024-12-28 14:45:39,113 - INFO - Epoch 38/1000, Train Loss: 0.9873, Val Loss: 1.0921
2024-12-28 14:45:39,592 - INFO - Epoch 39/1000, Train Loss: 0.9871, Val Loss: 1.1042
2024-12-28 14:45:40,085 - INFO - Epoch 40/1000, Train Loss: 0.9850, Val Loss: 1.0955
2024-12-28 14:45:40,540 - INFO - Epoch 41/1000, Train Loss: 0.9856, Val Loss: 1.0900
2024-12-28 14:45:40,986 - INFO - Epoch 42/1000, Train Loss: 0.9859, Val Loss: 1.1003
2024-12-28 14:45:41,394 - INFO - Epoch 43/1000, Train Loss: 0.9871, Val Loss: 1.1054
2024-12-28 14:45:41,794 - INFO - Epoch 44/1000, Train Loss: 0.9868, Val Loss: 1.0957
2024-12-28 14:45:42,214 - INFO - Epoch 45/1000, Train Loss: 0.9840, Val Loss: 1.1046
2024-12-28 14:45:42,643 - INFO - Epoch 46/1000, Train Loss: 0.9852, Val Loss: 1.0945
2024-12-28 14:45:42,643 - INFO - Early stopping triggered at epoch 46
2024-12-28 14:45:42,643 - INFO - Training completed in 20.63s
2024-12-28 14:45:42,644 - INFO - Final memory usage: CPU 1924.4 MB, GPU 105.1 MB
2024-12-28 14:45:42,644 - INFO - Model training completed in 20.63s
2024-12-28 14:45:42,711 - INFO - Prediction completed in 0.07s
2024-12-28 14:45:42,721 - INFO - Poison rate 0.07 completed in 29.10s
2024-12-28 14:45:42,721 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:45:42,724 - INFO - Total number of labels flipped: 1961
2024-12-28 14:45:42,724 - INFO - Label flipping completed in 0.00s
2024-12-28 14:45:42,724 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:45:42,724 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:45:43,974 - INFO - Feature scaling completed in 1.25s
2024-12-28 14:45:43,974 - INFO - Starting feature selection (k=50)
2024-12-28 14:45:44,001 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:45:44,001 - INFO - Starting anomaly detection
2024-12-28 14:45:49,816 - INFO - Anomaly detection completed in 5.81s
2024-12-28 14:45:49,816 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:45:49,816 - INFO - Total fit_transform time: 7.09s
2024-12-28 14:45:49,816 - INFO - Training set processing completed in 7.09s
2024-12-28 14:45:49,816 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:45:49,817 - INFO - Memory usage at start_fit: CPU 1885.8 MB, GPU 105.0 MB
2024-12-28 14:45:49,818 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:45:49,827 - INFO - Number of unique classes: 43
2024-12-28 14:45:49,963 - INFO - Fitted scaler and transformed data
2024-12-28 14:45:49,963 - INFO - Scaling time: 0.13s
2024-12-28 14:45:50,396 - INFO - Epoch 1/1000, Train Loss: 2.2792, Val Loss: 1.8613
2024-12-28 14:45:50,811 - INFO - Epoch 2/1000, Train Loss: 1.7145, Val Loss: 1.6681
2024-12-28 14:45:51,245 - INFO - Epoch 3/1000, Train Loss: 1.5494, Val Loss: 1.5602
2024-12-28 14:45:51,656 - INFO - Epoch 4/1000, Train Loss: 1.4566, Val Loss: 1.5021
2024-12-28 14:45:52,085 - INFO - Epoch 5/1000, Train Loss: 1.3942, Val Loss: 1.4593
2024-12-28 14:45:52,508 - INFO - Epoch 6/1000, Train Loss: 1.3481, Val Loss: 1.4211
2024-12-28 14:45:52,940 - INFO - Epoch 7/1000, Train Loss: 1.3108, Val Loss: 1.3947
2024-12-28 14:45:53,397 - INFO - Epoch 8/1000, Train Loss: 1.2841, Val Loss: 1.3812
2024-12-28 14:45:53,863 - INFO - Epoch 9/1000, Train Loss: 1.2610, Val Loss: 1.3659
2024-12-28 14:45:54,328 - INFO - Epoch 10/1000, Train Loss: 1.2398, Val Loss: 1.3355
2024-12-28 14:45:54,731 - INFO - Epoch 11/1000, Train Loss: 1.2196, Val Loss: 1.3292
2024-12-28 14:45:55,142 - INFO - Epoch 12/1000, Train Loss: 1.2057, Val Loss: 1.3082
2024-12-28 14:45:55,559 - INFO - Epoch 13/1000, Train Loss: 1.1910, Val Loss: 1.2951
2024-12-28 14:45:55,975 - INFO - Epoch 14/1000, Train Loss: 1.1790, Val Loss: 1.2916
2024-12-28 14:45:56,415 - INFO - Epoch 15/1000, Train Loss: 1.1684, Val Loss: 1.2727
2024-12-28 14:45:56,843 - INFO - Epoch 16/1000, Train Loss: 1.1588, Val Loss: 1.2625
2024-12-28 14:45:57,269 - INFO - Epoch 17/1000, Train Loss: 1.1486, Val Loss: 1.2508
2024-12-28 14:45:57,681 - INFO - Epoch 18/1000, Train Loss: 1.1392, Val Loss: 1.2393
2024-12-28 14:45:58,107 - INFO - Epoch 19/1000, Train Loss: 1.1299, Val Loss: 1.2259
2024-12-28 14:45:58,547 - INFO - Epoch 20/1000, Train Loss: 1.1243, Val Loss: 1.2208
2024-12-28 14:45:58,952 - INFO - Epoch 21/1000, Train Loss: 1.1170, Val Loss: 1.2133
2024-12-28 14:45:59,373 - INFO - Epoch 22/1000, Train Loss: 1.1111, Val Loss: 1.2096
2024-12-28 14:45:59,799 - INFO - Epoch 23/1000, Train Loss: 1.1041, Val Loss: 1.2079
2024-12-28 14:46:00,223 - INFO - Epoch 24/1000, Train Loss: 1.1001, Val Loss: 1.2042
2024-12-28 14:46:00,667 - INFO - Epoch 25/1000, Train Loss: 1.0937, Val Loss: 1.1909
2024-12-28 14:46:01,085 - INFO - Epoch 26/1000, Train Loss: 1.0893, Val Loss: 1.1926
2024-12-28 14:46:01,516 - INFO - Epoch 27/1000, Train Loss: 1.0846, Val Loss: 1.1838
2024-12-28 14:46:01,945 - INFO - Epoch 28/1000, Train Loss: 1.0803, Val Loss: 1.1786
2024-12-28 14:46:02,359 - INFO - Epoch 29/1000, Train Loss: 1.0772, Val Loss: 1.1753
2024-12-28 14:46:02,794 - INFO - Epoch 30/1000, Train Loss: 1.0739, Val Loss: 1.1776
2024-12-28 14:46:03,242 - INFO - Epoch 31/1000, Train Loss: 1.0727, Val Loss: 1.1541
2024-12-28 14:46:03,679 - INFO - Epoch 32/1000, Train Loss: 1.0668, Val Loss: 1.1595
2024-12-28 14:46:04,147 - INFO - Epoch 33/1000, Train Loss: 1.0654, Val Loss: 1.1603
2024-12-28 14:46:04,570 - INFO - Epoch 34/1000, Train Loss: 1.0631, Val Loss: 1.1616
2024-12-28 14:46:05,004 - INFO - Epoch 35/1000, Train Loss: 1.0611, Val Loss: 1.1435
2024-12-28 14:46:05,435 - INFO - Epoch 36/1000, Train Loss: 1.0574, Val Loss: 1.1554
2024-12-28 14:46:05,883 - INFO - Epoch 37/1000, Train Loss: 1.0577, Val Loss: 1.1528
2024-12-28 14:46:06,309 - INFO - Epoch 38/1000, Train Loss: 1.0558, Val Loss: 1.1461
2024-12-28 14:46:06,748 - INFO - Epoch 39/1000, Train Loss: 1.0551, Val Loss: 1.1395
2024-12-28 14:46:07,185 - INFO - Epoch 40/1000, Train Loss: 1.0530, Val Loss: 1.1538
2024-12-28 14:46:07,685 - INFO - Epoch 41/1000, Train Loss: 1.0519, Val Loss: 1.1506
2024-12-28 14:46:08,163 - INFO - Epoch 42/1000, Train Loss: 1.0521, Val Loss: 1.1431
2024-12-28 14:46:08,626 - INFO - Epoch 43/1000, Train Loss: 1.0512, Val Loss: 1.1443
2024-12-28 14:46:09,085 - INFO - Epoch 44/1000, Train Loss: 1.0496, Val Loss: 1.1264
2024-12-28 14:46:09,518 - INFO - Epoch 45/1000, Train Loss: 1.0478, Val Loss: 1.1368
2024-12-28 14:46:09,972 - INFO - Epoch 46/1000, Train Loss: 1.0475, Val Loss: 1.1438
2024-12-28 14:46:10,425 - INFO - Epoch 47/1000, Train Loss: 1.0475, Val Loss: 1.1399
2024-12-28 14:46:10,888 - INFO - Epoch 48/1000, Train Loss: 1.0474, Val Loss: 1.1357
2024-12-28 14:46:11,302 - INFO - Epoch 49/1000, Train Loss: 1.0446, Val Loss: 1.1335
2024-12-28 14:46:11,303 - INFO - Early stopping triggered at epoch 49
2024-12-28 14:46:11,303 - INFO - Training completed in 21.49s
2024-12-28 14:46:11,304 - INFO - Final memory usage: CPU 1924.4 MB, GPU 105.1 MB
2024-12-28 14:46:11,305 - INFO - Model training completed in 21.49s
2024-12-28 14:46:11,382 - INFO - Prediction completed in 0.08s
2024-12-28 14:46:11,393 - INFO - Poison rate 0.1 completed in 28.67s
2024-12-28 14:46:11,393 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:46:11,396 - INFO - Total number of labels flipped: 3928
2024-12-28 14:46:11,396 - INFO - Label flipping completed in 0.00s
2024-12-28 14:46:11,396 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:46:11,396 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:46:12,606 - INFO - Feature scaling completed in 1.21s
2024-12-28 14:46:12,606 - INFO - Starting feature selection (k=50)
2024-12-28 14:46:12,634 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:46:12,635 - INFO - Starting anomaly detection
2024-12-28 14:46:20,673 - INFO - Anomaly detection completed in 8.04s
2024-12-28 14:46:20,674 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:46:20,674 - INFO - Total fit_transform time: 9.28s
2024-12-28 14:46:20,674 - INFO - Training set processing completed in 9.28s
2024-12-28 14:46:20,674 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 14:46:20,675 - INFO - Memory usage at start_fit: CPU 1885.8 MB, GPU 105.0 MB
2024-12-28 14:46:20,675 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:46:20,683 - INFO - Number of unique classes: 43
2024-12-28 14:46:20,835 - INFO - Fitted scaler and transformed data
2024-12-28 14:46:20,835 - INFO - Scaling time: 0.15s
2024-12-28 14:46:21,250 - INFO - Epoch 1/1000, Train Loss: 2.5648, Val Loss: 2.3051
2024-12-28 14:46:21,655 - INFO - Epoch 2/1000, Train Loss: 2.0813, Val Loss: 2.1299
2024-12-28 14:46:22,055 - INFO - Epoch 3/1000, Train Loss: 1.9228, Val Loss: 2.0341
2024-12-28 14:46:22,451 - INFO - Epoch 4/1000, Train Loss: 1.8274, Val Loss: 1.9671
2024-12-28 14:46:22,945 - INFO - Epoch 5/1000, Train Loss: 1.7521, Val Loss: 1.8985
2024-12-28 14:46:23,377 - INFO - Epoch 6/1000, Train Loss: 1.6949, Val Loss: 1.8491
2024-12-28 14:46:23,806 - INFO - Epoch 7/1000, Train Loss: 1.6473, Val Loss: 1.8227
2024-12-28 14:46:24,245 - INFO - Epoch 8/1000, Train Loss: 1.6060, Val Loss: 1.7807
2024-12-28 14:46:24,653 - INFO - Epoch 9/1000, Train Loss: 1.5707, Val Loss: 1.7497
2024-12-28 14:46:25,072 - INFO - Epoch 10/1000, Train Loss: 1.5369, Val Loss: 1.7138
2024-12-28 14:46:25,503 - INFO - Epoch 11/1000, Train Loss: 1.5123, Val Loss: 1.6939
2024-12-28 14:46:25,918 - INFO - Epoch 12/1000, Train Loss: 1.4839, Val Loss: 1.6598
2024-12-28 14:46:26,341 - INFO - Epoch 13/1000, Train Loss: 1.4589, Val Loss: 1.6378
2024-12-28 14:46:26,786 - INFO - Epoch 14/1000, Train Loss: 1.4383, Val Loss: 1.6241
2024-12-28 14:46:27,218 - INFO - Epoch 15/1000, Train Loss: 1.4190, Val Loss: 1.6080
2024-12-28 14:46:27,663 - INFO - Epoch 16/1000, Train Loss: 1.3990, Val Loss: 1.5777
2024-12-28 14:46:28,078 - INFO - Epoch 17/1000, Train Loss: 1.3813, Val Loss: 1.5539
2024-12-28 14:46:28,477 - INFO - Epoch 18/1000, Train Loss: 1.3645, Val Loss: 1.5386
2024-12-28 14:46:28,913 - INFO - Epoch 19/1000, Train Loss: 1.3486, Val Loss: 1.5139
2024-12-28 14:46:29,330 - INFO - Epoch 20/1000, Train Loss: 1.3340, Val Loss: 1.4921
2024-12-28 14:46:29,770 - INFO - Epoch 21/1000, Train Loss: 1.3208, Val Loss: 1.4939
2024-12-28 14:46:30,201 - INFO - Epoch 22/1000, Train Loss: 1.3076, Val Loss: 1.4685
2024-12-28 14:46:30,615 - INFO - Epoch 23/1000, Train Loss: 1.2984, Val Loss: 1.4533
2024-12-28 14:46:31,025 - INFO - Epoch 24/1000, Train Loss: 1.2863, Val Loss: 1.4483
2024-12-28 14:46:31,458 - INFO - Epoch 25/1000, Train Loss: 1.2754, Val Loss: 1.4371
2024-12-28 14:46:31,903 - INFO - Epoch 26/1000, Train Loss: 1.2666, Val Loss: 1.4275
2024-12-28 14:46:32,341 - INFO - Epoch 27/1000, Train Loss: 1.2571, Val Loss: 1.4236
2024-12-28 14:46:32,762 - INFO - Epoch 28/1000, Train Loss: 1.2498, Val Loss: 1.4117
2024-12-28 14:46:33,209 - INFO - Epoch 29/1000, Train Loss: 1.2412, Val Loss: 1.3882
2024-12-28 14:46:33,650 - INFO - Epoch 30/1000, Train Loss: 1.2364, Val Loss: 1.3914
2024-12-28 14:46:34,098 - INFO - Epoch 31/1000, Train Loss: 1.2287, Val Loss: 1.3766
2024-12-28 14:46:34,551 - INFO - Epoch 32/1000, Train Loss: 1.2234, Val Loss: 1.3769
2024-12-28 14:46:34,955 - INFO - Epoch 33/1000, Train Loss: 1.2168, Val Loss: 1.3644
2024-12-28 14:46:35,379 - INFO - Epoch 34/1000, Train Loss: 1.2115, Val Loss: 1.3705
2024-12-28 14:46:35,814 - INFO - Epoch 35/1000, Train Loss: 1.2066, Val Loss: 1.3580
2024-12-28 14:46:36,250 - INFO - Epoch 36/1000, Train Loss: 1.2046, Val Loss: 1.3526
2024-12-28 14:46:36,692 - INFO - Epoch 37/1000, Train Loss: 1.1986, Val Loss: 1.3517
2024-12-28 14:46:37,112 - INFO - Epoch 38/1000, Train Loss: 1.1939, Val Loss: 1.3492
2024-12-28 14:46:37,547 - INFO - Epoch 39/1000, Train Loss: 1.1932, Val Loss: 1.3383
2024-12-28 14:46:38,010 - INFO - Epoch 40/1000, Train Loss: 1.1897, Val Loss: 1.3266
2024-12-28 14:46:38,489 - INFO - Epoch 41/1000, Train Loss: 1.1870, Val Loss: 1.3260
2024-12-28 14:46:38,966 - INFO - Epoch 42/1000, Train Loss: 1.1846, Val Loss: 1.3374
2024-12-28 14:46:39,475 - INFO - Epoch 43/1000, Train Loss: 1.1810, Val Loss: 1.3202
2024-12-28 14:46:39,978 - INFO - Epoch 44/1000, Train Loss: 1.1801, Val Loss: 1.3287
2024-12-28 14:46:40,459 - INFO - Epoch 45/1000, Train Loss: 1.1787, Val Loss: 1.3276
2024-12-28 14:46:40,936 - INFO - Epoch 46/1000, Train Loss: 1.1774, Val Loss: 1.3323
2024-12-28 14:46:41,571 - INFO - Epoch 47/1000, Train Loss: 1.1755, Val Loss: 1.3179
2024-12-28 14:46:42,427 - INFO - Epoch 48/1000, Train Loss: 1.1763, Val Loss: 1.3245
2024-12-28 14:46:43,284 - INFO - Epoch 49/1000, Train Loss: 1.1741, Val Loss: 1.3184
2024-12-28 14:46:44,154 - INFO - Epoch 50/1000, Train Loss: 1.1714, Val Loss: 1.3119
2024-12-28 14:46:44,869 - INFO - Epoch 51/1000, Train Loss: 1.1716, Val Loss: 1.3101
2024-12-28 14:46:45,500 - INFO - Epoch 52/1000, Train Loss: 1.1696, Val Loss: 1.3125
2024-12-28 14:46:46,339 - INFO - Epoch 53/1000, Train Loss: 1.1726, Val Loss: 1.3104
2024-12-28 14:46:47,184 - INFO - Epoch 54/1000, Train Loss: 1.1698, Val Loss: 1.3172
2024-12-28 14:46:47,999 - INFO - Epoch 55/1000, Train Loss: 1.1695, Val Loss: 1.3212
2024-12-28 14:46:48,834 - INFO - Epoch 56/1000, Train Loss: 1.1679, Val Loss: 1.3097
2024-12-28 14:46:48,835 - INFO - Early stopping triggered at epoch 56
2024-12-28 14:46:48,835 - INFO - Training completed in 28.16s
2024-12-28 14:46:48,835 - INFO - Final memory usage: CPU 1924.4 MB, GPU 105.1 MB
2024-12-28 14:46:48,836 - INFO - Model training completed in 28.16s
2024-12-28 14:46:48,926 - INFO - Prediction completed in 0.09s
2024-12-28 14:46:48,937 - INFO - Poison rate 0.2 completed in 37.54s
2024-12-28 14:46:48,940 - INFO - Loaded 77 existing results
2024-12-28 14:46:48,940 - INFO - Total results to save: 84
2024-12-28 14:46:48,940 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:46:48,946 - INFO - Saved 84 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:46:48,946 - INFO - Total evaluation time: 218.05s
2024-12-28 14:46:48,955 - INFO - 
Progress: 13.5% - Evaluating GTSRB with RandomForest (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:46:49,141 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:46:49,141 - INFO - Dataset type: image
2024-12-28 14:46:49,141 - INFO - Sample size: 39209
2024-12-28 14:46:49,141 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:46:49,142 - INFO - Loading datasets...
2024-12-28 14:47:07,235 - INFO - Dataset loading completed in 18.09s
2024-12-28 14:47:07,235 - INFO - Extracting validation features...
2024-12-28 14:47:07,235 - INFO - Extracting features from 4435 samples...
2024-12-28 14:47:08,041 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:47:08,051 - INFO - Validation feature extraction completed in 0.82s
2024-12-28 14:47:08,051 - INFO - Extracting training features...
2024-12-28 14:47:08,051 - INFO - Extracting features from 19755 samples...
2024-12-28 14:47:10,722 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:47:10,733 - INFO - Training feature extraction completed in 2.68s
2024-12-28 14:47:10,733 - INFO - Creating model for classifier: RandomForest
2024-12-28 14:47:10,733 - INFO - Using device: cuda
2024-12-28 14:47:10,734 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:47:10,734 - INFO - Training set processing completed in 0.00s
2024-12-28 14:47:10,734 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:47:10,735 - INFO - Memory usage at start_fit: CPU 1927.0 MB, GPU 104.0 MB
2024-12-28 14:47:10,735 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:47:11,324 - INFO - Fitted scaler and transformed data
2024-12-28 14:47:11,325 - INFO - Scaling time: 0.59s
2024-12-28 14:47:11,344 - INFO - Number of unique classes: 43
2024-12-28 14:47:17,145 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:47:23,878 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 14:47:30,172 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7593
2024-12-28 14:47:36,106 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7586
2024-12-28 14:47:36,106 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:47:36,106 - INFO - Training completed in 25.37s
2024-12-28 14:47:36,107 - INFO - Final memory usage: CPU 1927.0 MB, GPU 153.6 MB
2024-12-28 14:47:36,107 - INFO - Model training completed in 25.37s
2024-12-28 14:47:36,308 - INFO - Prediction completed in 0.20s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:47:36,320 - INFO - Poison rate 0.0 completed in 25.59s
2024-12-28 14:47:36,320 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:47:36,322 - INFO - Total number of labels flipped: 195
2024-12-28 14:47:36,322 - INFO - Label flipping completed in 0.00s
2024-12-28 14:47:36,322 - INFO - Training set processing completed in 0.00s
2024-12-28 14:47:36,322 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:47:36,323 - INFO - Memory usage at start_fit: CPU 1927.0 MB, GPU 112.5 MB
2024-12-28 14:47:36,323 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:47:36,826 - INFO - Fitted scaler and transformed data
2024-12-28 14:47:36,826 - INFO - Scaling time: 0.50s
2024-12-28 14:47:36,845 - INFO - Number of unique classes: 43
2024-12-28 14:47:43,160 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:47:48,801 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 14:47:54,830 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7593
2024-12-28 14:48:00,416 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7586
2024-12-28 14:48:00,416 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:48:00,416 - INFO - Training completed in 24.09s
2024-12-28 14:48:00,416 - INFO - Final memory usage: CPU 1927.0 MB, GPU 153.6 MB
2024-12-28 14:48:00,417 - INFO - Model training completed in 24.09s
2024-12-28 14:48:00,587 - INFO - Prediction completed in 0.17s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:48:00,599 - INFO - Poison rate 0.01 completed in 24.28s
2024-12-28 14:48:00,599 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:48:00,601 - INFO - Total number of labels flipped: 588
2024-12-28 14:48:00,601 - INFO - Label flipping completed in 0.00s
2024-12-28 14:48:00,601 - INFO - Training set processing completed in 0.00s
2024-12-28 14:48:00,602 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:48:00,602 - INFO - Memory usage at start_fit: CPU 1927.0 MB, GPU 112.5 MB
2024-12-28 14:48:00,603 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:48:01,158 - INFO - Fitted scaler and transformed data
2024-12-28 14:48:01,158 - INFO - Scaling time: 0.56s
2024-12-28 14:48:01,177 - INFO - Number of unique classes: 43
2024-12-28 14:48:07,480 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:48:13,392 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 14:48:19,004 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7593
2024-12-28 14:48:24,959 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7586
2024-12-28 14:48:24,959 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:48:24,959 - INFO - Training completed in 24.36s
2024-12-28 14:48:24,960 - INFO - Final memory usage: CPU 1927.0 MB, GPU 153.6 MB
2024-12-28 14:48:24,960 - INFO - Model training completed in 24.36s
2024-12-28 14:48:25,158 - INFO - Prediction completed in 0.20s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:48:25,170 - INFO - Poison rate 0.03 completed in 24.57s
2024-12-28 14:48:25,170 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:48:25,172 - INFO - Total number of labels flipped: 980
2024-12-28 14:48:25,173 - INFO - Label flipping completed in 0.00s
2024-12-28 14:48:25,173 - INFO - Training set processing completed in 0.00s
2024-12-28 14:48:25,173 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:48:25,174 - INFO - Memory usage at start_fit: CPU 1927.0 MB, GPU 112.5 MB
2024-12-28 14:48:25,174 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:48:25,674 - INFO - Fitted scaler and transformed data
2024-12-28 14:48:25,675 - INFO - Scaling time: 0.50s
2024-12-28 14:48:25,694 - INFO - Number of unique classes: 43
2024-12-28 14:48:31,929 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:48:38,517 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 14:48:45,947 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 14:48:52,613 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7585
2024-12-28 14:48:52,613 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:48:52,613 - INFO - Training completed in 27.44s
2024-12-28 14:48:52,613 - INFO - Final memory usage: CPU 1927.1 MB, GPU 153.6 MB
2024-12-28 14:48:52,614 - INFO - Model training completed in 27.44s
2024-12-28 14:48:52,943 - INFO - Prediction completed in 0.33s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:48:52,954 - INFO - Poison rate 0.05 completed in 27.78s
2024-12-28 14:48:52,954 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:48:52,956 - INFO - Total number of labels flipped: 1374
2024-12-28 14:48:52,957 - INFO - Label flipping completed in 0.00s
2024-12-28 14:48:52,957 - INFO - Training set processing completed in 0.00s
2024-12-28 14:48:52,957 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:48:52,958 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 112.5 MB
2024-12-28 14:48:52,958 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:48:53,482 - INFO - Fitted scaler and transformed data
2024-12-28 14:48:53,482 - INFO - Scaling time: 0.52s
2024-12-28 14:48:53,503 - INFO - Number of unique classes: 43
2024-12-28 14:49:00,973 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:49:08,816 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7600
2024-12-28 14:49:16,506 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7593
2024-12-28 14:49:23,372 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7586
2024-12-28 14:49:23,372 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:49:23,372 - INFO - Training completed in 30.41s
2024-12-28 14:49:23,372 - INFO - Final memory usage: CPU 1927.1 MB, GPU 153.6 MB
2024-12-28 14:49:23,373 - INFO - Model training completed in 30.42s
2024-12-28 14:49:23,601 - INFO - Prediction completed in 0.23s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:49:23,614 - INFO - Poison rate 0.07 completed in 30.66s
2024-12-28 14:49:23,614 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:49:23,617 - INFO - Total number of labels flipped: 1960
2024-12-28 14:49:23,617 - INFO - Label flipping completed in 0.00s
2024-12-28 14:49:23,617 - INFO - Training set processing completed in 0.00s
2024-12-28 14:49:23,617 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:49:23,618 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 112.5 MB
2024-12-28 14:49:23,618 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:49:24,149 - INFO - Fitted scaler and transformed data
2024-12-28 14:49:24,149 - INFO - Scaling time: 0.53s
2024-12-28 14:49:24,168 - INFO - Number of unique classes: 43
2024-12-28 14:49:32,025 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:49:40,500 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 14:49:47,933 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 14:49:55,828 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7585
2024-12-28 14:49:55,828 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:49:55,828 - INFO - Training completed in 32.21s
2024-12-28 14:49:55,829 - INFO - Final memory usage: CPU 1927.1 MB, GPU 153.6 MB
2024-12-28 14:49:55,829 - INFO - Model training completed in 32.21s
2024-12-28 14:49:56,101 - INFO - Prediction completed in 0.27s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:49:56,114 - INFO - Poison rate 0.1 completed in 32.50s
2024-12-28 14:49:56,114 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:49:56,117 - INFO - Total number of labels flipped: 3925
2024-12-28 14:49:56,117 - INFO - Label flipping completed in 0.00s
2024-12-28 14:49:56,117 - INFO - Training set processing completed in 0.00s
2024-12-28 14:49:56,117 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:49:56,118 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 112.5 MB
2024-12-28 14:49:56,118 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:49:56,686 - INFO - Fitted scaler and transformed data
2024-12-28 14:49:56,686 - INFO - Scaling time: 0.57s
2024-12-28 14:49:56,705 - INFO - Number of unique classes: 43
2024-12-28 14:50:03,291 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 14:50:09,470 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 14:50:15,988 - INFO - Epoch 3/10, Train Loss: 3.7594, Val Loss: 3.7592
2024-12-28 14:50:22,803 - INFO - Epoch 4/10, Train Loss: 3.7587, Val Loss: 3.7584
2024-12-28 14:50:22,803 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:50:22,803 - INFO - Training completed in 26.68s
2024-12-28 14:50:22,803 - INFO - Final memory usage: CPU 1927.1 MB, GPU 153.6 MB
2024-12-28 14:50:22,804 - INFO - Model training completed in 26.69s
2024-12-28 14:50:22,983 - INFO - Prediction completed in 0.18s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:50:22,996 - INFO - Poison rate 0.2 completed in 26.88s
2024-12-28 14:50:22,999 - INFO - Loaded 84 existing results
2024-12-28 14:50:22,999 - INFO - Total results to save: 91
2024-12-28 14:50:23,000 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:50:23,005 - INFO - Saved 91 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:50:23,005 - INFO - Total evaluation time: 213.86s
2024-12-28 14:50:23,010 - INFO - 
Progress: 14.6% - Evaluating GTSRB with RandomForest (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:50:23,186 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:50:23,186 - INFO - Dataset type: image
2024-12-28 14:50:23,186 - INFO - Sample size: 39209
2024-12-28 14:50:23,186 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:50:23,187 - INFO - Loading datasets...
2024-12-28 14:50:42,244 - INFO - Dataset loading completed in 19.06s
2024-12-28 14:50:42,244 - INFO - Extracting validation features...
2024-12-28 14:50:42,245 - INFO - Extracting features from 4435 samples...
2024-12-28 14:50:43,017 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:50:43,022 - INFO - Validation feature extraction completed in 0.78s
2024-12-28 14:50:43,022 - INFO - Extracting training features...
2024-12-28 14:50:43,023 - INFO - Extracting features from 19755 samples...
2024-12-28 14:50:45,682 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:50:45,693 - INFO - Training feature extraction completed in 2.67s
2024-12-28 14:50:45,694 - INFO - Creating model for classifier: RandomForest
2024-12-28 14:50:45,694 - INFO - Using device: cuda
2024-12-28 14:50:45,694 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:50:45,694 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:50:45,694 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:50:46,903 - INFO - Feature scaling completed in 1.21s
2024-12-28 14:50:46,904 - INFO - Starting feature selection (k=50)
2024-12-28 14:50:46,932 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:50:46,932 - INFO - Starting anomaly detection
2024-12-28 14:50:54,715 - INFO - Anomaly detection completed in 7.78s
2024-12-28 14:50:54,715 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:50:54,715 - INFO - Total fit_transform time: 9.02s
2024-12-28 14:50:54,715 - INFO - Training set processing completed in 9.02s
2024-12-28 14:50:54,715 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:50:54,717 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 104.6 MB
2024-12-28 14:50:54,717 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:50:55,289 - INFO - Fitted scaler and transformed data
2024-12-28 14:50:55,289 - INFO - Scaling time: 0.57s
2024-12-28 14:50:55,304 - INFO - Number of unique classes: 43
2024-12-28 14:51:03,446 - INFO - Epoch 1/10, Train Loss: 3.5719, Val Loss: 3.7606
2024-12-28 14:51:09,740 - INFO - Epoch 2/10, Train Loss: 3.5712, Val Loss: 3.7599
2024-12-28 14:51:15,677 - INFO - Epoch 3/10, Train Loss: 3.5705, Val Loss: 3.7592
2024-12-28 14:51:22,117 - INFO - Epoch 4/10, Train Loss: 3.5698, Val Loss: 3.7585
2024-12-28 14:51:22,117 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:51:22,117 - INFO - Training completed in 27.40s
2024-12-28 14:51:22,117 - INFO - Final memory usage: CPU 1927.1 MB, GPU 154.2 MB
2024-12-28 14:51:22,118 - INFO - Model training completed in 27.40s
2024-12-28 14:51:22,368 - INFO - Prediction completed in 0.25s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:51:22,381 - INFO - Poison rate 0.0 completed in 36.69s
2024-12-28 14:51:22,381 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:51:22,384 - INFO - Total number of labels flipped: 196
2024-12-28 14:51:22,384 - INFO - Label flipping completed in 0.00s
2024-12-28 14:51:22,384 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:51:22,384 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:51:23,632 - INFO - Feature scaling completed in 1.25s
2024-12-28 14:51:23,632 - INFO - Starting feature selection (k=50)
2024-12-28 14:51:23,656 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:51:23,656 - INFO - Starting anomaly detection
2024-12-28 14:51:31,880 - INFO - Anomaly detection completed in 8.22s
2024-12-28 14:51:31,880 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:51:31,881 - INFO - Total fit_transform time: 9.50s
2024-12-28 14:51:31,881 - INFO - Training set processing completed in 9.50s
2024-12-28 14:51:31,881 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:51:31,882 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 113.1 MB
2024-12-28 14:51:31,882 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:51:32,430 - INFO - Fitted scaler and transformed data
2024-12-28 14:51:32,430 - INFO - Scaling time: 0.55s
2024-12-28 14:51:32,449 - INFO - Number of unique classes: 43
2024-12-28 14:51:38,304 - INFO - Epoch 1/10, Train Loss: 3.5728, Val Loss: 3.7606
2024-12-28 14:51:45,475 - INFO - Epoch 2/10, Train Loss: 3.5722, Val Loss: 3.7599
2024-12-28 14:51:51,984 - INFO - Epoch 3/10, Train Loss: 3.5715, Val Loss: 3.7592
2024-12-28 14:51:58,653 - INFO - Epoch 4/10, Train Loss: 3.5708, Val Loss: 3.7585
2024-12-28 14:51:58,654 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:51:58,654 - INFO - Training completed in 26.77s
2024-12-28 14:51:58,654 - INFO - Final memory usage: CPU 1927.1 MB, GPU 154.2 MB
2024-12-28 14:51:58,654 - INFO - Model training completed in 26.77s
2024-12-28 14:51:58,936 - INFO - Prediction completed in 0.28s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:51:58,947 - INFO - Poison rate 0.01 completed in 36.57s
2024-12-28 14:51:58,947 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:51:58,949 - INFO - Total number of labels flipped: 590
2024-12-28 14:51:58,950 - INFO - Label flipping completed in 0.00s
2024-12-28 14:51:58,950 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:51:58,950 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:52:00,199 - INFO - Feature scaling completed in 1.25s
2024-12-28 14:52:00,199 - INFO - Starting feature selection (k=50)
2024-12-28 14:52:00,222 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:52:00,223 - INFO - Starting anomaly detection
2024-12-28 14:52:08,398 - INFO - Anomaly detection completed in 8.18s
2024-12-28 14:52:08,399 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:52:08,399 - INFO - Total fit_transform time: 9.45s
2024-12-28 14:52:08,399 - INFO - Training set processing completed in 9.45s
2024-12-28 14:52:08,399 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:52:08,400 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 113.1 MB
2024-12-28 14:52:08,400 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:52:08,919 - INFO - Fitted scaler and transformed data
2024-12-28 14:52:08,919 - INFO - Scaling time: 0.52s
2024-12-28 14:52:08,932 - INFO - Number of unique classes: 43
2024-12-28 14:52:14,424 - INFO - Epoch 1/10, Train Loss: 3.5725, Val Loss: 3.7606
2024-12-28 14:52:21,108 - INFO - Epoch 2/10, Train Loss: 3.5718, Val Loss: 3.7600
2024-12-28 14:52:27,440 - INFO - Epoch 3/10, Train Loss: 3.5712, Val Loss: 3.7593
2024-12-28 14:52:33,055 - INFO - Epoch 4/10, Train Loss: 3.5705, Val Loss: 3.7587
2024-12-28 14:52:33,055 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:52:33,055 - INFO - Training completed in 24.66s
2024-12-28 14:52:33,056 - INFO - Final memory usage: CPU 1927.1 MB, GPU 154.2 MB
2024-12-28 14:52:33,056 - INFO - Model training completed in 24.66s
2024-12-28 14:52:33,266 - INFO - Prediction completed in 0.21s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:52:33,277 - INFO - Poison rate 0.03 completed in 34.33s
2024-12-28 14:52:33,278 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:52:33,280 - INFO - Total number of labels flipped: 983
2024-12-28 14:52:33,280 - INFO - Label flipping completed in 0.00s
2024-12-28 14:52:33,280 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:52:33,280 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:52:34,456 - INFO - Feature scaling completed in 1.18s
2024-12-28 14:52:34,456 - INFO - Starting feature selection (k=50)
2024-12-28 14:52:34,480 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:52:34,480 - INFO - Starting anomaly detection
2024-12-28 14:52:42,869 - INFO - Anomaly detection completed in 8.39s
2024-12-28 14:52:42,869 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:52:42,870 - INFO - Total fit_transform time: 9.59s
2024-12-28 14:52:42,870 - INFO - Training set processing completed in 9.59s
2024-12-28 14:52:42,870 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:52:42,871 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 113.1 MB
2024-12-28 14:52:42,871 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:52:43,404 - INFO - Fitted scaler and transformed data
2024-12-28 14:52:43,404 - INFO - Scaling time: 0.53s
2024-12-28 14:52:43,422 - INFO - Number of unique classes: 43
2024-12-28 14:52:50,679 - INFO - Epoch 1/10, Train Loss: 3.5718, Val Loss: 3.7606
2024-12-28 14:52:56,524 - INFO - Epoch 2/10, Train Loss: 3.5711, Val Loss: 3.7599
2024-12-28 14:53:03,501 - INFO - Epoch 3/10, Train Loss: 3.5705, Val Loss: 3.7593
2024-12-28 14:53:10,273 - INFO - Epoch 4/10, Train Loss: 3.5698, Val Loss: 3.7586
2024-12-28 14:53:10,274 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:53:10,274 - INFO - Training completed in 27.40s
2024-12-28 14:53:10,274 - INFO - Final memory usage: CPU 1927.1 MB, GPU 154.2 MB
2024-12-28 14:53:10,274 - INFO - Model training completed in 27.40s
2024-12-28 14:53:10,488 - INFO - Prediction completed in 0.21s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:53:10,499 - INFO - Poison rate 0.05 completed in 37.22s
2024-12-28 14:53:10,500 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:53:10,502 - INFO - Total number of labels flipped: 1378
2024-12-28 14:53:10,502 - INFO - Label flipping completed in 0.00s
2024-12-28 14:53:10,502 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:53:10,502 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:53:11,636 - INFO - Feature scaling completed in 1.13s
2024-12-28 14:53:11,636 - INFO - Starting feature selection (k=50)
2024-12-28 14:53:11,658 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:53:11,658 - INFO - Starting anomaly detection
2024-12-28 14:53:19,247 - INFO - Anomaly detection completed in 7.59s
2024-12-28 14:53:19,247 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:53:19,247 - INFO - Total fit_transform time: 8.75s
2024-12-28 14:53:19,247 - INFO - Training set processing completed in 8.75s
2024-12-28 14:53:19,247 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:53:19,248 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 113.1 MB
2024-12-28 14:53:19,248 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:53:19,810 - INFO - Fitted scaler and transformed data
2024-12-28 14:53:19,810 - INFO - Scaling time: 0.56s
2024-12-28 14:53:19,823 - INFO - Number of unique classes: 43
2024-12-28 14:53:26,184 - INFO - Epoch 1/10, Train Loss: 3.5717, Val Loss: 3.7606
2024-12-28 14:53:32,455 - INFO - Epoch 2/10, Train Loss: 3.5711, Val Loss: 3.7599
2024-12-28 14:53:38,189 - INFO - Epoch 3/10, Train Loss: 3.5704, Val Loss: 3.7593
2024-12-28 14:53:44,198 - INFO - Epoch 4/10, Train Loss: 3.5698, Val Loss: 3.7586
2024-12-28 14:53:44,198 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:53:44,198 - INFO - Training completed in 24.95s
2024-12-28 14:53:44,198 - INFO - Final memory usage: CPU 1927.1 MB, GPU 154.2 MB
2024-12-28 14:53:44,199 - INFO - Model training completed in 24.95s
2024-12-28 14:53:44,442 - INFO - Prediction completed in 0.24s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:53:44,455 - INFO - Poison rate 0.07 completed in 33.96s
2024-12-28 14:53:44,455 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:53:44,458 - INFO - Total number of labels flipped: 1965
2024-12-28 14:53:44,458 - INFO - Label flipping completed in 0.00s
2024-12-28 14:53:44,458 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:53:44,458 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:53:45,645 - INFO - Feature scaling completed in 1.19s
2024-12-28 14:53:45,645 - INFO - Starting feature selection (k=50)
2024-12-28 14:53:45,667 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:53:45,668 - INFO - Starting anomaly detection
2024-12-28 14:53:52,507 - INFO - Anomaly detection completed in 6.84s
2024-12-28 14:53:52,507 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:53:52,507 - INFO - Total fit_transform time: 8.05s
2024-12-28 14:53:52,507 - INFO - Training set processing completed in 8.05s
2024-12-28 14:53:52,507 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:53:52,508 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 113.1 MB
2024-12-28 14:53:52,508 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:53:53,080 - INFO - Fitted scaler and transformed data
2024-12-28 14:53:53,080 - INFO - Scaling time: 0.57s
2024-12-28 14:53:53,095 - INFO - Number of unique classes: 43
2024-12-28 14:53:59,873 - INFO - Epoch 1/10, Train Loss: 3.5743, Val Loss: 3.7606
2024-12-28 14:54:06,865 - INFO - Epoch 2/10, Train Loss: 3.5737, Val Loss: 3.7600
2024-12-28 14:54:13,578 - INFO - Epoch 3/10, Train Loss: 3.5730, Val Loss: 3.7593
2024-12-28 14:54:19,568 - INFO - Epoch 4/10, Train Loss: 3.5723, Val Loss: 3.7587
2024-12-28 14:54:19,568 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:54:19,568 - INFO - Training completed in 27.06s
2024-12-28 14:54:19,568 - INFO - Final memory usage: CPU 1927.1 MB, GPU 154.2 MB
2024-12-28 14:54:19,569 - INFO - Model training completed in 27.06s
2024-12-28 14:54:19,760 - INFO - Prediction completed in 0.19s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:54:19,772 - INFO - Poison rate 0.1 completed in 35.32s
2024-12-28 14:54:19,772 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:54:19,775 - INFO - Total number of labels flipped: 3929
2024-12-28 14:54:19,776 - INFO - Label flipping completed in 0.00s
2024-12-28 14:54:19,776 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:54:19,776 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:54:21,077 - INFO - Feature scaling completed in 1.30s
2024-12-28 14:54:21,077 - INFO - Starting feature selection (k=50)
2024-12-28 14:54:21,102 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:54:21,102 - INFO - Starting anomaly detection
2024-12-28 14:54:29,309 - INFO - Anomaly detection completed in 8.21s
2024-12-28 14:54:29,309 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:54:29,310 - INFO - Total fit_transform time: 9.53s
2024-12-28 14:54:29,310 - INFO - Training set processing completed in 9.53s
2024-12-28 14:54:29,310 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 14:54:29,311 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 113.1 MB
2024-12-28 14:54:29,311 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:54:29,900 - INFO - Fitted scaler and transformed data
2024-12-28 14:54:29,900 - INFO - Scaling time: 0.59s
2024-12-28 14:54:29,913 - INFO - Number of unique classes: 43
2024-12-28 14:54:36,168 - INFO - Epoch 1/10, Train Loss: 3.5721, Val Loss: 3.7605
2024-12-28 14:54:43,589 - INFO - Epoch 2/10, Train Loss: 3.5715, Val Loss: 3.7599
2024-12-28 14:54:50,735 - INFO - Epoch 3/10, Train Loss: 3.5708, Val Loss: 3.7591
2024-12-28 14:54:56,993 - INFO - Epoch 4/10, Train Loss: 3.5700, Val Loss: 3.7584
2024-12-28 14:54:56,993 - INFO - Early stopping triggered at epoch 4
2024-12-28 14:54:56,993 - INFO - Training completed in 27.68s
2024-12-28 14:54:56,994 - INFO - Final memory usage: CPU 1927.1 MB, GPU 154.2 MB
2024-12-28 14:54:56,994 - INFO - Model training completed in 27.68s
2024-12-28 14:54:57,258 - INFO - Prediction completed in 0.26s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 14:54:57,271 - INFO - Poison rate 0.2 completed in 37.50s
2024-12-28 14:54:57,274 - INFO - Loaded 91 existing results
2024-12-28 14:54:57,274 - INFO - Total results to save: 98
2024-12-28 14:54:57,275 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:54:57,280 - INFO - Saved 98 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:54:57,281 - INFO - Total evaluation time: 274.09s
2024-12-28 14:54:57,287 - INFO - 
Progress: 15.6% - Evaluating GTSRB with KNeighbors (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:54:57,456 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:54:57,457 - INFO - Dataset type: image
2024-12-28 14:54:57,457 - INFO - Sample size: 39209
2024-12-28 14:54:57,457 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:54:57,458 - INFO - Loading datasets...
2024-12-28 14:55:16,033 - INFO - Dataset loading completed in 18.57s
2024-12-28 14:55:16,033 - INFO - Extracting validation features...
2024-12-28 14:55:16,033 - INFO - Extracting features from 4435 samples...
2024-12-28 14:55:16,842 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:55:16,848 - INFO - Validation feature extraction completed in 0.81s
2024-12-28 14:55:16,848 - INFO - Extracting training features...
2024-12-28 14:55:16,849 - INFO - Extracting features from 19755 samples...
2024-12-28 14:55:19,637 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:55:19,652 - INFO - Training feature extraction completed in 2.80s
2024-12-28 14:55:19,652 - INFO - Creating model for classifier: KNeighbors
2024-12-28 14:55:19,652 - INFO - Using device: cuda
2024-12-28 14:55:19,652 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:55:19,653 - INFO - Training set processing completed in 0.00s
2024-12-28 14:55:19,653 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:55:19,654 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 104.0 MB
2024-12-28 14:55:19,654 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:55:20,235 - INFO - Fitted scaler and transformed data
2024-12-28 14:55:20,235 - INFO - Scaling time: 0.58s
2024-12-28 14:55:20,255 - INFO - Training completed in 0.60s
2024-12-28 14:55:20,256 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:20,256 - INFO - Model training completed in 0.60s
2024-12-28 14:55:20,593 - INFO - Prediction completed in 0.34s
2024-12-28 14:55:20,604 - INFO - Poison rate 0.0 completed in 0.95s
2024-12-28 14:55:20,604 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:55:20,606 - INFO - Total number of labels flipped: 197
2024-12-28 14:55:20,606 - INFO - Label flipping completed in 0.00s
2024-12-28 14:55:20,606 - INFO - Training set processing completed in 0.00s
2024-12-28 14:55:20,607 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:55:20,608 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:20,608 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:55:21,188 - INFO - Fitted scaler and transformed data
2024-12-28 14:55:21,188 - INFO - Scaling time: 0.58s
2024-12-28 14:55:21,199 - INFO - Training completed in 0.59s
2024-12-28 14:55:21,200 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:21,200 - INFO - Model training completed in 0.59s
2024-12-28 14:55:21,293 - INFO - Prediction completed in 0.09s
2024-12-28 14:55:21,304 - INFO - Poison rate 0.01 completed in 0.70s
2024-12-28 14:55:21,304 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:55:21,306 - INFO - Total number of labels flipped: 590
2024-12-28 14:55:21,306 - INFO - Label flipping completed in 0.00s
2024-12-28 14:55:21,306 - INFO - Training set processing completed in 0.00s
2024-12-28 14:55:21,306 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:55:21,307 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:21,307 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:55:21,823 - INFO - Fitted scaler and transformed data
2024-12-28 14:55:21,823 - INFO - Scaling time: 0.52s
2024-12-28 14:55:21,834 - INFO - Training completed in 0.53s
2024-12-28 14:55:21,834 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:21,835 - INFO - Model training completed in 0.53s
2024-12-28 14:55:21,937 - INFO - Prediction completed in 0.10s
2024-12-28 14:55:21,949 - INFO - Poison rate 0.03 completed in 0.65s
2024-12-28 14:55:21,949 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:55:21,951 - INFO - Total number of labels flipped: 981
2024-12-28 14:55:21,952 - INFO - Label flipping completed in 0.00s
2024-12-28 14:55:21,952 - INFO - Training set processing completed in 0.00s
2024-12-28 14:55:21,952 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:55:21,952 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:21,952 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:55:22,479 - INFO - Fitted scaler and transformed data
2024-12-28 14:55:22,480 - INFO - Scaling time: 0.53s
2024-12-28 14:55:22,490 - INFO - Training completed in 0.54s
2024-12-28 14:55:22,491 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:22,491 - INFO - Model training completed in 0.54s
2024-12-28 14:55:22,620 - INFO - Prediction completed in 0.13s
2024-12-28 14:55:22,633 - INFO - Poison rate 0.05 completed in 0.68s
2024-12-28 14:55:22,633 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:55:22,635 - INFO - Total number of labels flipped: 1372
2024-12-28 14:55:22,636 - INFO - Label flipping completed in 0.00s
2024-12-28 14:55:22,636 - INFO - Training set processing completed in 0.00s
2024-12-28 14:55:22,636 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:55:22,637 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:22,637 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:55:23,166 - INFO - Fitted scaler and transformed data
2024-12-28 14:55:23,166 - INFO - Scaling time: 0.53s
2024-12-28 14:55:23,177 - INFO - Training completed in 0.54s
2024-12-28 14:55:23,177 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:23,178 - INFO - Model training completed in 0.54s
2024-12-28 14:55:23,325 - INFO - Prediction completed in 0.15s
2024-12-28 14:55:23,336 - INFO - Poison rate 0.07 completed in 0.70s
2024-12-28 14:55:23,336 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:55:23,338 - INFO - Total number of labels flipped: 1963
2024-12-28 14:55:23,339 - INFO - Label flipping completed in 0.00s
2024-12-28 14:55:23,339 - INFO - Training set processing completed in 0.00s
2024-12-28 14:55:23,339 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:55:23,339 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:23,339 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:55:23,856 - INFO - Fitted scaler and transformed data
2024-12-28 14:55:23,856 - INFO - Scaling time: 0.52s
2024-12-28 14:55:23,867 - INFO - Training completed in 0.53s
2024-12-28 14:55:23,868 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:23,868 - INFO - Model training completed in 0.53s
2024-12-28 14:55:23,989 - INFO - Prediction completed in 0.12s
2024-12-28 14:55:24,000 - INFO - Poison rate 0.1 completed in 0.66s
2024-12-28 14:55:24,000 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:55:24,004 - INFO - Total number of labels flipped: 3926
2024-12-28 14:55:24,004 - INFO - Label flipping completed in 0.00s
2024-12-28 14:55:24,004 - INFO - Training set processing completed in 0.00s
2024-12-28 14:55:24,004 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:55:24,005 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:24,005 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:55:24,566 - INFO - Fitted scaler and transformed data
2024-12-28 14:55:24,566 - INFO - Scaling time: 0.56s
2024-12-28 14:55:24,577 - INFO - Training completed in 0.57s
2024-12-28 14:55:24,578 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.7 MB
2024-12-28 14:55:24,578 - INFO - Model training completed in 0.57s
2024-12-28 14:55:24,712 - INFO - Prediction completed in 0.13s
2024-12-28 14:55:24,724 - INFO - Poison rate 0.2 completed in 0.72s
2024-12-28 14:55:24,727 - INFO - Loaded 98 existing results
2024-12-28 14:55:24,727 - INFO - Total results to save: 105
2024-12-28 14:55:24,728 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:55:24,735 - INFO - Saved 105 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:55:24,735 - INFO - Total evaluation time: 27.28s
2024-12-28 14:55:24,742 - INFO - 
Progress: 16.7% - Evaluating GTSRB with KNeighbors (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:55:24,911 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:55:24,911 - INFO - Dataset type: image
2024-12-28 14:55:24,911 - INFO - Sample size: 39209
2024-12-28 14:55:24,911 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:55:24,912 - INFO - Loading datasets...
2024-12-28 14:55:43,955 - INFO - Dataset loading completed in 19.04s
2024-12-28 14:55:43,955 - INFO - Extracting validation features...
2024-12-28 14:55:43,955 - INFO - Extracting features from 4435 samples...
2024-12-28 14:55:44,677 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:55:44,679 - INFO - Validation feature extraction completed in 0.72s
2024-12-28 14:55:44,680 - INFO - Extracting training features...
2024-12-28 14:55:44,680 - INFO - Extracting features from 19755 samples...
2024-12-28 14:55:47,428 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:55:47,439 - INFO - Training feature extraction completed in 2.76s
2024-12-28 14:55:47,439 - INFO - Creating model for classifier: KNeighbors
2024-12-28 14:55:47,440 - INFO - Using device: cuda
2024-12-28 14:55:47,440 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:55:47,441 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:55:47,441 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:55:48,606 - INFO - Feature scaling completed in 1.17s
2024-12-28 14:55:48,607 - INFO - Starting feature selection (k=50)
2024-12-28 14:55:48,634 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 14:55:48,634 - INFO - Starting anomaly detection
2024-12-28 14:55:56,771 - INFO - Anomaly detection completed in 8.14s
2024-12-28 14:55:56,771 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:55:56,772 - INFO - Total fit_transform time: 9.33s
2024-12-28 14:55:56,772 - INFO - Training set processing completed in 9.33s
2024-12-28 14:55:56,772 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:55:56,773 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 104.0 MB
2024-12-28 14:55:56,773 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:55:57,278 - INFO - Fitted scaler and transformed data
2024-12-28 14:55:57,278 - INFO - Scaling time: 0.51s
2024-12-28 14:55:57,291 - INFO - Training completed in 0.52s
2024-12-28 14:55:57,291 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:55:57,292 - INFO - Model training completed in 0.52s
2024-12-28 14:55:57,604 - INFO - Prediction completed in 0.31s
2024-12-28 14:55:57,626 - INFO - Poison rate 0.0 completed in 10.19s
2024-12-28 14:55:57,626 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:55:57,628 - INFO - Total number of labels flipped: 196
2024-12-28 14:55:57,628 - INFO - Label flipping completed in 0.00s
2024-12-28 14:55:57,629 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:55:57,629 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:55:58,878 - INFO - Feature scaling completed in 1.25s
2024-12-28 14:55:58,878 - INFO - Starting feature selection (k=50)
2024-12-28 14:55:58,890 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 14:55:58,891 - INFO - Starting anomaly detection
2024-12-28 14:56:06,682 - INFO - Anomaly detection completed in 7.79s
2024-12-28 14:56:06,683 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:56:06,683 - INFO - Total fit_transform time: 9.05s
2024-12-28 14:56:06,683 - INFO - Training set processing completed in 9.05s
2024-12-28 14:56:06,683 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:56:06,684 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:06,685 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:56:07,200 - INFO - Fitted scaler and transformed data
2024-12-28 14:56:07,200 - INFO - Scaling time: 0.52s
2024-12-28 14:56:07,213 - INFO - Training completed in 0.53s
2024-12-28 14:56:07,214 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:07,214 - INFO - Model training completed in 0.53s
2024-12-28 14:56:07,535 - INFO - Prediction completed in 0.32s
2024-12-28 14:56:07,552 - INFO - Poison rate 0.01 completed in 9.93s
2024-12-28 14:56:07,552 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:56:07,554 - INFO - Total number of labels flipped: 586
2024-12-28 14:56:07,554 - INFO - Label flipping completed in 0.00s
2024-12-28 14:56:07,554 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:56:07,555 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:56:08,777 - INFO - Feature scaling completed in 1.22s
2024-12-28 14:56:08,777 - INFO - Starting feature selection (k=50)
2024-12-28 14:56:08,792 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 14:56:08,792 - INFO - Starting anomaly detection
2024-12-28 14:56:16,599 - INFO - Anomaly detection completed in 7.81s
2024-12-28 14:56:16,599 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:56:16,599 - INFO - Total fit_transform time: 9.04s
2024-12-28 14:56:16,599 - INFO - Training set processing completed in 9.04s
2024-12-28 14:56:16,599 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:56:16,600 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:16,600 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:56:17,130 - INFO - Fitted scaler and transformed data
2024-12-28 14:56:17,130 - INFO - Scaling time: 0.53s
2024-12-28 14:56:17,143 - INFO - Training completed in 0.54s
2024-12-28 14:56:17,143 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:17,144 - INFO - Model training completed in 0.54s
2024-12-28 14:56:17,477 - INFO - Prediction completed in 0.33s
2024-12-28 14:56:17,488 - INFO - Poison rate 0.03 completed in 9.94s
2024-12-28 14:56:17,488 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:56:17,490 - INFO - Total number of labels flipped: 981
2024-12-28 14:56:17,490 - INFO - Label flipping completed in 0.00s
2024-12-28 14:56:17,490 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:56:17,490 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:56:18,657 - INFO - Feature scaling completed in 1.17s
2024-12-28 14:56:18,657 - INFO - Starting feature selection (k=50)
2024-12-28 14:56:18,670 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 14:56:18,670 - INFO - Starting anomaly detection
2024-12-28 14:56:26,724 - INFO - Anomaly detection completed in 8.05s
2024-12-28 14:56:26,724 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:56:26,725 - INFO - Total fit_transform time: 9.23s
2024-12-28 14:56:26,725 - INFO - Training set processing completed in 9.23s
2024-12-28 14:56:26,725 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:56:26,726 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:26,726 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:56:27,261 - INFO - Fitted scaler and transformed data
2024-12-28 14:56:27,261 - INFO - Scaling time: 0.53s
2024-12-28 14:56:27,274 - INFO - Training completed in 0.55s
2024-12-28 14:56:27,274 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:27,274 - INFO - Model training completed in 0.55s
2024-12-28 14:56:27,549 - INFO - Prediction completed in 0.27s
2024-12-28 14:56:27,560 - INFO - Poison rate 0.05 completed in 10.07s
2024-12-28 14:56:27,560 - INFO - 
Processing poison rate: 0.07
2024-12-28 14:56:27,562 - INFO - Total number of labels flipped: 1374
2024-12-28 14:56:27,562 - INFO - Label flipping completed in 0.00s
2024-12-28 14:56:27,562 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:56:27,562 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:56:28,750 - INFO - Feature scaling completed in 1.19s
2024-12-28 14:56:28,750 - INFO - Starting feature selection (k=50)
2024-12-28 14:56:28,765 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 14:56:28,765 - INFO - Starting anomaly detection
2024-12-28 14:56:36,515 - INFO - Anomaly detection completed in 7.75s
2024-12-28 14:56:36,515 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:56:36,515 - INFO - Total fit_transform time: 8.95s
2024-12-28 14:56:36,515 - INFO - Training set processing completed in 8.95s
2024-12-28 14:56:36,516 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:56:36,516 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:36,517 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:56:37,122 - INFO - Fitted scaler and transformed data
2024-12-28 14:56:37,122 - INFO - Scaling time: 0.61s
2024-12-28 14:56:37,138 - INFO - Training completed in 0.62s
2024-12-28 14:56:37,139 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:37,139 - INFO - Model training completed in 0.62s
2024-12-28 14:56:37,467 - INFO - Prediction completed in 0.33s
2024-12-28 14:56:37,482 - INFO - Poison rate 0.07 completed in 9.92s
2024-12-28 14:56:37,482 - INFO - 
Processing poison rate: 0.1
2024-12-28 14:56:37,485 - INFO - Total number of labels flipped: 1965
2024-12-28 14:56:37,485 - INFO - Label flipping completed in 0.00s
2024-12-28 14:56:37,485 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:56:37,485 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:56:38,726 - INFO - Feature scaling completed in 1.24s
2024-12-28 14:56:38,727 - INFO - Starting feature selection (k=50)
2024-12-28 14:56:38,739 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 14:56:38,739 - INFO - Starting anomaly detection
2024-12-28 14:56:45,297 - INFO - Anomaly detection completed in 6.56s
2024-12-28 14:56:45,298 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:56:45,298 - INFO - Total fit_transform time: 7.81s
2024-12-28 14:56:45,298 - INFO - Training set processing completed in 7.81s
2024-12-28 14:56:45,298 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:56:45,299 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:45,299 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:56:45,830 - INFO - Fitted scaler and transformed data
2024-12-28 14:56:45,830 - INFO - Scaling time: 0.53s
2024-12-28 14:56:45,842 - INFO - Training completed in 0.54s
2024-12-28 14:56:45,843 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:45,843 - INFO - Model training completed in 0.55s
2024-12-28 14:56:46,140 - INFO - Prediction completed in 0.30s
2024-12-28 14:56:46,150 - INFO - Poison rate 0.1 completed in 8.67s
2024-12-28 14:56:46,151 - INFO - 
Processing poison rate: 0.2
2024-12-28 14:56:46,154 - INFO - Total number of labels flipped: 3930
2024-12-28 14:56:46,154 - INFO - Label flipping completed in 0.00s
2024-12-28 14:56:46,154 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 14:56:46,155 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 14:56:47,428 - INFO - Feature scaling completed in 1.27s
2024-12-28 14:56:47,428 - INFO - Starting feature selection (k=50)
2024-12-28 14:56:47,445 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 14:56:47,445 - INFO - Starting anomaly detection
2024-12-28 14:56:53,672 - INFO - Anomaly detection completed in 6.23s
2024-12-28 14:56:53,672 - INFO - Found 1976 outliers (10.0%)
2024-12-28 14:56:53,672 - INFO - Total fit_transform time: 7.52s
2024-12-28 14:56:53,672 - INFO - Training set processing completed in 7.52s
2024-12-28 14:56:53,673 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 14:56:53,673 - INFO - Memory usage at start_fit: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:53,673 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:56:54,209 - INFO - Fitted scaler and transformed data
2024-12-28 14:56:54,209 - INFO - Scaling time: 0.54s
2024-12-28 14:56:54,222 - INFO - Training completed in 0.55s
2024-12-28 14:56:54,222 - INFO - Final memory usage: CPU 1927.1 MB, GPU 142.8 MB
2024-12-28 14:56:54,223 - INFO - Model training completed in 0.55s
2024-12-28 14:56:54,532 - INFO - Prediction completed in 0.31s
2024-12-28 14:56:54,542 - INFO - Poison rate 0.2 completed in 8.39s
2024-12-28 14:56:54,546 - INFO - Loaded 105 existing results
2024-12-28 14:56:54,546 - INFO - Total results to save: 112
2024-12-28 14:56:54,547 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 14:56:54,554 - INFO - Saved 112 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 14:56:54,554 - INFO - Total evaluation time: 89.64s
2024-12-28 14:56:54,561 - INFO - Completed evaluation for GTSRB
2024-12-28 14:56:54,561 - INFO - 
Processing dataset: GTSRB
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 14:56:54,722 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:56:54,723 - INFO - Dataset type: image
2024-12-28 14:56:54,723 - INFO - Sample size: 39209
2024-12-28 14:56:54,723 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:56:54,723 - INFO - 
Progress: 17.7% - Evaluating GTSRB with SVM (standard mode, iteration 1/1)
2024-12-28 14:56:54,890 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 14:56:54,890 - INFO - Dataset type: image
2024-12-28 14:56:54,890 - INFO - Sample size: 39209
2024-12-28 14:56:54,890 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 14:56:54,891 - INFO - Loading datasets...
2024-12-28 14:57:13,965 - INFO - Dataset loading completed in 19.07s
2024-12-28 14:57:13,965 - INFO - Extracting validation features...
2024-12-28 14:57:13,965 - INFO - Extracting features from 4435 samples...
2024-12-28 14:57:14,689 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 14:57:14,692 - INFO - Validation feature extraction completed in 0.73s
2024-12-28 14:57:14,692 - INFO - Extracting training features...
2024-12-28 14:57:14,692 - INFO - Extracting features from 19755 samples...
2024-12-28 14:57:17,408 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 14:57:17,416 - INFO - Training feature extraction completed in 2.72s
2024-12-28 14:57:17,416 - INFO - Creating model for classifier: SVM
2024-12-28 14:57:17,416 - INFO - Using device: cuda
2024-12-28 14:57:17,416 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 14:57:17,416 - INFO - 
Processing poison rate: 0.0
2024-12-28 14:57:17,416 - INFO - Training set processing completed in 0.00s
2024-12-28 14:57:17,416 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:57:17,418 - INFO - Memory usage at start_fit: CPU 1866.8 MB, GPU 103.4 MB
2024-12-28 14:57:17,418 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:57:17,429 - INFO - Number of unique classes: 43
2024-12-28 14:57:17,595 - INFO - Fitted scaler and transformed data
2024-12-28 14:57:17,596 - INFO - Scaling time: 0.16s
2024-12-28 14:57:18,854 - INFO - Epoch 1/500, Train Loss: 6.9134, Val Loss: 3.1748
2024-12-28 14:57:19,997 - INFO - Epoch 2/500, Train Loss: 2.4856, Val Loss: 2.2164
2024-12-28 14:57:21,269 - INFO - Epoch 3/500, Train Loss: 1.8012, Val Loss: 1.8216
2024-12-28 14:57:22,524 - INFO - Epoch 4/500, Train Loss: 1.4511, Val Loss: 1.5859
2024-12-28 14:57:23,757 - INFO - Epoch 5/500, Train Loss: 1.2378, Val Loss: 1.4469
2024-12-28 14:57:24,866 - INFO - Epoch 6/500, Train Loss: 1.0906, Val Loss: 1.3161
2024-12-28 14:57:25,965 - INFO - Epoch 7/500, Train Loss: 0.9809, Val Loss: 1.2595
2024-12-28 14:57:27,095 - INFO - Epoch 8/500, Train Loss: 0.8965, Val Loss: 1.1966
2024-12-28 14:57:28,215 - INFO - Epoch 9/500, Train Loss: 0.8335, Val Loss: 1.1551
2024-12-28 14:57:29,278 - INFO - Epoch 10/500, Train Loss: 0.7773, Val Loss: 1.1171
2024-12-28 14:57:30,399 - INFO - Epoch 11/500, Train Loss: 0.7301, Val Loss: 1.0849
2024-12-28 14:57:31,583 - INFO - Epoch 12/500, Train Loss: 0.7015, Val Loss: 1.0458
2024-12-28 14:57:32,777 - INFO - Epoch 13/500, Train Loss: 0.6663, Val Loss: 1.0205
2024-12-28 14:57:33,776 - INFO - Epoch 14/500, Train Loss: 0.6326, Val Loss: 1.0051
2024-12-28 14:57:34,726 - INFO - Epoch 15/500, Train Loss: 0.6116, Val Loss: 0.9804
2024-12-28 14:57:35,652 - INFO - Epoch 16/500, Train Loss: 0.5917, Val Loss: 0.9872
2024-12-28 14:57:36,684 - INFO - Epoch 17/500, Train Loss: 0.5715, Val Loss: 0.9641
2024-12-28 14:57:37,722 - INFO - Epoch 18/500, Train Loss: 0.5614, Val Loss: 0.9273
2024-12-28 14:57:38,671 - INFO - Epoch 19/500, Train Loss: 0.5430, Val Loss: 0.9553
2024-12-28 14:57:39,704 - INFO - Epoch 20/500, Train Loss: 0.5339, Val Loss: 0.9344
2024-12-28 14:57:40,693 - INFO - Epoch 21/500, Train Loss: 0.5184, Val Loss: 0.9251
2024-12-28 14:57:41,872 - INFO - Epoch 22/500, Train Loss: 0.5151, Val Loss: 0.9031
2024-12-28 14:57:43,097 - INFO - Epoch 23/500, Train Loss: 0.5030, Val Loss: 0.8933
2024-12-28 14:57:44,360 - INFO - Epoch 24/500, Train Loss: 0.5031, Val Loss: 0.9008
2024-12-28 14:57:45,582 - INFO - Epoch 25/500, Train Loss: 0.4944, Val Loss: 0.9012
2024-12-28 14:57:46,798 - INFO - Epoch 26/500, Train Loss: 0.4830, Val Loss: 0.9147
2024-12-28 14:57:47,964 - INFO - Epoch 27/500, Train Loss: 0.4808, Val Loss: 0.9201
2024-12-28 14:57:49,188 - INFO - Epoch 28/500, Train Loss: 0.4795, Val Loss: 0.8785
2024-12-28 14:57:50,387 - INFO - Epoch 29/500, Train Loss: 0.4759, Val Loss: 0.8621
2024-12-28 14:57:51,601 - INFO - Epoch 30/500, Train Loss: 0.4693, Val Loss: 0.8788
2024-12-28 14:57:52,658 - INFO - Epoch 31/500, Train Loss: 0.4658, Val Loss: 0.9105
2024-12-28 14:57:53,750 - INFO - Epoch 32/500, Train Loss: 0.4681, Val Loss: 0.9063
2024-12-28 14:57:54,892 - INFO - Epoch 33/500, Train Loss: 0.4616, Val Loss: 0.8693
2024-12-28 14:57:55,994 - INFO - Epoch 34/500, Train Loss: 0.4528, Val Loss: 0.8935
2024-12-28 14:57:55,994 - INFO - Early stopping triggered at epoch 34
2024-12-28 14:57:55,994 - INFO - Training completed in 38.58s
2024-12-28 14:57:55,995 - INFO - Final memory usage: CPU 1914.9 MB, GPU 103.8 MB
2024-12-28 14:57:55,995 - INFO - Model training completed in 38.58s
2024-12-28 14:57:56,060 - INFO - Prediction completed in 0.06s
2024-12-28 14:57:56,071 - INFO - Poison rate 0.0 completed in 38.66s
2024-12-28 14:57:56,072 - INFO - 
Processing poison rate: 0.01
2024-12-28 14:57:56,073 - INFO - Label flipping details:
2024-12-28 14:57:56,073 - INFO - - Source class: 1
2024-12-28 14:57:56,073 - INFO - - Target class: 0
2024-12-28 14:57:56,073 - INFO - - Available samples in source class: 922
2024-12-28 14:57:56,073 - INFO - - Requested samples to poison: 197
2024-12-28 14:57:56,073 - INFO - - Actual samples to flip: 197
2024-12-28 14:57:56,073 - INFO - - Samples remaining in source class: 725
2024-12-28 14:57:56,073 - INFO - Successfully flipped 197 labels from class 1 to 0
2024-12-28 14:57:56,074 - INFO - Total number of labels flipped: 197
2024-12-28 14:57:56,074 - INFO - Label flipping completed in 0.00s
2024-12-28 14:57:56,074 - INFO - Training set processing completed in 0.00s
2024-12-28 14:57:56,074 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:57:56,075 - INFO - Memory usage at start_fit: CPU 1876.3 MB, GPU 103.7 MB
2024-12-28 14:57:56,075 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:57:56,082 - INFO - Number of unique classes: 43
2024-12-28 14:57:56,233 - INFO - Fitted scaler and transformed data
2024-12-28 14:57:56,233 - INFO - Scaling time: 0.15s
2024-12-28 14:57:57,468 - INFO - Epoch 1/500, Train Loss: 6.8559, Val Loss: 3.2052
2024-12-28 14:57:58,687 - INFO - Epoch 2/500, Train Loss: 2.5288, Val Loss: 2.3042
2024-12-28 14:57:59,898 - INFO - Epoch 3/500, Train Loss: 1.8464, Val Loss: 1.9125
2024-12-28 14:58:01,117 - INFO - Epoch 4/500, Train Loss: 1.4901, Val Loss: 1.6962
2024-12-28 14:58:02,255 - INFO - Epoch 5/500, Train Loss: 1.2735, Val Loss: 1.5635
2024-12-28 14:58:03,433 - INFO - Epoch 6/500, Train Loss: 1.1342, Val Loss: 1.4437
2024-12-28 14:58:04,695 - INFO - Epoch 7/500, Train Loss: 1.0293, Val Loss: 1.3819
2024-12-28 14:58:05,868 - INFO - Epoch 8/500, Train Loss: 0.9372, Val Loss: 1.3522
2024-12-28 14:58:07,172 - INFO - Epoch 9/500, Train Loss: 0.8745, Val Loss: 1.2507
2024-12-28 14:58:08,453 - INFO - Epoch 10/500, Train Loss: 0.8152, Val Loss: 1.2407
2024-12-28 14:58:09,603 - INFO - Epoch 11/500, Train Loss: 0.7738, Val Loss: 1.1945
2024-12-28 14:58:10,661 - INFO - Epoch 12/500, Train Loss: 0.7373, Val Loss: 1.1648
2024-12-28 14:58:11,878 - INFO - Epoch 13/500, Train Loss: 0.7027, Val Loss: 1.1334
2024-12-28 14:58:13,005 - INFO - Epoch 14/500, Train Loss: 0.6743, Val Loss: 1.1090
2024-12-28 14:58:14,125 - INFO - Epoch 15/500, Train Loss: 0.6536, Val Loss: 1.1145
2024-12-28 14:58:15,231 - INFO - Epoch 16/500, Train Loss: 0.6352, Val Loss: 1.1020
2024-12-28 14:58:16,317 - INFO - Epoch 17/500, Train Loss: 0.6136, Val Loss: 1.0752
2024-12-28 14:58:17,436 - INFO - Epoch 18/500, Train Loss: 0.5997, Val Loss: 1.0697
2024-12-28 14:58:18,637 - INFO - Epoch 19/500, Train Loss: 0.5835, Val Loss: 1.0259
2024-12-28 14:58:19,888 - INFO - Epoch 20/500, Train Loss: 0.5728, Val Loss: 1.0268
2024-12-28 14:58:21,137 - INFO - Epoch 21/500, Train Loss: 0.5624, Val Loss: 1.0068
2024-12-28 14:58:22,385 - INFO - Epoch 22/500, Train Loss: 0.5544, Val Loss: 1.0129
2024-12-28 14:58:23,589 - INFO - Epoch 23/500, Train Loss: 0.5460, Val Loss: 1.0254
2024-12-28 14:58:24,824 - INFO - Epoch 24/500, Train Loss: 0.5363, Val Loss: 1.0050
2024-12-28 14:58:26,005 - INFO - Epoch 25/500, Train Loss: 0.5396, Val Loss: 0.9996
2024-12-28 14:58:27,307 - INFO - Epoch 26/500, Train Loss: 0.5298, Val Loss: 1.0024
2024-12-28 14:58:28,549 - INFO - Epoch 27/500, Train Loss: 0.5261, Val Loss: 1.0057
2024-12-28 14:58:29,781 - INFO - Epoch 28/500, Train Loss: 0.5173, Val Loss: 1.0208
2024-12-28 14:58:30,967 - INFO - Epoch 29/500, Train Loss: 0.5105, Val Loss: 0.9900
2024-12-28 14:58:32,109 - INFO - Epoch 30/500, Train Loss: 0.5108, Val Loss: 0.9998
2024-12-28 14:58:33,238 - INFO - Epoch 31/500, Train Loss: 0.5058, Val Loss: 1.0028
2024-12-28 14:58:34,375 - INFO - Epoch 32/500, Train Loss: 0.5020, Val Loss: 0.9768
2024-12-28 14:58:35,467 - INFO - Epoch 33/500, Train Loss: 0.5017, Val Loss: 0.9772
2024-12-28 14:58:36,578 - INFO - Epoch 34/500, Train Loss: 0.4899, Val Loss: 0.9846
2024-12-28 14:58:37,792 - INFO - Epoch 35/500, Train Loss: 0.4974, Val Loss: 0.9430
2024-12-28 14:58:38,990 - INFO - Epoch 36/500, Train Loss: 0.4961, Val Loss: 0.9877
2024-12-28 14:58:40,258 - INFO - Epoch 37/500, Train Loss: 0.4956, Val Loss: 0.9503
2024-12-28 14:58:41,333 - INFO - Epoch 38/500, Train Loss: 0.4868, Val Loss: 1.0042
2024-12-28 14:58:42,388 - INFO - Epoch 39/500, Train Loss: 0.4838, Val Loss: 0.9667
2024-12-28 14:58:43,472 - INFO - Epoch 40/500, Train Loss: 0.4891, Val Loss: 0.9743
2024-12-28 14:58:43,472 - INFO - Early stopping triggered at epoch 40
2024-12-28 14:58:43,472 - INFO - Training completed in 47.40s
2024-12-28 14:58:43,473 - INFO - Final memory usage: CPU 1914.9 MB, GPU 103.8 MB
2024-12-28 14:58:43,473 - INFO - Model training completed in 47.40s
2024-12-28 14:58:43,535 - INFO - Prediction completed in 0.06s
2024-12-28 14:58:43,546 - INFO - Poison rate 0.01 completed in 47.47s
2024-12-28 14:58:43,546 - INFO - 
Processing poison rate: 0.03
2024-12-28 14:58:43,548 - INFO - Label flipping details:
2024-12-28 14:58:43,548 - INFO - - Source class: 1
2024-12-28 14:58:43,548 - INFO - - Target class: 0
2024-12-28 14:58:43,548 - INFO - - Available samples in source class: 922
2024-12-28 14:58:43,548 - INFO - - Requested samples to poison: 592
2024-12-28 14:58:43,548 - INFO - - Actual samples to flip: 592
2024-12-28 14:58:43,548 - INFO - - Samples remaining in source class: 330
2024-12-28 14:58:43,548 - INFO - Successfully flipped 592 labels from class 1 to 0
2024-12-28 14:58:43,548 - INFO - Total number of labels flipped: 592
2024-12-28 14:58:43,548 - INFO - Label flipping completed in 0.00s
2024-12-28 14:58:43,549 - INFO - Training set processing completed in 0.00s
2024-12-28 14:58:43,549 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:58:43,549 - INFO - Memory usage at start_fit: CPU 1876.3 MB, GPU 103.7 MB
2024-12-28 14:58:43,550 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:58:43,557 - INFO - Number of unique classes: 43
2024-12-28 14:58:43,720 - INFO - Fitted scaler and transformed data
2024-12-28 14:58:43,720 - INFO - Scaling time: 0.16s
2024-12-28 14:58:44,919 - INFO - Epoch 1/500, Train Loss: 7.0920, Val Loss: 3.3760
2024-12-28 14:58:46,164 - INFO - Epoch 2/500, Train Loss: 2.6080, Val Loss: 2.4816
2024-12-28 14:58:47,361 - INFO - Epoch 3/500, Train Loss: 1.8991, Val Loss: 2.0017
2024-12-28 14:58:48,607 - INFO - Epoch 4/500, Train Loss: 1.5482, Val Loss: 1.7955
2024-12-28 14:58:49,864 - INFO - Epoch 5/500, Train Loss: 1.3307, Val Loss: 1.6393
2024-12-28 14:58:51,031 - INFO - Epoch 6/500, Train Loss: 1.1761, Val Loss: 1.5408
2024-12-28 14:58:52,110 - INFO - Epoch 7/500, Train Loss: 1.0608, Val Loss: 1.4295
2024-12-28 14:58:53,201 - INFO - Epoch 8/500, Train Loss: 0.9776, Val Loss: 1.3636
2024-12-28 14:58:54,343 - INFO - Epoch 9/500, Train Loss: 0.9058, Val Loss: 1.3006
2024-12-28 14:58:55,553 - INFO - Epoch 10/500, Train Loss: 0.8498, Val Loss: 1.2755
2024-12-28 14:58:56,744 - INFO - Epoch 11/500, Train Loss: 0.8031, Val Loss: 1.2180
2024-12-28 14:58:57,946 - INFO - Epoch 12/500, Train Loss: 0.7616, Val Loss: 1.1881
2024-12-28 14:58:59,185 - INFO - Epoch 13/500, Train Loss: 0.7294, Val Loss: 1.1682
2024-12-28 14:59:00,423 - INFO - Epoch 14/500, Train Loss: 0.6992, Val Loss: 1.1451
2024-12-28 14:59:01,724 - INFO - Epoch 15/500, Train Loss: 0.6746, Val Loss: 1.1143
2024-12-28 14:59:02,840 - INFO - Epoch 16/500, Train Loss: 0.6573, Val Loss: 1.0976
2024-12-28 14:59:03,940 - INFO - Epoch 17/500, Train Loss: 0.6384, Val Loss: 1.0887
2024-12-28 14:59:05,112 - INFO - Epoch 18/500, Train Loss: 0.6250, Val Loss: 1.0755
2024-12-28 14:59:06,300 - INFO - Epoch 19/500, Train Loss: 0.6063, Val Loss: 1.0570
2024-12-28 14:59:07,518 - INFO - Epoch 20/500, Train Loss: 0.5988, Val Loss: 1.0486
2024-12-28 14:59:08,757 - INFO - Epoch 21/500, Train Loss: 0.5767, Val Loss: 1.0422
2024-12-28 14:59:10,010 - INFO - Epoch 22/500, Train Loss: 0.5735, Val Loss: 1.0595
2024-12-28 14:59:11,122 - INFO - Epoch 23/500, Train Loss: 0.5612, Val Loss: 1.0034
2024-12-28 14:59:12,185 - INFO - Epoch 24/500, Train Loss: 0.5527, Val Loss: 1.0167
2024-12-28 14:59:13,427 - INFO - Epoch 25/500, Train Loss: 0.5523, Val Loss: 1.0192
2024-12-28 14:59:14,603 - INFO - Epoch 26/500, Train Loss: 0.5420, Val Loss: 0.9992
2024-12-28 14:59:15,784 - INFO - Epoch 27/500, Train Loss: 0.5420, Val Loss: 0.9792
2024-12-28 14:59:17,042 - INFO - Epoch 28/500, Train Loss: 0.5304, Val Loss: 1.0126
2024-12-28 14:59:18,204 - INFO - Epoch 29/500, Train Loss: 0.5300, Val Loss: 0.9857
2024-12-28 14:59:19,334 - INFO - Epoch 30/500, Train Loss: 0.5231, Val Loss: 0.9780
2024-12-28 14:59:20,430 - INFO - Epoch 31/500, Train Loss: 0.5196, Val Loss: 0.9930
2024-12-28 14:59:21,543 - INFO - Epoch 32/500, Train Loss: 0.5172, Val Loss: 0.9880
2024-12-28 14:59:22,598 - INFO - Epoch 33/500, Train Loss: 0.5181, Val Loss: 0.9896
2024-12-28 14:59:23,615 - INFO - Epoch 34/500, Train Loss: 0.5094, Val Loss: 0.9787
2024-12-28 14:59:24,691 - INFO - Epoch 35/500, Train Loss: 0.5019, Val Loss: 0.9645
2024-12-28 14:59:25,803 - INFO - Epoch 36/500, Train Loss: 0.5055, Val Loss: 0.9577
2024-12-28 14:59:26,827 - INFO - Epoch 37/500, Train Loss: 0.5025, Val Loss: 0.9795
2024-12-28 14:59:27,883 - INFO - Epoch 38/500, Train Loss: 0.5062, Val Loss: 0.9786
2024-12-28 14:59:28,993 - INFO - Epoch 39/500, Train Loss: 0.5017, Val Loss: 0.9924
2024-12-28 14:59:30,193 - INFO - Epoch 40/500, Train Loss: 0.4989, Val Loss: 0.9705
2024-12-28 14:59:31,408 - INFO - Epoch 41/500, Train Loss: 0.4974, Val Loss: 0.9794
2024-12-28 14:59:31,408 - INFO - Early stopping triggered at epoch 41
2024-12-28 14:59:31,408 - INFO - Training completed in 47.86s
2024-12-28 14:59:31,409 - INFO - Final memory usage: CPU 1914.9 MB, GPU 103.8 MB
2024-12-28 14:59:31,410 - INFO - Model training completed in 47.86s
2024-12-28 14:59:31,491 - INFO - Prediction completed in 0.08s
2024-12-28 14:59:31,502 - INFO - Poison rate 0.03 completed in 47.96s
2024-12-28 14:59:31,502 - INFO - 
Processing poison rate: 0.05
2024-12-28 14:59:31,503 - INFO - Label flipping details:
2024-12-28 14:59:31,504 - INFO - - Source class: 1
2024-12-28 14:59:31,504 - INFO - - Target class: 0
2024-12-28 14:59:31,504 - INFO - - Available samples in source class: 922
2024-12-28 14:59:31,504 - INFO - - Requested samples to poison: 987
2024-12-28 14:59:31,504 - INFO - - Actual samples to flip: 921
2024-12-28 14:59:31,504 - INFO - - Samples remaining in source class: 1
2024-12-28 14:59:31,504 - INFO - Successfully flipped 921 labels from class 1 to 0
2024-12-28 14:59:31,504 - INFO - Total number of labels flipped: 921
2024-12-28 14:59:31,504 - INFO - Label flipping completed in 0.00s
2024-12-28 14:59:31,504 - INFO - Training set processing completed in 0.00s
2024-12-28 14:59:31,504 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 14:59:31,505 - INFO - Memory usage at start_fit: CPU 1876.3 MB, GPU 103.7 MB
2024-12-28 14:59:31,506 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 14:59:31,516 - INFO - Number of unique classes: 43
2024-12-28 14:59:31,660 - INFO - Fitted scaler and transformed data
2024-12-28 14:59:31,660 - INFO - Scaling time: 0.14s
2024-12-28 14:59:32,870 - INFO - Epoch 1/500, Train Loss: 6.5388, Val Loss: 3.0669
2024-12-28 14:59:34,076 - INFO - Epoch 2/500, Train Loss: 2.4225, Val Loss: 2.1482
2024-12-28 14:59:35,220 - INFO - Epoch 3/500, Train Loss: 1.7568, Val Loss: 1.7599
2024-12-28 14:59:36,333 - INFO - Epoch 4/500, Train Loss: 1.4189, Val Loss: 1.5656
2024-12-28 14:59:37,478 - INFO - Epoch 5/500, Train Loss: 1.2198, Val Loss: 1.3925
2024-12-28 14:59:38,673 - INFO - Epoch 6/500, Train Loss: 1.0754, Val Loss: 1.3108
2024-12-28 14:59:39,800 - INFO - Epoch 7/500, Train Loss: 0.9729, Val Loss: 1.2353
2024-12-28 14:59:40,889 - INFO - Epoch 8/500, Train Loss: 0.8913, Val Loss: 1.1571
2024-12-28 14:59:42,006 - INFO - Epoch 9/500, Train Loss: 0.8301, Val Loss: 1.0942
2024-12-28 14:59:43,092 - INFO - Epoch 10/500, Train Loss: 0.7719, Val Loss: 1.0612
2024-12-28 14:59:44,227 - INFO - Epoch 11/500, Train Loss: 0.7300, Val Loss: 1.0077
2024-12-28 14:59:45,393 - INFO - Epoch 12/500, Train Loss: 0.6933, Val Loss: 0.9932
2024-12-28 14:59:46,546 - INFO - Epoch 13/500, Train Loss: 0.6621, Val Loss: 0.9876
2024-12-28 14:59:47,751 - INFO - Epoch 14/500, Train Loss: 0.6356, Val Loss: 0.9793
2024-12-28 14:59:49,010 - INFO - Epoch 15/500, Train Loss: 0.6166, Val Loss: 0.9143
2024-12-28 14:59:50,281 - INFO - Epoch 16/500, Train Loss: 0.5923, Val Loss: 0.9012
2024-12-28 14:59:51,449 - INFO - Epoch 17/500, Train Loss: 0.5768, Val Loss: 0.9058
2024-12-28 14:59:52,614 - INFO - Epoch 18/500, Train Loss: 0.5610, Val Loss: 0.9158
2024-12-28 14:59:53,887 - INFO - Epoch 19/500, Train Loss: 0.5463, Val Loss: 0.8712
2024-12-28 14:59:55,084 - INFO - Epoch 20/500, Train Loss: 0.5394, Val Loss: 0.8904
2024-12-28 14:59:56,229 - INFO - Epoch 21/500, Train Loss: 0.5242, Val Loss: 0.8747
2024-12-28 14:59:57,410 - INFO - Epoch 22/500, Train Loss: 0.5139, Val Loss: 0.8683
2024-12-28 14:59:58,514 - INFO - Epoch 23/500, Train Loss: 0.5064, Val Loss: 0.8549
2024-12-28 14:59:59,538 - INFO - Epoch 24/500, Train Loss: 0.4995, Val Loss: 0.8375
2024-12-28 15:00:00,641 - INFO - Epoch 25/500, Train Loss: 0.4955, Val Loss: 0.8486
2024-12-28 15:00:01,729 - INFO - Epoch 26/500, Train Loss: 0.4842, Val Loss: 0.8451
2024-12-28 15:00:02,756 - INFO - Epoch 27/500, Train Loss: 0.4866, Val Loss: 0.8296
2024-12-28 15:00:03,817 - INFO - Epoch 28/500, Train Loss: 0.4787, Val Loss: 0.8477
2024-12-28 15:00:04,972 - INFO - Epoch 29/500, Train Loss: 0.4725, Val Loss: 0.8602
2024-12-28 15:00:06,063 - INFO - Epoch 30/500, Train Loss: 0.4691, Val Loss: 0.8560
2024-12-28 15:00:07,236 - INFO - Epoch 31/500, Train Loss: 0.4680, Val Loss: 0.8451
2024-12-28 15:00:08,393 - INFO - Epoch 32/500, Train Loss: 0.4663, Val Loss: 0.8665
2024-12-28 15:00:08,393 - INFO - Early stopping triggered at epoch 32
2024-12-28 15:00:08,393 - INFO - Training completed in 36.89s
2024-12-28 15:00:08,393 - INFO - Final memory usage: CPU 1914.9 MB, GPU 103.8 MB
2024-12-28 15:00:08,394 - INFO - Model training completed in 36.89s
2024-12-28 15:00:08,456 - INFO - Prediction completed in 0.06s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:00:08,467 - INFO - Poison rate 0.05 completed in 36.96s
2024-12-28 15:00:08,467 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:00:08,468 - INFO - Label flipping details:
2024-12-28 15:00:08,468 - INFO - - Source class: 1
2024-12-28 15:00:08,469 - INFO - - Target class: 0
2024-12-28 15:00:08,469 - INFO - - Available samples in source class: 922
2024-12-28 15:00:08,469 - INFO - - Requested samples to poison: 1382
2024-12-28 15:00:08,469 - INFO - - Actual samples to flip: 921
2024-12-28 15:00:08,469 - INFO - - Samples remaining in source class: 1
2024-12-28 15:00:08,469 - INFO - Successfully flipped 921 labels from class 1 to 0
2024-12-28 15:00:08,469 - INFO - Total number of labels flipped: 921
2024-12-28 15:00:08,469 - INFO - Label flipping completed in 0.00s
2024-12-28 15:00:08,469 - INFO - Training set processing completed in 0.00s
2024-12-28 15:00:08,469 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:00:08,470 - INFO - Memory usage at start_fit: CPU 1876.3 MB, GPU 103.7 MB
2024-12-28 15:00:08,470 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:00:08,477 - INFO - Number of unique classes: 43
2024-12-28 15:00:08,636 - INFO - Fitted scaler and transformed data
2024-12-28 15:00:08,636 - INFO - Scaling time: 0.16s
2024-12-28 15:00:09,933 - INFO - Epoch 1/500, Train Loss: 6.7870, Val Loss: 3.1381
2024-12-28 15:00:11,134 - INFO - Epoch 2/500, Train Loss: 2.4706, Val Loss: 2.2016
2024-12-28 15:00:12,314 - INFO - Epoch 3/500, Train Loss: 1.7957, Val Loss: 1.7973
2024-12-28 15:00:13,427 - INFO - Epoch 4/500, Train Loss: 1.4549, Val Loss: 1.5261
2024-12-28 15:00:14,636 - INFO - Epoch 5/500, Train Loss: 1.2389, Val Loss: 1.3863
2024-12-28 15:00:15,862 - INFO - Epoch 6/500, Train Loss: 1.0945, Val Loss: 1.3121
2024-12-28 15:00:17,085 - INFO - Epoch 7/500, Train Loss: 0.9819, Val Loss: 1.1978
2024-12-28 15:00:18,302 - INFO - Epoch 8/500, Train Loss: 0.9057, Val Loss: 1.1314
2024-12-28 15:00:19,512 - INFO - Epoch 9/500, Train Loss: 0.8375, Val Loss: 1.0843
2024-12-28 15:00:20,741 - INFO - Epoch 10/500, Train Loss: 0.7761, Val Loss: 1.0506
2024-12-28 15:00:21,939 - INFO - Epoch 11/500, Train Loss: 0.7394, Val Loss: 1.0201
2024-12-28 15:00:23,158 - INFO - Epoch 12/500, Train Loss: 0.6995, Val Loss: 0.9895
2024-12-28 15:00:24,416 - INFO - Epoch 13/500, Train Loss: 0.6693, Val Loss: 0.9554
2024-12-28 15:00:25,513 - INFO - Epoch 14/500, Train Loss: 0.6432, Val Loss: 0.9469
2024-12-28 15:00:26,594 - INFO - Epoch 15/500, Train Loss: 0.6136, Val Loss: 0.9123
2024-12-28 15:00:27,712 - INFO - Epoch 16/500, Train Loss: 0.5968, Val Loss: 0.9022
2024-12-28 15:00:28,818 - INFO - Epoch 17/500, Train Loss: 0.5769, Val Loss: 0.9113
2024-12-28 15:00:29,892 - INFO - Epoch 18/500, Train Loss: 0.5627, Val Loss: 0.8914
2024-12-28 15:00:30,954 - INFO - Epoch 19/500, Train Loss: 0.5540, Val Loss: 0.8870
2024-12-28 15:00:31,990 - INFO - Epoch 20/500, Train Loss: 0.5409, Val Loss: 0.8793
2024-12-28 15:00:33,192 - INFO - Epoch 21/500, Train Loss: 0.5280, Val Loss: 0.8670
2024-12-28 15:00:34,262 - INFO - Epoch 22/500, Train Loss: 0.5133, Val Loss: 0.8311
2024-12-28 15:00:35,337 - INFO - Epoch 23/500, Train Loss: 0.5098, Val Loss: 0.8381
2024-12-28 15:00:36,475 - INFO - Epoch 24/500, Train Loss: 0.4973, Val Loss: 0.8466
2024-12-28 15:00:37,682 - INFO - Epoch 25/500, Train Loss: 0.4953, Val Loss: 0.8272
2024-12-28 15:00:38,839 - INFO - Epoch 26/500, Train Loss: 0.4926, Val Loss: 0.8314
2024-12-28 15:00:39,972 - INFO - Epoch 27/500, Train Loss: 0.4843, Val Loss: 0.8529
2024-12-28 15:00:41,087 - INFO - Epoch 28/500, Train Loss: 0.4808, Val Loss: 0.8160
2024-12-28 15:00:42,238 - INFO - Epoch 29/500, Train Loss: 0.4802, Val Loss: 0.8040
2024-12-28 15:00:43,478 - INFO - Epoch 30/500, Train Loss: 0.4741, Val Loss: 0.8283
2024-12-28 15:00:44,708 - INFO - Epoch 31/500, Train Loss: 0.4724, Val Loss: 0.8123
2024-12-28 15:00:45,931 - INFO - Epoch 32/500, Train Loss: 0.4638, Val Loss: 0.7949
2024-12-28 15:00:47,087 - INFO - Epoch 33/500, Train Loss: 0.4679, Val Loss: 0.7979
2024-12-28 15:00:48,283 - INFO - Epoch 34/500, Train Loss: 0.4615, Val Loss: 0.8383
2024-12-28 15:00:49,427 - INFO - Epoch 35/500, Train Loss: 0.4594, Val Loss: 0.7862
2024-12-28 15:00:50,604 - INFO - Epoch 36/500, Train Loss: 0.4564, Val Loss: 0.7658
2024-12-28 15:00:51,860 - INFO - Epoch 37/500, Train Loss: 0.4521, Val Loss: 0.7873
2024-12-28 15:00:52,993 - INFO - Epoch 38/500, Train Loss: 0.4490, Val Loss: 0.7806
2024-12-28 15:00:54,130 - INFO - Epoch 39/500, Train Loss: 0.4546, Val Loss: 0.7862
2024-12-28 15:00:55,199 - INFO - Epoch 40/500, Train Loss: 0.4474, Val Loss: 0.8142
2024-12-28 15:00:56,254 - INFO - Epoch 41/500, Train Loss: 0.4407, Val Loss: 0.7786
2024-12-28 15:00:56,254 - INFO - Early stopping triggered at epoch 41
2024-12-28 15:00:56,254 - INFO - Training completed in 47.78s
2024-12-28 15:00:56,254 - INFO - Final memory usage: CPU 1914.9 MB, GPU 103.8 MB
2024-12-28 15:00:56,255 - INFO - Model training completed in 47.79s
2024-12-28 15:00:56,333 - INFO - Prediction completed in 0.08s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:00:56,345 - INFO - Poison rate 0.07 completed in 47.88s
2024-12-28 15:00:56,345 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:00:56,347 - INFO - Label flipping details:
2024-12-28 15:00:56,347 - INFO - - Source class: 1
2024-12-28 15:00:56,347 - INFO - - Target class: 0
2024-12-28 15:00:56,347 - INFO - - Available samples in source class: 922
2024-12-28 15:00:56,347 - INFO - - Requested samples to poison: 1975
2024-12-28 15:00:56,347 - INFO - - Actual samples to flip: 921
2024-12-28 15:00:56,347 - INFO - - Samples remaining in source class: 1
2024-12-28 15:00:56,347 - INFO - Successfully flipped 921 labels from class 1 to 0
2024-12-28 15:00:56,347 - INFO - Total number of labels flipped: 921
2024-12-28 15:00:56,347 - INFO - Label flipping completed in 0.00s
2024-12-28 15:00:56,347 - INFO - Training set processing completed in 0.00s
2024-12-28 15:00:56,348 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:00:56,348 - INFO - Memory usage at start_fit: CPU 1876.3 MB, GPU 103.7 MB
2024-12-28 15:00:56,349 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:00:56,356 - INFO - Number of unique classes: 43
2024-12-28 15:00:56,492 - INFO - Fitted scaler and transformed data
2024-12-28 15:00:56,492 - INFO - Scaling time: 0.13s
2024-12-28 15:00:57,549 - INFO - Epoch 1/500, Train Loss: 6.9048, Val Loss: 3.2665
2024-12-28 15:00:58,646 - INFO - Epoch 2/500, Train Loss: 2.4342, Val Loss: 2.3233
2024-12-28 15:00:59,653 - INFO - Epoch 3/500, Train Loss: 1.7668, Val Loss: 1.9282
2024-12-28 15:01:00,617 - INFO - Epoch 4/500, Train Loss: 1.4261, Val Loss: 1.6671
2024-12-28 15:01:01,625 - INFO - Epoch 5/500, Train Loss: 1.2143, Val Loss: 1.5190
2024-12-28 15:01:02,643 - INFO - Epoch 6/500, Train Loss: 1.0746, Val Loss: 1.3918
2024-12-28 15:01:03,644 - INFO - Epoch 7/500, Train Loss: 0.9699, Val Loss: 1.2952
2024-12-28 15:01:04,733 - INFO - Epoch 8/500, Train Loss: 0.8899, Val Loss: 1.2302
2024-12-28 15:01:05,848 - INFO - Epoch 9/500, Train Loss: 0.8228, Val Loss: 1.1843
2024-12-28 15:01:06,965 - INFO - Epoch 10/500, Train Loss: 0.7729, Val Loss: 1.1431
2024-12-28 15:01:08,113 - INFO - Epoch 11/500, Train Loss: 0.7287, Val Loss: 1.0750
2024-12-28 15:01:09,357 - INFO - Epoch 12/500, Train Loss: 0.6910, Val Loss: 1.0817
2024-12-28 15:01:10,565 - INFO - Epoch 13/500, Train Loss: 0.6600, Val Loss: 1.0261
2024-12-28 15:01:11,752 - INFO - Epoch 14/500, Train Loss: 0.6324, Val Loss: 1.0178
2024-12-28 15:01:12,998 - INFO - Epoch 15/500, Train Loss: 0.6118, Val Loss: 0.9829
2024-12-28 15:01:14,264 - INFO - Epoch 16/500, Train Loss: 0.5909, Val Loss: 0.9705
2024-12-28 15:01:15,446 - INFO - Epoch 17/500, Train Loss: 0.5779, Val Loss: 0.9474
2024-12-28 15:01:16,629 - INFO - Epoch 18/500, Train Loss: 0.5567, Val Loss: 0.9243
2024-12-28 15:01:17,784 - INFO - Epoch 19/500, Train Loss: 0.5464, Val Loss: 0.9410
2024-12-28 15:01:19,020 - INFO - Epoch 20/500, Train Loss: 0.5352, Val Loss: 0.9631
2024-12-28 15:01:20,189 - INFO - Epoch 21/500, Train Loss: 0.5252, Val Loss: 0.9091
2024-12-28 15:01:21,333 - INFO - Epoch 22/500, Train Loss: 0.5107, Val Loss: 0.9191
2024-12-28 15:01:22,483 - INFO - Epoch 23/500, Train Loss: 0.5057, Val Loss: 0.8860
2024-12-28 15:01:23,483 - INFO - Epoch 24/500, Train Loss: 0.5018, Val Loss: 0.8856
2024-12-28 15:01:24,595 - INFO - Epoch 25/500, Train Loss: 0.4969, Val Loss: 0.8971
2024-12-28 15:01:25,791 - INFO - Epoch 26/500, Train Loss: 0.4889, Val Loss: 0.8709
2024-12-28 15:01:27,016 - INFO - Epoch 27/500, Train Loss: 0.4864, Val Loss: 0.9260
2024-12-28 15:01:28,175 - INFO - Epoch 28/500, Train Loss: 0.4796, Val Loss: 0.9196
2024-12-28 15:01:29,374 - INFO - Epoch 29/500, Train Loss: 0.4753, Val Loss: 0.8921
2024-12-28 15:01:30,610 - INFO - Epoch 30/500, Train Loss: 0.4713, Val Loss: 0.8716
2024-12-28 15:01:31,760 - INFO - Epoch 31/500, Train Loss: 0.4722, Val Loss: 0.8643
2024-12-28 15:01:32,876 - INFO - Epoch 32/500, Train Loss: 0.4667, Val Loss: 0.8531
2024-12-28 15:01:34,034 - INFO - Epoch 33/500, Train Loss: 0.4652, Val Loss: 0.8941
2024-12-28 15:01:35,286 - INFO - Epoch 34/500, Train Loss: 0.4596, Val Loss: 0.8805
2024-12-28 15:01:36,509 - INFO - Epoch 35/500, Train Loss: 0.4610, Val Loss: 0.8530
2024-12-28 15:01:37,752 - INFO - Epoch 36/500, Train Loss: 0.4577, Val Loss: 0.8796
2024-12-28 15:01:38,823 - INFO - Epoch 37/500, Train Loss: 0.4541, Val Loss: 0.8554
2024-12-28 15:01:38,823 - INFO - Early stopping triggered at epoch 37
2024-12-28 15:01:38,823 - INFO - Training completed in 42.48s
2024-12-28 15:01:38,824 - INFO - Final memory usage: CPU 1914.9 MB, GPU 103.8 MB
2024-12-28 15:01:38,824 - INFO - Model training completed in 42.48s
2024-12-28 15:01:38,886 - INFO - Prediction completed in 0.06s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:01:38,897 - INFO - Poison rate 0.1 completed in 42.55s
2024-12-28 15:01:38,897 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:01:38,898 - INFO - Label flipping details:
2024-12-28 15:01:38,898 - INFO - - Source class: 1
2024-12-28 15:01:38,899 - INFO - - Target class: 0
2024-12-28 15:01:38,899 - INFO - - Available samples in source class: 922
2024-12-28 15:01:38,899 - INFO - - Requested samples to poison: 3951
2024-12-28 15:01:38,899 - INFO - - Actual samples to flip: 921
2024-12-28 15:01:38,899 - INFO - - Samples remaining in source class: 1
2024-12-28 15:01:38,899 - INFO - Successfully flipped 921 labels from class 1 to 0
2024-12-28 15:01:38,899 - INFO - Total number of labels flipped: 921
2024-12-28 15:01:38,899 - INFO - Label flipping completed in 0.00s
2024-12-28 15:01:38,899 - INFO - Training set processing completed in 0.00s
2024-12-28 15:01:38,899 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:01:38,900 - INFO - Memory usage at start_fit: CPU 1876.3 MB, GPU 103.7 MB
2024-12-28 15:01:38,900 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:01:38,910 - INFO - Number of unique classes: 43
2024-12-28 15:01:39,094 - INFO - Fitted scaler and transformed data
2024-12-28 15:01:39,095 - INFO - Scaling time: 0.18s
2024-12-28 15:01:40,217 - INFO - Epoch 1/500, Train Loss: 6.8464, Val Loss: 3.2490
2024-12-28 15:01:41,326 - INFO - Epoch 2/500, Train Loss: 2.4480, Val Loss: 2.3303
2024-12-28 15:01:42,406 - INFO - Epoch 3/500, Train Loss: 1.7743, Val Loss: 1.9490
2024-12-28 15:01:43,485 - INFO - Epoch 4/500, Train Loss: 1.4319, Val Loss: 1.6818
2024-12-28 15:01:44,505 - INFO - Epoch 5/500, Train Loss: 1.2175, Val Loss: 1.5409
2024-12-28 15:01:45,596 - INFO - Epoch 6/500, Train Loss: 1.0798, Val Loss: 1.4336
2024-12-28 15:01:46,687 - INFO - Epoch 7/500, Train Loss: 0.9670, Val Loss: 1.3237
2024-12-28 15:01:47,809 - INFO - Epoch 8/500, Train Loss: 0.8883, Val Loss: 1.2857
2024-12-28 15:01:48,902 - INFO - Epoch 9/500, Train Loss: 0.8270, Val Loss: 1.2251
2024-12-28 15:01:49,992 - INFO - Epoch 10/500, Train Loss: 0.7712, Val Loss: 1.1956
2024-12-28 15:01:51,188 - INFO - Epoch 11/500, Train Loss: 0.7282, Val Loss: 1.1337
2024-12-28 15:01:52,311 - INFO - Epoch 12/500, Train Loss: 0.6857, Val Loss: 1.1283
2024-12-28 15:01:53,449 - INFO - Epoch 13/500, Train Loss: 0.6643, Val Loss: 1.1136
2024-12-28 15:01:54,614 - INFO - Epoch 14/500, Train Loss: 0.6334, Val Loss: 1.0596
2024-12-28 15:01:55,735 - INFO - Epoch 15/500, Train Loss: 0.6090, Val Loss: 1.0639
2024-12-28 15:01:56,853 - INFO - Epoch 16/500, Train Loss: 0.5922, Val Loss: 1.0327
2024-12-28 15:01:58,000 - INFO - Epoch 17/500, Train Loss: 0.5731, Val Loss: 1.0259
2024-12-28 15:01:59,129 - INFO - Epoch 18/500, Train Loss: 0.5565, Val Loss: 1.0183
2024-12-28 15:02:00,289 - INFO - Epoch 19/500, Train Loss: 0.5413, Val Loss: 1.0358
2024-12-28 15:02:01,446 - INFO - Epoch 20/500, Train Loss: 0.5333, Val Loss: 0.9999
2024-12-28 15:02:02,568 - INFO - Epoch 21/500, Train Loss: 0.5249, Val Loss: 0.9727
2024-12-28 15:02:03,680 - INFO - Epoch 22/500, Train Loss: 0.5121, Val Loss: 0.9794
2024-12-28 15:02:04,816 - INFO - Epoch 23/500, Train Loss: 0.5074, Val Loss: 1.0030
2024-12-28 15:02:05,910 - INFO - Epoch 24/500, Train Loss: 0.4972, Val Loss: 0.9631
2024-12-28 15:02:07,050 - INFO - Epoch 25/500, Train Loss: 0.4920, Val Loss: 0.9594
2024-12-28 15:02:08,162 - INFO - Epoch 26/500, Train Loss: 0.4860, Val Loss: 0.9892
2024-12-28 15:02:09,298 - INFO - Epoch 27/500, Train Loss: 0.4860, Val Loss: 0.9639
2024-12-28 15:02:10,396 - INFO - Epoch 28/500, Train Loss: 0.4803, Val Loss: 0.9678
2024-12-28 15:02:11,600 - INFO - Epoch 29/500, Train Loss: 0.4741, Val Loss: 0.9891
2024-12-28 15:02:12,765 - INFO - Epoch 30/500, Train Loss: 0.4740, Val Loss: 0.9666
2024-12-28 15:02:12,765 - INFO - Early stopping triggered at epoch 30
2024-12-28 15:02:12,765 - INFO - Training completed in 33.87s
2024-12-28 15:02:12,766 - INFO - Final memory usage: CPU 1914.9 MB, GPU 103.8 MB
2024-12-28 15:02:12,766 - INFO - Model training completed in 33.87s
2024-12-28 15:02:12,828 - INFO - Prediction completed in 0.06s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:02:12,839 - INFO - Poison rate 0.2 completed in 33.94s
2024-12-28 15:02:12,842 - INFO - Loaded 112 existing results
2024-12-28 15:02:12,842 - INFO - Total results to save: 119
2024-12-28 15:02:12,843 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:02:12,850 - INFO - Saved 119 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:02:12,850 - INFO - Total evaluation time: 317.96s
2024-12-28 15:02:12,856 - INFO - 
Progress: 18.8% - Evaluating GTSRB with SVM (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:02:13,043 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:02:13,044 - INFO - Dataset type: image
2024-12-28 15:02:13,044 - INFO - Sample size: 39209
2024-12-28 15:02:13,044 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:02:13,045 - INFO - Loading datasets...
2024-12-28 15:02:32,103 - INFO - Dataset loading completed in 19.06s
2024-12-28 15:02:32,103 - INFO - Extracting validation features...
2024-12-28 15:02:32,103 - INFO - Extracting features from 4435 samples...
2024-12-28 15:02:32,867 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:02:32,871 - INFO - Validation feature extraction completed in 0.77s
2024-12-28 15:02:32,872 - INFO - Extracting training features...
2024-12-28 15:02:32,872 - INFO - Extracting features from 19755 samples...
2024-12-28 15:02:35,521 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:02:35,527 - INFO - Training feature extraction completed in 2.65s
2024-12-28 15:02:35,527 - INFO - Creating model for classifier: SVM
2024-12-28 15:02:35,527 - INFO - Using device: cuda
2024-12-28 15:02:35,527 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 15:02:35,527 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:02:35,527 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:02:35,528 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:02:36,718 - INFO - Feature scaling completed in 1.19s
2024-12-28 15:02:36,719 - INFO - Starting feature selection (k=50)
2024-12-28 15:02:36,752 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:02:36,752 - INFO - Starting anomaly detection
2024-12-28 15:02:44,713 - INFO - Anomaly detection completed in 7.96s
2024-12-28 15:02:44,714 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:02:44,714 - INFO - Total fit_transform time: 9.19s
2024-12-28 15:02:44,714 - INFO - Training set processing completed in 9.19s
2024-12-28 15:02:44,714 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:02:44,716 - INFO - Memory usage at start_fit: CPU 1924.9 MB, GPU 104.0 MB
2024-12-28 15:02:44,716 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:02:44,722 - INFO - Number of unique classes: 43
2024-12-28 15:02:44,872 - INFO - Fitted scaler and transformed data
2024-12-28 15:02:44,873 - INFO - Scaling time: 0.15s
2024-12-28 15:02:45,991 - INFO - Epoch 1/500, Train Loss: 6.5771, Val Loss: 3.1725
2024-12-28 15:02:47,078 - INFO - Epoch 2/500, Train Loss: 2.3732, Val Loss: 2.2090
2024-12-28 15:02:48,197 - INFO - Epoch 3/500, Train Loss: 1.7256, Val Loss: 1.8456
2024-12-28 15:02:49,352 - INFO - Epoch 4/500, Train Loss: 1.3946, Val Loss: 1.6270
2024-12-28 15:02:50,489 - INFO - Epoch 5/500, Train Loss: 1.1965, Val Loss: 1.4870
2024-12-28 15:02:51,656 - INFO - Epoch 6/500, Train Loss: 1.0556, Val Loss: 1.3706
2024-12-28 15:02:52,785 - INFO - Epoch 7/500, Train Loss: 0.9461, Val Loss: 1.2708
2024-12-28 15:02:53,984 - INFO - Epoch 8/500, Train Loss: 0.8678, Val Loss: 1.2242
2024-12-28 15:02:55,241 - INFO - Epoch 9/500, Train Loss: 0.8046, Val Loss: 1.1912
2024-12-28 15:02:56,394 - INFO - Epoch 10/500, Train Loss: 0.7552, Val Loss: 1.1368
2024-12-28 15:02:57,647 - INFO - Epoch 11/500, Train Loss: 0.7103, Val Loss: 1.1149
2024-12-28 15:02:58,900 - INFO - Epoch 12/500, Train Loss: 0.6726, Val Loss: 1.0849
2024-12-28 15:02:59,960 - INFO - Epoch 13/500, Train Loss: 0.6449, Val Loss: 1.0814
2024-12-28 15:03:01,119 - INFO - Epoch 14/500, Train Loss: 0.6202, Val Loss: 1.0485
2024-12-28 15:03:02,168 - INFO - Epoch 15/500, Train Loss: 0.5977, Val Loss: 1.0195
2024-12-28 15:03:03,148 - INFO - Epoch 16/500, Train Loss: 0.5756, Val Loss: 1.0129
2024-12-28 15:03:04,185 - INFO - Epoch 17/500, Train Loss: 0.5618, Val Loss: 0.9983
2024-12-28 15:03:05,290 - INFO - Epoch 18/500, Train Loss: 0.5461, Val Loss: 0.9491
2024-12-28 15:03:06,471 - INFO - Epoch 19/500, Train Loss: 0.5280, Val Loss: 0.9778
2024-12-28 15:03:07,631 - INFO - Epoch 20/500, Train Loss: 0.5233, Val Loss: 0.9733
2024-12-28 15:03:08,859 - INFO - Epoch 21/500, Train Loss: 0.5119, Val Loss: 0.9794
2024-12-28 15:03:10,065 - INFO - Epoch 22/500, Train Loss: 0.5067, Val Loss: 0.9551
2024-12-28 15:03:11,291 - INFO - Epoch 23/500, Train Loss: 0.4957, Val Loss: 0.9475
2024-12-28 15:03:12,465 - INFO - Epoch 24/500, Train Loss: 0.4905, Val Loss: 0.9589
2024-12-28 15:03:13,666 - INFO - Epoch 25/500, Train Loss: 0.4813, Val Loss: 0.9411
2024-12-28 15:03:14,830 - INFO - Epoch 26/500, Train Loss: 0.4812, Val Loss: 0.9185
2024-12-28 15:03:16,028 - INFO - Epoch 27/500, Train Loss: 0.4756, Val Loss: 0.9493
2024-12-28 15:03:17,205 - INFO - Epoch 28/500, Train Loss: 0.4641, Val Loss: 0.9475
2024-12-28 15:03:18,349 - INFO - Epoch 29/500, Train Loss: 0.4693, Val Loss: 0.9275
2024-12-28 15:03:19,491 - INFO - Epoch 30/500, Train Loss: 0.4624, Val Loss: 0.9404
2024-12-28 15:03:20,654 - INFO - Epoch 31/500, Train Loss: 0.4619, Val Loss: 0.9059
2024-12-28 15:03:21,839 - INFO - Epoch 32/500, Train Loss: 0.4565, Val Loss: 0.9298
2024-12-28 15:03:23,028 - INFO - Epoch 33/500, Train Loss: 0.4529, Val Loss: 0.8986
2024-12-28 15:03:24,245 - INFO - Epoch 34/500, Train Loss: 0.4520, Val Loss: 0.9223
2024-12-28 15:03:25,498 - INFO - Epoch 35/500, Train Loss: 0.4495, Val Loss: 0.9333
2024-12-28 15:03:26,638 - INFO - Epoch 36/500, Train Loss: 0.4455, Val Loss: 0.9585
2024-12-28 15:03:27,737 - INFO - Epoch 37/500, Train Loss: 0.4491, Val Loss: 0.9031
2024-12-28 15:03:28,790 - INFO - Epoch 38/500, Train Loss: 0.4430, Val Loss: 0.9027
2024-12-28 15:03:28,791 - INFO - Early stopping triggered at epoch 38
2024-12-28 15:03:28,791 - INFO - Training completed in 44.08s
2024-12-28 15:03:28,791 - INFO - Final memory usage: CPU 1905.4 MB, GPU 104.5 MB
2024-12-28 15:03:28,792 - INFO - Model training completed in 44.08s
2024-12-28 15:03:28,856 - INFO - Prediction completed in 0.06s
2024-12-28 15:03:28,867 - INFO - Poison rate 0.0 completed in 53.34s
2024-12-28 15:03:28,867 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:03:28,869 - INFO - Label flipping details:
2024-12-28 15:03:28,869 - INFO - - Source class: 1
2024-12-28 15:03:28,869 - INFO - - Target class: 0
2024-12-28 15:03:28,869 - INFO - - Available samples in source class: 917
2024-12-28 15:03:28,869 - INFO - - Requested samples to poison: 197
2024-12-28 15:03:28,869 - INFO - - Actual samples to flip: 197
2024-12-28 15:03:28,869 - INFO - - Samples remaining in source class: 720
2024-12-28 15:03:28,869 - INFO - Successfully flipped 197 labels from class 1 to 0
2024-12-28 15:03:28,869 - INFO - Total number of labels flipped: 197
2024-12-28 15:03:28,869 - INFO - Label flipping completed in 0.00s
2024-12-28 15:03:28,869 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:03:28,869 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:03:30,051 - INFO - Feature scaling completed in 1.18s
2024-12-28 15:03:30,051 - INFO - Starting feature selection (k=50)
2024-12-28 15:03:30,084 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:03:30,085 - INFO - Starting anomaly detection
2024-12-28 15:03:37,946 - INFO - Anomaly detection completed in 7.86s
2024-12-28 15:03:37,946 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:03:37,947 - INFO - Total fit_transform time: 9.08s
2024-12-28 15:03:37,947 - INFO - Training set processing completed in 9.08s
2024-12-28 15:03:37,947 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:03:37,948 - INFO - Memory usage at start_fit: CPU 1886.0 MB, GPU 104.3 MB
2024-12-28 15:03:37,948 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:03:37,954 - INFO - Number of unique classes: 43
2024-12-28 15:03:38,095 - INFO - Fitted scaler and transformed data
2024-12-28 15:03:38,096 - INFO - Scaling time: 0.14s
2024-12-28 15:03:39,149 - INFO - Epoch 1/500, Train Loss: 6.5565, Val Loss: 3.0973
2024-12-28 15:03:40,206 - INFO - Epoch 2/500, Train Loss: 2.4794, Val Loss: 2.2063
2024-12-28 15:03:41,348 - INFO - Epoch 3/500, Train Loss: 1.8104, Val Loss: 1.8180
2024-12-28 15:03:42,546 - INFO - Epoch 4/500, Train Loss: 1.4622, Val Loss: 1.5781
2024-12-28 15:03:43,669 - INFO - Epoch 5/500, Train Loss: 1.2616, Val Loss: 1.4507
2024-12-28 15:03:44,870 - INFO - Epoch 6/500, Train Loss: 1.1110, Val Loss: 1.3574
2024-12-28 15:03:46,034 - INFO - Epoch 7/500, Train Loss: 1.0109, Val Loss: 1.2678
2024-12-28 15:03:47,169 - INFO - Epoch 8/500, Train Loss: 0.9262, Val Loss: 1.1920
2024-12-28 15:03:48,421 - INFO - Epoch 9/500, Train Loss: 0.8616, Val Loss: 1.1552
2024-12-28 15:03:49,587 - INFO - Epoch 10/500, Train Loss: 0.8137, Val Loss: 1.1162
2024-12-28 15:03:50,821 - INFO - Epoch 11/500, Train Loss: 0.7648, Val Loss: 1.0829
2024-12-28 15:03:52,031 - INFO - Epoch 12/500, Train Loss: 0.7293, Val Loss: 1.0512
2024-12-28 15:03:53,218 - INFO - Epoch 13/500, Train Loss: 0.6960, Val Loss: 1.0348
2024-12-28 15:03:54,360 - INFO - Epoch 14/500, Train Loss: 0.6735, Val Loss: 1.0274
2024-12-28 15:03:55,527 - INFO - Epoch 15/500, Train Loss: 0.6527, Val Loss: 1.0023
2024-12-28 15:03:56,702 - INFO - Epoch 16/500, Train Loss: 0.6292, Val Loss: 0.9706
2024-12-28 15:03:57,926 - INFO - Epoch 17/500, Train Loss: 0.6095, Val Loss: 0.9967
2024-12-28 15:03:59,146 - INFO - Epoch 18/500, Train Loss: 0.5954, Val Loss: 0.9570
2024-12-28 15:04:00,323 - INFO - Epoch 19/500, Train Loss: 0.5795, Val Loss: 0.9648
2024-12-28 15:04:01,545 - INFO - Epoch 20/500, Train Loss: 0.5670, Val Loss: 0.9434
2024-12-28 15:04:02,801 - INFO - Epoch 21/500, Train Loss: 0.5634, Val Loss: 0.9281
2024-12-28 15:04:04,030 - INFO - Epoch 22/500, Train Loss: 0.5498, Val Loss: 0.9626
2024-12-28 15:04:05,251 - INFO - Epoch 23/500, Train Loss: 0.5488, Val Loss: 0.9079
2024-12-28 15:04:06,467 - INFO - Epoch 24/500, Train Loss: 0.5349, Val Loss: 0.9067
2024-12-28 15:04:07,683 - INFO - Epoch 25/500, Train Loss: 0.5294, Val Loss: 0.8905
2024-12-28 15:04:08,790 - INFO - Epoch 26/500, Train Loss: 0.5213, Val Loss: 0.9123
2024-12-28 15:04:10,035 - INFO - Epoch 27/500, Train Loss: 0.5185, Val Loss: 0.8996
2024-12-28 15:04:11,228 - INFO - Epoch 28/500, Train Loss: 0.5146, Val Loss: 0.9031
2024-12-28 15:04:12,333 - INFO - Epoch 29/500, Train Loss: 0.5108, Val Loss: 0.9282
2024-12-28 15:04:13,462 - INFO - Epoch 30/500, Train Loss: 0.5168, Val Loss: 0.8934
2024-12-28 15:04:13,462 - INFO - Early stopping triggered at epoch 30
2024-12-28 15:04:13,462 - INFO - Training completed in 35.51s
2024-12-28 15:04:13,462 - INFO - Final memory usage: CPU 1924.5 MB, GPU 104.5 MB
2024-12-28 15:04:13,463 - INFO - Model training completed in 35.52s
2024-12-28 15:04:13,525 - INFO - Prediction completed in 0.06s
2024-12-28 15:04:13,536 - INFO - Poison rate 0.01 completed in 44.67s
2024-12-28 15:04:13,536 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:04:13,537 - INFO - Label flipping details:
2024-12-28 15:04:13,537 - INFO - - Source class: 1
2024-12-28 15:04:13,537 - INFO - - Target class: 0
2024-12-28 15:04:13,537 - INFO - - Available samples in source class: 917
2024-12-28 15:04:13,537 - INFO - - Requested samples to poison: 592
2024-12-28 15:04:13,537 - INFO - - Actual samples to flip: 592
2024-12-28 15:04:13,537 - INFO - - Samples remaining in source class: 325
2024-12-28 15:04:13,537 - INFO - Successfully flipped 592 labels from class 1 to 0
2024-12-28 15:04:13,538 - INFO - Total number of labels flipped: 592
2024-12-28 15:04:13,538 - INFO - Label flipping completed in 0.00s
2024-12-28 15:04:13,538 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:04:13,538 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:04:14,786 - INFO - Feature scaling completed in 1.25s
2024-12-28 15:04:14,787 - INFO - Starting feature selection (k=50)
2024-12-28 15:04:14,815 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:04:14,816 - INFO - Starting anomaly detection
2024-12-28 15:04:19,874 - INFO - Anomaly detection completed in 5.06s
2024-12-28 15:04:19,874 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:04:19,874 - INFO - Total fit_transform time: 6.34s
2024-12-28 15:04:19,875 - INFO - Training set processing completed in 6.34s
2024-12-28 15:04:19,875 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:04:19,875 - INFO - Memory usage at start_fit: CPU 1886.0 MB, GPU 104.3 MB
2024-12-28 15:04:19,876 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:04:19,883 - INFO - Number of unique classes: 43
2024-12-28 15:04:20,038 - INFO - Fitted scaler and transformed data
2024-12-28 15:04:20,039 - INFO - Scaling time: 0.15s
2024-12-28 15:04:21,120 - INFO - Epoch 1/500, Train Loss: 6.9899, Val Loss: 3.3017
2024-12-28 15:04:22,184 - INFO - Epoch 2/500, Train Loss: 2.5241, Val Loss: 2.2753
2024-12-28 15:04:23,287 - INFO - Epoch 3/500, Train Loss: 1.8426, Val Loss: 1.9101
2024-12-28 15:04:24,442 - INFO - Epoch 4/500, Train Loss: 1.4918, Val Loss: 1.7030
2024-12-28 15:04:25,622 - INFO - Epoch 5/500, Train Loss: 1.2842, Val Loss: 1.5436
2024-12-28 15:04:26,840 - INFO - Epoch 6/500, Train Loss: 1.1392, Val Loss: 1.4256
2024-12-28 15:04:28,009 - INFO - Epoch 7/500, Train Loss: 1.0252, Val Loss: 1.3621
2024-12-28 15:04:29,187 - INFO - Epoch 8/500, Train Loss: 0.9522, Val Loss: 1.2698
2024-12-28 15:04:30,461 - INFO - Epoch 9/500, Train Loss: 0.8791, Val Loss: 1.2081
2024-12-28 15:04:31,679 - INFO - Epoch 10/500, Train Loss: 0.8277, Val Loss: 1.1979
2024-12-28 15:04:32,934 - INFO - Epoch 11/500, Train Loss: 0.7848, Val Loss: 1.1242
2024-12-28 15:04:34,090 - INFO - Epoch 12/500, Train Loss: 0.7416, Val Loss: 1.1341
2024-12-28 15:04:35,263 - INFO - Epoch 13/500, Train Loss: 0.7084, Val Loss: 1.0999
2024-12-28 15:04:36,512 - INFO - Epoch 14/500, Train Loss: 0.6852, Val Loss: 1.0856
2024-12-28 15:04:37,651 - INFO - Epoch 15/500, Train Loss: 0.6603, Val Loss: 1.0752
2024-12-28 15:04:38,736 - INFO - Epoch 16/500, Train Loss: 0.6410, Val Loss: 1.0485
2024-12-28 15:04:39,858 - INFO - Epoch 17/500, Train Loss: 0.6208, Val Loss: 1.0611
2024-12-28 15:04:41,106 - INFO - Epoch 18/500, Train Loss: 0.6065, Val Loss: 1.0579
2024-12-28 15:04:42,329 - INFO - Epoch 19/500, Train Loss: 0.5954, Val Loss: 1.0238
2024-12-28 15:04:43,528 - INFO - Epoch 20/500, Train Loss: 0.5792, Val Loss: 0.9967
2024-12-28 15:04:44,747 - INFO - Epoch 21/500, Train Loss: 0.5700, Val Loss: 0.9995
2024-12-28 15:04:45,991 - INFO - Epoch 22/500, Train Loss: 0.5605, Val Loss: 1.0123
2024-12-28 15:04:47,248 - INFO - Epoch 23/500, Train Loss: 0.5534, Val Loss: 1.0063
2024-12-28 15:04:48,480 - INFO - Epoch 24/500, Train Loss: 0.5438, Val Loss: 1.0172
2024-12-28 15:04:49,715 - INFO - Epoch 25/500, Train Loss: 0.5407, Val Loss: 0.9364
2024-12-28 15:04:50,939 - INFO - Epoch 26/500, Train Loss: 0.5313, Val Loss: 0.9593
2024-12-28 15:04:52,056 - INFO - Epoch 27/500, Train Loss: 0.5254, Val Loss: 0.9889
2024-12-28 15:04:53,189 - INFO - Epoch 28/500, Train Loss: 0.5247, Val Loss: 0.9534
2024-12-28 15:04:54,363 - INFO - Epoch 29/500, Train Loss: 0.5196, Val Loss: 0.9926
2024-12-28 15:04:55,528 - INFO - Epoch 30/500, Train Loss: 0.5152, Val Loss: 0.9268
2024-12-28 15:04:56,746 - INFO - Epoch 31/500, Train Loss: 0.5118, Val Loss: 0.9462
2024-12-28 15:04:57,906 - INFO - Epoch 32/500, Train Loss: 0.5073, Val Loss: 0.9674
2024-12-28 15:04:59,009 - INFO - Epoch 33/500, Train Loss: 0.5031, Val Loss: 0.9747
2024-12-28 15:05:00,139 - INFO - Epoch 34/500, Train Loss: 0.5042, Val Loss: 0.9302
2024-12-28 15:05:01,260 - INFO - Epoch 35/500, Train Loss: 0.4980, Val Loss: 0.9649
2024-12-28 15:05:01,260 - INFO - Early stopping triggered at epoch 35
2024-12-28 15:05:01,260 - INFO - Training completed in 41.38s
2024-12-28 15:05:01,260 - INFO - Final memory usage: CPU 1924.5 MB, GPU 104.5 MB
2024-12-28 15:05:01,261 - INFO - Model training completed in 41.39s
2024-12-28 15:05:01,322 - INFO - Prediction completed in 0.06s
2024-12-28 15:05:01,334 - INFO - Poison rate 0.03 completed in 47.80s
2024-12-28 15:05:01,334 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:05:01,336 - INFO - Label flipping details:
2024-12-28 15:05:01,336 - INFO - - Source class: 1
2024-12-28 15:05:01,336 - INFO - - Target class: 0
2024-12-28 15:05:01,336 - INFO - - Available samples in source class: 917
2024-12-28 15:05:01,336 - INFO - - Requested samples to poison: 987
2024-12-28 15:05:01,336 - INFO - - Actual samples to flip: 916
2024-12-28 15:05:01,336 - INFO - - Samples remaining in source class: 1
2024-12-28 15:05:01,336 - INFO - Successfully flipped 916 labels from class 1 to 0
2024-12-28 15:05:01,336 - INFO - Total number of labels flipped: 916
2024-12-28 15:05:01,336 - INFO - Label flipping completed in 0.00s
2024-12-28 15:05:01,336 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:05:01,336 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:05:02,570 - INFO - Feature scaling completed in 1.23s
2024-12-28 15:05:02,570 - INFO - Starting feature selection (k=50)
2024-12-28 15:05:02,598 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:05:02,598 - INFO - Starting anomaly detection
2024-12-28 15:05:08,277 - INFO - Anomaly detection completed in 5.68s
2024-12-28 15:05:08,277 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:05:08,278 - INFO - Total fit_transform time: 6.94s
2024-12-28 15:05:08,278 - INFO - Training set processing completed in 6.94s
2024-12-28 15:05:08,278 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:05:08,279 - INFO - Memory usage at start_fit: CPU 1886.0 MB, GPU 104.3 MB
2024-12-28 15:05:08,279 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:05:08,285 - INFO - Number of unique classes: 43
2024-12-28 15:05:08,430 - INFO - Fitted scaler and transformed data
2024-12-28 15:05:08,430 - INFO - Scaling time: 0.14s
2024-12-28 15:05:09,450 - INFO - Epoch 1/500, Train Loss: 6.5521, Val Loss: 3.2499
2024-12-28 15:05:10,469 - INFO - Epoch 2/500, Train Loss: 2.3640, Val Loss: 2.2535
2024-12-28 15:05:11,534 - INFO - Epoch 3/500, Train Loss: 1.7181, Val Loss: 1.8412
2024-12-28 15:05:12,545 - INFO - Epoch 4/500, Train Loss: 1.3830, Val Loss: 1.6060
2024-12-28 15:05:13,509 - INFO - Epoch 5/500, Train Loss: 1.1880, Val Loss: 1.4525
2024-12-28 15:05:14,475 - INFO - Epoch 6/500, Train Loss: 1.0440, Val Loss: 1.3289
2024-12-28 15:05:15,543 - INFO - Epoch 7/500, Train Loss: 0.9461, Val Loss: 1.2447
2024-12-28 15:05:16,601 - INFO - Epoch 8/500, Train Loss: 0.8630, Val Loss: 1.1761
2024-12-28 15:05:17,958 - INFO - Epoch 9/500, Train Loss: 0.7985, Val Loss: 1.1475
2024-12-28 15:05:19,177 - INFO - Epoch 10/500, Train Loss: 0.7440, Val Loss: 1.1010
2024-12-28 15:05:20,357 - INFO - Epoch 11/500, Train Loss: 0.7087, Val Loss: 1.0695
2024-12-28 15:05:21,553 - INFO - Epoch 12/500, Train Loss: 0.6692, Val Loss: 0.9986
2024-12-28 15:05:22,636 - INFO - Epoch 13/500, Train Loss: 0.6364, Val Loss: 0.9999
2024-12-28 15:05:23,648 - INFO - Epoch 14/500, Train Loss: 0.6106, Val Loss: 0.9810
2024-12-28 15:05:24,763 - INFO - Epoch 15/500, Train Loss: 0.5897, Val Loss: 0.9736
2024-12-28 15:05:25,847 - INFO - Epoch 16/500, Train Loss: 0.5719, Val Loss: 0.9451
2024-12-28 15:05:26,926 - INFO - Epoch 17/500, Train Loss: 0.5515, Val Loss: 0.9510
2024-12-28 15:05:28,046 - INFO - Epoch 18/500, Train Loss: 0.5426, Val Loss: 0.9385
2024-12-28 15:05:29,172 - INFO - Epoch 19/500, Train Loss: 0.5272, Val Loss: 0.9131
2024-12-28 15:05:30,239 - INFO - Epoch 20/500, Train Loss: 0.5147, Val Loss: 0.9133
2024-12-28 15:05:31,354 - INFO - Epoch 21/500, Train Loss: 0.5041, Val Loss: 0.8995
2024-12-28 15:05:32,471 - INFO - Epoch 22/500, Train Loss: 0.4981, Val Loss: 0.9115
2024-12-28 15:05:33,632 - INFO - Epoch 23/500, Train Loss: 0.4890, Val Loss: 0.9002
2024-12-28 15:05:34,749 - INFO - Epoch 24/500, Train Loss: 0.4826, Val Loss: 0.9281
2024-12-28 15:05:35,871 - INFO - Epoch 25/500, Train Loss: 0.4783, Val Loss: 0.9205
2024-12-28 15:05:37,005 - INFO - Epoch 26/500, Train Loss: 0.4760, Val Loss: 0.8561
2024-12-28 15:05:38,136 - INFO - Epoch 27/500, Train Loss: 0.4693, Val Loss: 0.8921
2024-12-28 15:05:39,256 - INFO - Epoch 28/500, Train Loss: 0.4660, Val Loss: 0.9056
2024-12-28 15:05:40,422 - INFO - Epoch 29/500, Train Loss: 0.4561, Val Loss: 0.8860
2024-12-28 15:05:41,634 - INFO - Epoch 30/500, Train Loss: 0.4547, Val Loss: 0.8842
2024-12-28 15:05:42,702 - INFO - Epoch 31/500, Train Loss: 0.4511, Val Loss: 0.8881
2024-12-28 15:05:42,703 - INFO - Early stopping triggered at epoch 31
2024-12-28 15:05:42,703 - INFO - Training completed in 34.42s
2024-12-28 15:05:42,703 - INFO - Final memory usage: CPU 1924.5 MB, GPU 104.5 MB
2024-12-28 15:05:42,703 - INFO - Model training completed in 34.43s
2024-12-28 15:05:42,786 - INFO - Prediction completed in 0.08s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:05:42,797 - INFO - Poison rate 0.05 completed in 41.46s
2024-12-28 15:05:42,797 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:05:42,799 - INFO - Label flipping details:
2024-12-28 15:05:42,799 - INFO - - Source class: 1
2024-12-28 15:05:42,799 - INFO - - Target class: 0
2024-12-28 15:05:42,799 - INFO - - Available samples in source class: 917
2024-12-28 15:05:42,799 - INFO - - Requested samples to poison: 1382
2024-12-28 15:05:42,799 - INFO - - Actual samples to flip: 916
2024-12-28 15:05:42,799 - INFO - - Samples remaining in source class: 1
2024-12-28 15:05:42,799 - INFO - Successfully flipped 916 labels from class 1 to 0
2024-12-28 15:05:42,799 - INFO - Total number of labels flipped: 916
2024-12-28 15:05:42,799 - INFO - Label flipping completed in 0.00s
2024-12-28 15:05:42,799 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:05:42,799 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:05:44,005 - INFO - Feature scaling completed in 1.21s
2024-12-28 15:05:44,005 - INFO - Starting feature selection (k=50)
2024-12-28 15:05:44,035 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:05:44,035 - INFO - Starting anomaly detection
2024-12-28 15:05:52,124 - INFO - Anomaly detection completed in 8.09s
2024-12-28 15:05:52,124 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:05:52,124 - INFO - Total fit_transform time: 9.32s
2024-12-28 15:05:52,125 - INFO - Training set processing completed in 9.33s
2024-12-28 15:05:52,125 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:05:52,126 - INFO - Memory usage at start_fit: CPU 1886.0 MB, GPU 104.3 MB
2024-12-28 15:05:52,126 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:05:52,135 - INFO - Number of unique classes: 43
2024-12-28 15:05:52,301 - INFO - Fitted scaler and transformed data
2024-12-28 15:05:52,302 - INFO - Scaling time: 0.16s
2024-12-28 15:05:53,312 - INFO - Epoch 1/500, Train Loss: 6.8363, Val Loss: 3.4650
2024-12-28 15:05:54,366 - INFO - Epoch 2/500, Train Loss: 2.3827, Val Loss: 2.4093
2024-12-28 15:05:55,381 - INFO - Epoch 3/500, Train Loss: 1.7301, Val Loss: 2.0287
2024-12-28 15:05:56,524 - INFO - Epoch 4/500, Train Loss: 1.4024, Val Loss: 1.7573
2024-12-28 15:05:57,664 - INFO - Epoch 5/500, Train Loss: 1.1951, Val Loss: 1.6010
2024-12-28 15:05:58,729 - INFO - Epoch 6/500, Train Loss: 1.0534, Val Loss: 1.5022
2024-12-28 15:05:59,848 - INFO - Epoch 7/500, Train Loss: 0.9465, Val Loss: 1.3793
2024-12-28 15:06:00,983 - INFO - Epoch 8/500, Train Loss: 0.8743, Val Loss: 1.3287
2024-12-28 15:06:02,072 - INFO - Epoch 9/500, Train Loss: 0.8016, Val Loss: 1.2549
2024-12-28 15:06:03,156 - INFO - Epoch 10/500, Train Loss: 0.7513, Val Loss: 1.2091
2024-12-28 15:06:04,304 - INFO - Epoch 11/500, Train Loss: 0.7095, Val Loss: 1.1553
2024-12-28 15:06:05,409 - INFO - Epoch 12/500, Train Loss: 0.6764, Val Loss: 1.1446
2024-12-28 15:06:06,500 - INFO - Epoch 13/500, Train Loss: 0.6447, Val Loss: 1.0981
2024-12-28 15:06:07,643 - INFO - Epoch 14/500, Train Loss: 0.6179, Val Loss: 1.0912
2024-12-28 15:06:08,858 - INFO - Epoch 15/500, Train Loss: 0.5944, Val Loss: 1.0701
2024-12-28 15:06:10,002 - INFO - Epoch 16/500, Train Loss: 0.5785, Val Loss: 1.0566
2024-12-28 15:06:11,125 - INFO - Epoch 17/500, Train Loss: 0.5595, Val Loss: 1.0382
2024-12-28 15:06:12,305 - INFO - Epoch 18/500, Train Loss: 0.5453, Val Loss: 1.0096
2024-12-28 15:06:13,423 - INFO - Epoch 19/500, Train Loss: 0.5348, Val Loss: 0.9802
2024-12-28 15:06:14,520 - INFO - Epoch 20/500, Train Loss: 0.5218, Val Loss: 0.9957
2024-12-28 15:06:15,570 - INFO - Epoch 21/500, Train Loss: 0.5149, Val Loss: 0.9712
2024-12-28 15:06:16,687 - INFO - Epoch 22/500, Train Loss: 0.5029, Val Loss: 0.9951
2024-12-28 15:06:17,868 - INFO - Epoch 23/500, Train Loss: 0.4917, Val Loss: 0.9802
2024-12-28 15:06:19,111 - INFO - Epoch 24/500, Train Loss: 0.4874, Val Loss: 0.9572
2024-12-28 15:06:20,304 - INFO - Epoch 25/500, Train Loss: 0.4868, Val Loss: 0.9349
2024-12-28 15:06:21,482 - INFO - Epoch 26/500, Train Loss: 0.4790, Val Loss: 0.9902
2024-12-28 15:06:22,685 - INFO - Epoch 27/500, Train Loss: 0.4742, Val Loss: 0.9436
2024-12-28 15:06:23,854 - INFO - Epoch 28/500, Train Loss: 0.4681, Val Loss: 0.9477
2024-12-28 15:06:25,037 - INFO - Epoch 29/500, Train Loss: 0.4626, Val Loss: 0.9613
2024-12-28 15:06:26,230 - INFO - Epoch 30/500, Train Loss: 0.4561, Val Loss: 0.9560
2024-12-28 15:06:26,230 - INFO - Early stopping triggered at epoch 30
2024-12-28 15:06:26,230 - INFO - Training completed in 34.10s
2024-12-28 15:06:26,230 - INFO - Final memory usage: CPU 1924.5 MB, GPU 104.5 MB
2024-12-28 15:06:26,231 - INFO - Model training completed in 34.11s
2024-12-28 15:06:26,314 - INFO - Prediction completed in 0.08s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:06:26,328 - INFO - Poison rate 0.07 completed in 43.53s
2024-12-28 15:06:26,329 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:06:26,330 - INFO - Label flipping details:
2024-12-28 15:06:26,330 - INFO - - Source class: 1
2024-12-28 15:06:26,330 - INFO - - Target class: 0
2024-12-28 15:06:26,330 - INFO - - Available samples in source class: 917
2024-12-28 15:06:26,330 - INFO - - Requested samples to poison: 1975
2024-12-28 15:06:26,330 - INFO - - Actual samples to flip: 916
2024-12-28 15:06:26,330 - INFO - - Samples remaining in source class: 1
2024-12-28 15:06:26,330 - INFO - Successfully flipped 916 labels from class 1 to 0
2024-12-28 15:06:26,330 - INFO - Total number of labels flipped: 916
2024-12-28 15:06:26,330 - INFO - Label flipping completed in 0.00s
2024-12-28 15:06:26,330 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:06:26,331 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:06:27,638 - INFO - Feature scaling completed in 1.31s
2024-12-28 15:06:27,638 - INFO - Starting feature selection (k=50)
2024-12-28 15:06:27,672 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:06:27,672 - INFO - Starting anomaly detection
2024-12-28 15:06:34,551 - INFO - Anomaly detection completed in 6.88s
2024-12-28 15:06:34,552 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:06:34,552 - INFO - Total fit_transform time: 8.22s
2024-12-28 15:06:34,552 - INFO - Training set processing completed in 8.22s
2024-12-28 15:06:34,552 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:06:34,553 - INFO - Memory usage at start_fit: CPU 1886.0 MB, GPU 104.3 MB
2024-12-28 15:06:34,553 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:06:34,561 - INFO - Number of unique classes: 43
2024-12-28 15:06:34,699 - INFO - Fitted scaler and transformed data
2024-12-28 15:06:34,699 - INFO - Scaling time: 0.14s
2024-12-28 15:06:35,677 - INFO - Epoch 1/500, Train Loss: 6.6445, Val Loss: 3.3389
2024-12-28 15:06:36,744 - INFO - Epoch 2/500, Train Loss: 2.3820, Val Loss: 2.3084
2024-12-28 15:06:37,788 - INFO - Epoch 3/500, Train Loss: 1.7175, Val Loss: 1.9088
2024-12-28 15:06:38,879 - INFO - Epoch 4/500, Train Loss: 1.3877, Val Loss: 1.6432
2024-12-28 15:06:39,962 - INFO - Epoch 5/500, Train Loss: 1.1863, Val Loss: 1.4840
2024-12-28 15:06:41,164 - INFO - Epoch 6/500, Train Loss: 1.0467, Val Loss: 1.3574
2024-12-28 15:06:42,360 - INFO - Epoch 7/500, Train Loss: 0.9432, Val Loss: 1.2689
2024-12-28 15:06:43,527 - INFO - Epoch 8/500, Train Loss: 0.8698, Val Loss: 1.1786
2024-12-28 15:06:44,574 - INFO - Epoch 9/500, Train Loss: 0.8000, Val Loss: 1.0967
2024-12-28 15:06:45,609 - INFO - Epoch 10/500, Train Loss: 0.7517, Val Loss: 1.0966
2024-12-28 15:06:46,633 - INFO - Epoch 11/500, Train Loss: 0.7078, Val Loss: 1.0511
2024-12-28 15:06:47,707 - INFO - Epoch 12/500, Train Loss: 0.6723, Val Loss: 1.0118
2024-12-28 15:06:48,820 - INFO - Epoch 13/500, Train Loss: 0.6427, Val Loss: 0.9973
2024-12-28 15:06:49,939 - INFO - Epoch 14/500, Train Loss: 0.6154, Val Loss: 0.9629
2024-12-28 15:06:50,966 - INFO - Epoch 15/500, Train Loss: 0.5939, Val Loss: 0.9379
2024-12-28 15:06:51,936 - INFO - Epoch 16/500, Train Loss: 0.5788, Val Loss: 0.9328
2024-12-28 15:06:52,946 - INFO - Epoch 17/500, Train Loss: 0.5628, Val Loss: 0.9020
2024-12-28 15:06:54,028 - INFO - Epoch 18/500, Train Loss: 0.5508, Val Loss: 0.8939
2024-12-28 15:06:55,159 - INFO - Epoch 19/500, Train Loss: 0.5347, Val Loss: 0.8734
2024-12-28 15:06:56,279 - INFO - Epoch 20/500, Train Loss: 0.5247, Val Loss: 0.8995
2024-12-28 15:06:57,391 - INFO - Epoch 21/500, Train Loss: 0.5133, Val Loss: 0.8768
2024-12-28 15:06:58,565 - INFO - Epoch 22/500, Train Loss: 0.5045, Val Loss: 0.8663
2024-12-28 15:06:59,747 - INFO - Epoch 23/500, Train Loss: 0.5008, Val Loss: 0.8569
2024-12-28 15:07:00,871 - INFO - Epoch 24/500, Train Loss: 0.4926, Val Loss: 0.8542
2024-12-28 15:07:01,993 - INFO - Epoch 25/500, Train Loss: 0.4828, Val Loss: 0.8665
2024-12-28 15:07:03,118 - INFO - Epoch 26/500, Train Loss: 0.4797, Val Loss: 0.8487
2024-12-28 15:07:04,196 - INFO - Epoch 27/500, Train Loss: 0.4671, Val Loss: 0.8443
2024-12-28 15:07:05,268 - INFO - Epoch 28/500, Train Loss: 0.4669, Val Loss: 0.8258
2024-12-28 15:07:06,408 - INFO - Epoch 29/500, Train Loss: 0.4689, Val Loss: 0.8205
2024-12-28 15:07:07,548 - INFO - Epoch 30/500, Train Loss: 0.4619, Val Loss: 0.8540
2024-12-28 15:07:08,644 - INFO - Epoch 31/500, Train Loss: 0.4611, Val Loss: 0.8319
2024-12-28 15:07:09,758 - INFO - Epoch 32/500, Train Loss: 0.4601, Val Loss: 0.8293
2024-12-28 15:07:10,773 - INFO - Epoch 33/500, Train Loss: 0.4514, Val Loss: 0.8057
2024-12-28 15:07:11,821 - INFO - Epoch 34/500, Train Loss: 0.4463, Val Loss: 0.8236
2024-12-28 15:07:12,804 - INFO - Epoch 35/500, Train Loss: 0.4476, Val Loss: 0.8034
2024-12-28 15:07:13,875 - INFO - Epoch 36/500, Train Loss: 0.4454, Val Loss: 0.8121
2024-12-28 15:07:14,991 - INFO - Epoch 37/500, Train Loss: 0.4399, Val Loss: 0.8383
2024-12-28 15:07:16,104 - INFO - Epoch 38/500, Train Loss: 0.4416, Val Loss: 0.8199
2024-12-28 15:07:17,258 - INFO - Epoch 39/500, Train Loss: 0.4436, Val Loss: 0.8147
2024-12-28 15:07:18,337 - INFO - Epoch 40/500, Train Loss: 0.4366, Val Loss: 0.8391
2024-12-28 15:07:18,337 - INFO - Early stopping triggered at epoch 40
2024-12-28 15:07:18,337 - INFO - Training completed in 43.78s
2024-12-28 15:07:18,337 - INFO - Final memory usage: CPU 1924.5 MB, GPU 104.5 MB
2024-12-28 15:07:18,338 - INFO - Model training completed in 43.79s
2024-12-28 15:07:18,434 - INFO - Prediction completed in 0.10s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:07:18,446 - INFO - Poison rate 0.1 completed in 52.12s
2024-12-28 15:07:18,446 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:07:18,447 - INFO - Label flipping details:
2024-12-28 15:07:18,447 - INFO - - Source class: 1
2024-12-28 15:07:18,447 - INFO - - Target class: 0
2024-12-28 15:07:18,447 - INFO - - Available samples in source class: 917
2024-12-28 15:07:18,447 - INFO - - Requested samples to poison: 3951
2024-12-28 15:07:18,447 - INFO - - Actual samples to flip: 916
2024-12-28 15:07:18,447 - INFO - - Samples remaining in source class: 1
2024-12-28 15:07:18,448 - INFO - Successfully flipped 916 labels from class 1 to 0
2024-12-28 15:07:18,448 - INFO - Total number of labels flipped: 916
2024-12-28 15:07:18,448 - INFO - Label flipping completed in 0.00s
2024-12-28 15:07:18,448 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:07:18,448 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:07:19,693 - INFO - Feature scaling completed in 1.25s
2024-12-28 15:07:19,693 - INFO - Starting feature selection (k=50)
2024-12-28 15:07:19,721 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:07:19,721 - INFO - Starting anomaly detection
2024-12-28 15:07:26,746 - INFO - Anomaly detection completed in 7.03s
2024-12-28 15:07:26,746 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:07:26,747 - INFO - Total fit_transform time: 8.30s
2024-12-28 15:07:26,747 - INFO - Training set processing completed in 8.30s
2024-12-28 15:07:26,747 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:07:26,748 - INFO - Memory usage at start_fit: CPU 1886.0 MB, GPU 104.3 MB
2024-12-28 15:07:26,748 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:07:26,755 - INFO - Number of unique classes: 43
2024-12-28 15:07:26,899 - INFO - Fitted scaler and transformed data
2024-12-28 15:07:26,899 - INFO - Scaling time: 0.14s
2024-12-28 15:07:27,877 - INFO - Epoch 1/500, Train Loss: 6.7967, Val Loss: 3.1655
2024-12-28 15:07:28,926 - INFO - Epoch 2/500, Train Loss: 2.3715, Val Loss: 2.2627
2024-12-28 15:07:29,893 - INFO - Epoch 3/500, Train Loss: 1.7154, Val Loss: 1.8486
2024-12-28 15:07:30,972 - INFO - Epoch 4/500, Train Loss: 1.3862, Val Loss: 1.6153
2024-12-28 15:07:32,226 - INFO - Epoch 5/500, Train Loss: 1.1892, Val Loss: 1.4694
2024-12-28 15:07:33,452 - INFO - Epoch 6/500, Train Loss: 1.0472, Val Loss: 1.3477
2024-12-28 15:07:34,706 - INFO - Epoch 7/500, Train Loss: 0.9466, Val Loss: 1.2706
2024-12-28 15:07:35,940 - INFO - Epoch 8/500, Train Loss: 0.8690, Val Loss: 1.2099
2024-12-28 15:07:37,182 - INFO - Epoch 9/500, Train Loss: 0.8013, Val Loss: 1.1686
2024-12-28 15:07:38,391 - INFO - Epoch 10/500, Train Loss: 0.7568, Val Loss: 1.1034
2024-12-28 15:07:39,541 - INFO - Epoch 11/500, Train Loss: 0.7100, Val Loss: 1.0659
2024-12-28 15:07:40,701 - INFO - Epoch 12/500, Train Loss: 0.6731, Val Loss: 1.0215
2024-12-28 15:07:41,856 - INFO - Epoch 13/500, Train Loss: 0.6428, Val Loss: 0.9979
2024-12-28 15:07:43,031 - INFO - Epoch 14/500, Train Loss: 0.6177, Val Loss: 0.9963
2024-12-28 15:07:44,160 - INFO - Epoch 15/500, Train Loss: 0.5959, Val Loss: 0.9367
2024-12-28 15:07:45,383 - INFO - Epoch 16/500, Train Loss: 0.5770, Val Loss: 0.9636
2024-12-28 15:07:46,527 - INFO - Epoch 17/500, Train Loss: 0.5559, Val Loss: 0.9404
2024-12-28 15:07:47,593 - INFO - Epoch 18/500, Train Loss: 0.5484, Val Loss: 0.9365
2024-12-28 15:07:48,741 - INFO - Epoch 19/500, Train Loss: 0.5332, Val Loss: 0.9307
2024-12-28 15:07:49,893 - INFO - Epoch 20/500, Train Loss: 0.5174, Val Loss: 0.8959
2024-12-28 15:07:51,136 - INFO - Epoch 21/500, Train Loss: 0.5094, Val Loss: 0.9044
2024-12-28 15:07:52,255 - INFO - Epoch 22/500, Train Loss: 0.5079, Val Loss: 0.9125
2024-12-28 15:07:53,409 - INFO - Epoch 23/500, Train Loss: 0.4989, Val Loss: 0.8831
2024-12-28 15:07:54,543 - INFO - Epoch 24/500, Train Loss: 0.4867, Val Loss: 0.9111
2024-12-28 15:07:55,718 - INFO - Epoch 25/500, Train Loss: 0.4780, Val Loss: 0.8955
2024-12-28 15:07:56,861 - INFO - Epoch 26/500, Train Loss: 0.4755, Val Loss: 0.8707
2024-12-28 15:07:58,059 - INFO - Epoch 27/500, Train Loss: 0.4744, Val Loss: 0.8626
2024-12-28 15:07:59,236 - INFO - Epoch 28/500, Train Loss: 0.4700, Val Loss: 0.8600
2024-12-28 15:08:00,417 - INFO - Epoch 29/500, Train Loss: 0.4667, Val Loss: 0.8828
2024-12-28 15:08:01,673 - INFO - Epoch 30/500, Train Loss: 0.4615, Val Loss: 0.8402
2024-12-28 15:08:02,846 - INFO - Epoch 31/500, Train Loss: 0.4495, Val Loss: 0.8581
2024-12-28 15:08:03,947 - INFO - Epoch 32/500, Train Loss: 0.4541, Val Loss: 0.8829
2024-12-28 15:08:05,128 - INFO - Epoch 33/500, Train Loss: 0.4475, Val Loss: 0.8864
2024-12-28 15:08:06,322 - INFO - Epoch 34/500, Train Loss: 0.4474, Val Loss: 0.8674
2024-12-28 15:08:07,421 - INFO - Epoch 35/500, Train Loss: 0.4498, Val Loss: 0.8399
2024-12-28 15:08:07,421 - INFO - Early stopping triggered at epoch 35
2024-12-28 15:08:07,421 - INFO - Training completed in 40.67s
2024-12-28 15:08:07,421 - INFO - Final memory usage: CPU 1924.5 MB, GPU 104.5 MB
2024-12-28 15:08:07,422 - INFO - Model training completed in 40.68s
2024-12-28 15:08:07,485 - INFO - Prediction completed in 0.06s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:08:07,496 - INFO - Poison rate 0.2 completed in 49.05s
2024-12-28 15:08:07,500 - INFO - Loaded 119 existing results
2024-12-28 15:08:07,500 - INFO - Total results to save: 126
2024-12-28 15:08:07,501 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:08:07,509 - INFO - Saved 126 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:08:07,509 - INFO - Total evaluation time: 354.46s
2024-12-28 15:08:07,515 - INFO - 
Progress: 19.8% - Evaluating GTSRB with LogisticRegression (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:08:07,737 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:08:07,737 - INFO - Dataset type: image
2024-12-28 15:08:07,737 - INFO - Sample size: 39209
2024-12-28 15:08:07,737 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:08:07,739 - INFO - Loading datasets...
2024-12-28 15:08:26,878 - INFO - Dataset loading completed in 19.14s
2024-12-28 15:08:26,878 - INFO - Extracting validation features...
2024-12-28 15:08:26,878 - INFO - Extracting features from 4435 samples...
2024-12-28 15:08:27,654 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:08:27,658 - INFO - Validation feature extraction completed in 0.78s
2024-12-28 15:08:27,659 - INFO - Extracting training features...
2024-12-28 15:08:27,659 - INFO - Extracting features from 19755 samples...
2024-12-28 15:08:30,402 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:08:30,409 - INFO - Training feature extraction completed in 2.75s
2024-12-28 15:08:30,410 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 15:08:30,410 - INFO - Using device: cuda
2024-12-28 15:08:30,411 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:08:30,411 - INFO - Training set processing completed in 0.00s
2024-12-28 15:08:30,411 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:08:30,413 - INFO - Memory usage at start_fit: CPU 1924.9 MB, GPU 103.4 MB
2024-12-28 15:08:30,413 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:08:30,421 - INFO - Number of unique classes: 43
2024-12-28 15:08:30,568 - INFO - Fitted scaler and transformed data
2024-12-28 15:08:30,569 - INFO - Scaling time: 0.15s
2024-12-28 15:08:31,430 - INFO - Epoch 1/1000, Train Loss: 1.9012, Val Loss: 1.3917
2024-12-28 15:08:32,314 - INFO - Epoch 2/1000, Train Loss: 1.2002, Val Loss: 1.1449
2024-12-28 15:08:33,062 - INFO - Epoch 3/1000, Train Loss: 1.0127, Val Loss: 1.0384
2024-12-28 15:08:33,938 - INFO - Epoch 4/1000, Train Loss: 0.9169, Val Loss: 0.9652
2024-12-28 15:08:34,735 - INFO - Epoch 5/1000, Train Loss: 0.8581, Val Loss: 0.9189
2024-12-28 15:08:35,618 - INFO - Epoch 6/1000, Train Loss: 0.8189, Val Loss: 0.8924
2024-12-28 15:08:36,317 - INFO - Epoch 7/1000, Train Loss: 0.7947, Val Loss: 0.8749
2024-12-28 15:08:36,807 - INFO - Epoch 8/1000, Train Loss: 0.7743, Val Loss: 0.8614
2024-12-28 15:08:37,264 - INFO - Epoch 9/1000, Train Loss: 0.7617, Val Loss: 0.8550
2024-12-28 15:08:37,744 - INFO - Epoch 10/1000, Train Loss: 0.7520, Val Loss: 0.8447
2024-12-28 15:08:38,200 - INFO - Epoch 11/1000, Train Loss: 0.7427, Val Loss: 0.8338
2024-12-28 15:08:38,685 - INFO - Epoch 12/1000, Train Loss: 0.7367, Val Loss: 0.8318
2024-12-28 15:08:39,180 - INFO - Epoch 13/1000, Train Loss: 0.7308, Val Loss: 0.8328
2024-12-28 15:08:39,682 - INFO - Epoch 14/1000, Train Loss: 0.7284, Val Loss: 0.8319
2024-12-28 15:08:40,140 - INFO - Epoch 15/1000, Train Loss: 0.7240, Val Loss: 0.8263
2024-12-28 15:08:40,598 - INFO - Epoch 16/1000, Train Loss: 0.7230, Val Loss: 0.8220
2024-12-28 15:08:41,083 - INFO - Epoch 17/1000, Train Loss: 0.7203, Val Loss: 0.8176
2024-12-28 15:08:41,548 - INFO - Epoch 18/1000, Train Loss: 0.7182, Val Loss: 0.8052
2024-12-28 15:08:42,020 - INFO - Epoch 19/1000, Train Loss: 0.7171, Val Loss: 0.8146
2024-12-28 15:08:42,475 - INFO - Epoch 20/1000, Train Loss: 0.7177, Val Loss: 0.8092
2024-12-28 15:08:42,938 - INFO - Epoch 21/1000, Train Loss: 0.7147, Val Loss: 0.8186
2024-12-28 15:08:43,410 - INFO - Epoch 22/1000, Train Loss: 0.7135, Val Loss: 0.8205
2024-12-28 15:08:43,903 - INFO - Epoch 23/1000, Train Loss: 0.7146, Val Loss: 0.8136
2024-12-28 15:08:43,903 - INFO - Early stopping triggered at epoch 23
2024-12-28 15:08:43,903 - INFO - Training completed in 13.49s
2024-12-28 15:08:43,904 - INFO - Final memory usage: CPU 1905.5 MB, GPU 103.8 MB
2024-12-28 15:08:43,905 - INFO - Model training completed in 13.49s
2024-12-28 15:08:43,985 - INFO - Prediction completed in 0.08s
2024-12-28 15:08:43,996 - INFO - Poison rate 0.0 completed in 13.59s
2024-12-28 15:08:43,996 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:08:43,998 - INFO - Label flipping details:
2024-12-28 15:08:43,998 - INFO - - Source class: 1
2024-12-28 15:08:43,998 - INFO - - Target class: 0
2024-12-28 15:08:43,998 - INFO - - Available samples in source class: 918
2024-12-28 15:08:43,998 - INFO - - Requested samples to poison: 197
2024-12-28 15:08:43,998 - INFO - - Actual samples to flip: 197
2024-12-28 15:08:43,998 - INFO - - Samples remaining in source class: 721
2024-12-28 15:08:43,998 - INFO - Successfully flipped 197 labels from class 1 to 0
2024-12-28 15:08:43,998 - INFO - Total number of labels flipped: 197
2024-12-28 15:08:43,998 - INFO - Label flipping completed in 0.00s
2024-12-28 15:08:43,998 - INFO - Training set processing completed in 0.00s
2024-12-28 15:08:43,998 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:08:43,999 - INFO - Memory usage at start_fit: CPU 1866.9 MB, GPU 103.7 MB
2024-12-28 15:08:43,999 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:08:44,013 - INFO - Number of unique classes: 43
2024-12-28 15:08:44,162 - INFO - Fitted scaler and transformed data
2024-12-28 15:08:44,163 - INFO - Scaling time: 0.15s
2024-12-28 15:08:44,662 - INFO - Epoch 1/1000, Train Loss: 1.9309, Val Loss: 1.4012
2024-12-28 15:08:45,128 - INFO - Epoch 2/1000, Train Loss: 1.2270, Val Loss: 1.1371
2024-12-28 15:08:45,584 - INFO - Epoch 3/1000, Train Loss: 1.0407, Val Loss: 1.0282
2024-12-28 15:08:46,069 - INFO - Epoch 4/1000, Train Loss: 0.9440, Val Loss: 0.9573
2024-12-28 15:08:46,536 - INFO - Epoch 5/1000, Train Loss: 0.8859, Val Loss: 0.9185
2024-12-28 15:08:46,986 - INFO - Epoch 6/1000, Train Loss: 0.8457, Val Loss: 0.8999
2024-12-28 15:08:47,454 - INFO - Epoch 7/1000, Train Loss: 0.8223, Val Loss: 0.8713
2024-12-28 15:08:47,940 - INFO - Epoch 8/1000, Train Loss: 0.8024, Val Loss: 0.8533
2024-12-28 15:08:48,402 - INFO - Epoch 9/1000, Train Loss: 0.7862, Val Loss: 0.8490
2024-12-28 15:08:48,835 - INFO - Epoch 10/1000, Train Loss: 0.7773, Val Loss: 0.8361
2024-12-28 15:08:49,295 - INFO - Epoch 11/1000, Train Loss: 0.7685, Val Loss: 0.8370
2024-12-28 15:08:49,783 - INFO - Epoch 12/1000, Train Loss: 0.7636, Val Loss: 0.8284
2024-12-28 15:08:50,281 - INFO - Epoch 13/1000, Train Loss: 0.7569, Val Loss: 0.8297
2024-12-28 15:08:50,754 - INFO - Epoch 14/1000, Train Loss: 0.7538, Val Loss: 0.8195
2024-12-28 15:08:51,231 - INFO - Epoch 15/1000, Train Loss: 0.7499, Val Loss: 0.8271
2024-12-28 15:08:51,694 - INFO - Epoch 16/1000, Train Loss: 0.7469, Val Loss: 0.8100
2024-12-28 15:08:52,146 - INFO - Epoch 17/1000, Train Loss: 0.7443, Val Loss: 0.8170
2024-12-28 15:08:52,600 - INFO - Epoch 18/1000, Train Loss: 0.7445, Val Loss: 0.8151
2024-12-28 15:08:53,053 - INFO - Epoch 19/1000, Train Loss: 0.7425, Val Loss: 0.8162
2024-12-28 15:08:53,536 - INFO - Epoch 20/1000, Train Loss: 0.7418, Val Loss: 0.8237
2024-12-28 15:08:53,996 - INFO - Epoch 21/1000, Train Loss: 0.7397, Val Loss: 0.8076
2024-12-28 15:08:54,457 - INFO - Epoch 22/1000, Train Loss: 0.7406, Val Loss: 0.8132
2024-12-28 15:08:54,978 - INFO - Epoch 23/1000, Train Loss: 0.7384, Val Loss: 0.8203
2024-12-28 15:08:55,469 - INFO - Epoch 24/1000, Train Loss: 0.7387, Val Loss: 0.8130
2024-12-28 15:08:56,000 - INFO - Epoch 25/1000, Train Loss: 0.7391, Val Loss: 0.8111
2024-12-28 15:08:56,576 - INFO - Epoch 26/1000, Train Loss: 0.7371, Val Loss: 0.8122
2024-12-28 15:08:56,576 - INFO - Early stopping triggered at epoch 26
2024-12-28 15:08:56,576 - INFO - Training completed in 12.58s
2024-12-28 15:08:56,576 - INFO - Final memory usage: CPU 1915.0 MB, GPU 103.8 MB
2024-12-28 15:08:56,577 - INFO - Model training completed in 12.58s
2024-12-28 15:08:56,639 - INFO - Prediction completed in 0.06s
2024-12-28 15:08:56,650 - INFO - Poison rate 0.01 completed in 12.65s
2024-12-28 15:08:56,650 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:08:56,652 - INFO - Label flipping details:
2024-12-28 15:08:56,652 - INFO - - Source class: 1
2024-12-28 15:08:56,652 - INFO - - Target class: 0
2024-12-28 15:08:56,652 - INFO - - Available samples in source class: 918
2024-12-28 15:08:56,652 - INFO - - Requested samples to poison: 592
2024-12-28 15:08:56,652 - INFO - - Actual samples to flip: 592
2024-12-28 15:08:56,652 - INFO - - Samples remaining in source class: 326
2024-12-28 15:08:56,652 - INFO - Successfully flipped 592 labels from class 1 to 0
2024-12-28 15:08:56,652 - INFO - Total number of labels flipped: 592
2024-12-28 15:08:56,652 - INFO - Label flipping completed in 0.00s
2024-12-28 15:08:56,652 - INFO - Training set processing completed in 0.00s
2024-12-28 15:08:56,652 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:08:56,653 - INFO - Memory usage at start_fit: CPU 1876.4 MB, GPU 103.7 MB
2024-12-28 15:08:56,653 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:08:56,660 - INFO - Number of unique classes: 43
2024-12-28 15:08:56,810 - INFO - Fitted scaler and transformed data
2024-12-28 15:08:56,811 - INFO - Scaling time: 0.15s
2024-12-28 15:08:57,488 - INFO - Epoch 1/1000, Train Loss: 1.9120, Val Loss: 1.3870
2024-12-28 15:08:58,367 - INFO - Epoch 2/1000, Train Loss: 1.2229, Val Loss: 1.1324
2024-12-28 15:08:59,124 - INFO - Epoch 3/1000, Train Loss: 1.0395, Val Loss: 1.0042
2024-12-28 15:08:59,881 - INFO - Epoch 4/1000, Train Loss: 0.9463, Val Loss: 0.9538
2024-12-28 15:09:00,365 - INFO - Epoch 5/1000, Train Loss: 0.8876, Val Loss: 0.8977
2024-12-28 15:09:00,900 - INFO - Epoch 6/1000, Train Loss: 0.8508, Val Loss: 0.8821
2024-12-28 15:09:01,394 - INFO - Epoch 7/1000, Train Loss: 0.8244, Val Loss: 0.8638
2024-12-28 15:09:01,863 - INFO - Epoch 8/1000, Train Loss: 0.8051, Val Loss: 0.8491
2024-12-28 15:09:02,312 - INFO - Epoch 9/1000, Train Loss: 0.7920, Val Loss: 0.8457
2024-12-28 15:09:02,854 - INFO - Epoch 10/1000, Train Loss: 0.7831, Val Loss: 0.8266
2024-12-28 15:09:03,496 - INFO - Epoch 11/1000, Train Loss: 0.7737, Val Loss: 0.8180
2024-12-28 15:09:03,958 - INFO - Epoch 12/1000, Train Loss: 0.7689, Val Loss: 0.8193
2024-12-28 15:09:04,421 - INFO - Epoch 13/1000, Train Loss: 0.7638, Val Loss: 0.8172
2024-12-28 15:09:04,905 - INFO - Epoch 14/1000, Train Loss: 0.7597, Val Loss: 0.8182
2024-12-28 15:09:05,425 - INFO - Epoch 15/1000, Train Loss: 0.7554, Val Loss: 0.8116
2024-12-28 15:09:05,876 - INFO - Epoch 16/1000, Train Loss: 0.7539, Val Loss: 0.8091
2024-12-28 15:09:06,292 - INFO - Epoch 17/1000, Train Loss: 0.7527, Val Loss: 0.8233
2024-12-28 15:09:06,740 - INFO - Epoch 18/1000, Train Loss: 0.7493, Val Loss: 0.8005
2024-12-28 15:09:07,173 - INFO - Epoch 19/1000, Train Loss: 0.7473, Val Loss: 0.8077
2024-12-28 15:09:07,645 - INFO - Epoch 20/1000, Train Loss: 0.7476, Val Loss: 0.8005
2024-12-28 15:09:08,079 - INFO - Epoch 21/1000, Train Loss: 0.7477, Val Loss: 0.8014
2024-12-28 15:09:08,529 - INFO - Epoch 22/1000, Train Loss: 0.7465, Val Loss: 0.8026
2024-12-28 15:09:08,972 - INFO - Epoch 23/1000, Train Loss: 0.7434, Val Loss: 0.8043
2024-12-28 15:09:08,972 - INFO - Early stopping triggered at epoch 23
2024-12-28 15:09:08,972 - INFO - Training completed in 12.32s
2024-12-28 15:09:08,973 - INFO - Final memory usage: CPU 1915.0 MB, GPU 103.8 MB
2024-12-28 15:09:08,974 - INFO - Model training completed in 12.32s
2024-12-28 15:09:09,076 - INFO - Prediction completed in 0.10s
2024-12-28 15:09:09,086 - INFO - Poison rate 0.03 completed in 12.44s
2024-12-28 15:09:09,086 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:09:09,088 - INFO - Label flipping details:
2024-12-28 15:09:09,088 - INFO - - Source class: 1
2024-12-28 15:09:09,088 - INFO - - Target class: 0
2024-12-28 15:09:09,088 - INFO - - Available samples in source class: 918
2024-12-28 15:09:09,088 - INFO - - Requested samples to poison: 987
2024-12-28 15:09:09,088 - INFO - - Actual samples to flip: 917
2024-12-28 15:09:09,088 - INFO - - Samples remaining in source class: 1
2024-12-28 15:09:09,088 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:09:09,088 - INFO - Total number of labels flipped: 917
2024-12-28 15:09:09,088 - INFO - Label flipping completed in 0.00s
2024-12-28 15:09:09,089 - INFO - Training set processing completed in 0.00s
2024-12-28 15:09:09,089 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:09:09,089 - INFO - Memory usage at start_fit: CPU 1876.4 MB, GPU 103.7 MB
2024-12-28 15:09:09,090 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:09:09,098 - INFO - Number of unique classes: 43
2024-12-28 15:09:09,258 - INFO - Fitted scaler and transformed data
2024-12-28 15:09:09,259 - INFO - Scaling time: 0.16s
2024-12-28 15:09:09,725 - INFO - Epoch 1/1000, Train Loss: 1.9099, Val Loss: 1.3757
2024-12-28 15:09:10,163 - INFO - Epoch 2/1000, Train Loss: 1.1963, Val Loss: 1.1109
2024-12-28 15:09:10,637 - INFO - Epoch 3/1000, Train Loss: 1.0078, Val Loss: 1.0000
2024-12-28 15:09:11,069 - INFO - Epoch 4/1000, Train Loss: 0.9117, Val Loss: 0.9298
2024-12-28 15:09:11,513 - INFO - Epoch 5/1000, Train Loss: 0.8553, Val Loss: 0.8879
2024-12-28 15:09:11,957 - INFO - Epoch 6/1000, Train Loss: 0.8152, Val Loss: 0.8723
2024-12-28 15:09:12,401 - INFO - Epoch 7/1000, Train Loss: 0.7903, Val Loss: 0.8412
2024-12-28 15:09:12,861 - INFO - Epoch 8/1000, Train Loss: 0.7710, Val Loss: 0.8282
2024-12-28 15:09:13,335 - INFO - Epoch 9/1000, Train Loss: 0.7573, Val Loss: 0.8243
2024-12-28 15:09:13,792 - INFO - Epoch 10/1000, Train Loss: 0.7452, Val Loss: 0.8072
2024-12-28 15:09:14,255 - INFO - Epoch 11/1000, Train Loss: 0.7387, Val Loss: 0.8120
2024-12-28 15:09:14,702 - INFO - Epoch 12/1000, Train Loss: 0.7322, Val Loss: 0.7957
2024-12-28 15:09:15,160 - INFO - Epoch 13/1000, Train Loss: 0.7268, Val Loss: 0.7994
2024-12-28 15:09:15,611 - INFO - Epoch 14/1000, Train Loss: 0.7249, Val Loss: 0.7924
2024-12-28 15:09:16,063 - INFO - Epoch 15/1000, Train Loss: 0.7194, Val Loss: 0.7926
2024-12-28 15:09:16,569 - INFO - Epoch 16/1000, Train Loss: 0.7168, Val Loss: 0.7867
2024-12-28 15:09:17,056 - INFO - Epoch 17/1000, Train Loss: 0.7140, Val Loss: 0.7928
2024-12-28 15:09:17,581 - INFO - Epoch 18/1000, Train Loss: 0.7131, Val Loss: 0.7861
2024-12-28 15:09:18,090 - INFO - Epoch 19/1000, Train Loss: 0.7134, Val Loss: 0.7946
2024-12-28 15:09:18,554 - INFO - Epoch 20/1000, Train Loss: 0.7112, Val Loss: 0.7891
2024-12-28 15:09:19,080 - INFO - Epoch 21/1000, Train Loss: 0.7114, Val Loss: 0.7859
2024-12-28 15:09:19,080 - INFO - Early stopping triggered at epoch 21
2024-12-28 15:09:19,081 - INFO - Training completed in 9.99s
2024-12-28 15:09:19,081 - INFO - Final memory usage: CPU 1915.0 MB, GPU 103.8 MB
2024-12-28 15:09:19,082 - INFO - Model training completed in 9.99s
2024-12-28 15:09:19,148 - INFO - Prediction completed in 0.07s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:09:19,159 - INFO - Poison rate 0.05 completed in 10.07s
2024-12-28 15:09:19,159 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:09:19,160 - INFO - Label flipping details:
2024-12-28 15:09:19,161 - INFO - - Source class: 1
2024-12-28 15:09:19,161 - INFO - - Target class: 0
2024-12-28 15:09:19,161 - INFO - - Available samples in source class: 918
2024-12-28 15:09:19,161 - INFO - - Requested samples to poison: 1382
2024-12-28 15:09:19,161 - INFO - - Actual samples to flip: 917
2024-12-28 15:09:19,161 - INFO - - Samples remaining in source class: 1
2024-12-28 15:09:19,161 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:09:19,161 - INFO - Total number of labels flipped: 917
2024-12-28 15:09:19,161 - INFO - Label flipping completed in 0.00s
2024-12-28 15:09:19,161 - INFO - Training set processing completed in 0.00s
2024-12-28 15:09:19,161 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:09:19,162 - INFO - Memory usage at start_fit: CPU 1876.4 MB, GPU 103.7 MB
2024-12-28 15:09:19,162 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:09:19,170 - INFO - Number of unique classes: 43
2024-12-28 15:09:19,358 - INFO - Fitted scaler and transformed data
2024-12-28 15:09:19,358 - INFO - Scaling time: 0.19s
2024-12-28 15:09:19,830 - INFO - Epoch 1/1000, Train Loss: 1.8884, Val Loss: 1.3876
2024-12-28 15:09:20,294 - INFO - Epoch 2/1000, Train Loss: 1.1862, Val Loss: 1.1212
2024-12-28 15:09:20,730 - INFO - Epoch 3/1000, Train Loss: 1.0050, Val Loss: 1.0112
2024-12-28 15:09:21,169 - INFO - Epoch 4/1000, Train Loss: 0.9097, Val Loss: 0.9439
2024-12-28 15:09:21,622 - INFO - Epoch 5/1000, Train Loss: 0.8517, Val Loss: 0.9027
2024-12-28 15:09:22,069 - INFO - Epoch 6/1000, Train Loss: 0.8158, Val Loss: 0.8711
2024-12-28 15:09:22,524 - INFO - Epoch 7/1000, Train Loss: 0.7891, Val Loss: 0.8527
2024-12-28 15:09:22,947 - INFO - Epoch 8/1000, Train Loss: 0.7721, Val Loss: 0.8395
2024-12-28 15:09:23,419 - INFO - Epoch 9/1000, Train Loss: 0.7563, Val Loss: 0.8223
2024-12-28 15:09:23,850 - INFO - Epoch 10/1000, Train Loss: 0.7462, Val Loss: 0.8254
2024-12-28 15:09:24,294 - INFO - Epoch 11/1000, Train Loss: 0.7376, Val Loss: 0.7983
2024-12-28 15:09:24,767 - INFO - Epoch 12/1000, Train Loss: 0.7323, Val Loss: 0.7976
2024-12-28 15:09:25,215 - INFO - Epoch 13/1000, Train Loss: 0.7270, Val Loss: 0.8075
2024-12-28 15:09:25,641 - INFO - Epoch 14/1000, Train Loss: 0.7242, Val Loss: 0.7973
2024-12-28 15:09:26,114 - INFO - Epoch 15/1000, Train Loss: 0.7217, Val Loss: 0.7944
2024-12-28 15:09:26,594 - INFO - Epoch 16/1000, Train Loss: 0.7196, Val Loss: 0.7951
2024-12-28 15:09:27,034 - INFO - Epoch 17/1000, Train Loss: 0.7147, Val Loss: 0.7904
2024-12-28 15:09:27,466 - INFO - Epoch 18/1000, Train Loss: 0.7142, Val Loss: 0.7936
2024-12-28 15:09:27,886 - INFO - Epoch 19/1000, Train Loss: 0.7126, Val Loss: 0.7892
2024-12-28 15:09:28,338 - INFO - Epoch 20/1000, Train Loss: 0.7117, Val Loss: 0.7832
2024-12-28 15:09:28,776 - INFO - Epoch 21/1000, Train Loss: 0.7111, Val Loss: 0.7834
2024-12-28 15:09:29,217 - INFO - Epoch 22/1000, Train Loss: 0.7107, Val Loss: 0.7887
2024-12-28 15:09:29,655 - INFO - Epoch 23/1000, Train Loss: 0.7093, Val Loss: 0.7817
2024-12-28 15:09:30,088 - INFO - Epoch 24/1000, Train Loss: 0.7099, Val Loss: 0.7881
2024-12-28 15:09:30,521 - INFO - Epoch 25/1000, Train Loss: 0.7071, Val Loss: 0.7803
2024-12-28 15:09:30,981 - INFO - Epoch 26/1000, Train Loss: 0.7078, Val Loss: 0.7776
2024-12-28 15:09:31,420 - INFO - Epoch 27/1000, Train Loss: 0.7085, Val Loss: 0.7883
2024-12-28 15:09:31,848 - INFO - Epoch 28/1000, Train Loss: 0.7082, Val Loss: 0.7882
2024-12-28 15:09:32,315 - INFO - Epoch 29/1000, Train Loss: 0.7062, Val Loss: 0.7826
2024-12-28 15:09:32,771 - INFO - Epoch 30/1000, Train Loss: 0.7070, Val Loss: 0.7827
2024-12-28 15:09:33,205 - INFO - Epoch 31/1000, Train Loss: 0.7064, Val Loss: 0.7748
2024-12-28 15:09:33,624 - INFO - Epoch 32/1000, Train Loss: 0.7082, Val Loss: 0.7798
2024-12-28 15:09:34,064 - INFO - Epoch 33/1000, Train Loss: 0.7064, Val Loss: 0.7860
2024-12-28 15:09:34,531 - INFO - Epoch 34/1000, Train Loss: 0.7073, Val Loss: 0.7766
2024-12-28 15:09:34,983 - INFO - Epoch 35/1000, Train Loss: 0.7061, Val Loss: 0.7864
2024-12-28 15:09:35,416 - INFO - Epoch 36/1000, Train Loss: 0.7069, Val Loss: 0.7807
2024-12-28 15:09:35,417 - INFO - Early stopping triggered at epoch 36
2024-12-28 15:09:35,417 - INFO - Training completed in 16.26s
2024-12-28 15:09:35,417 - INFO - Final memory usage: CPU 1915.0 MB, GPU 103.8 MB
2024-12-28 15:09:35,418 - INFO - Model training completed in 16.26s
2024-12-28 15:09:35,485 - INFO - Prediction completed in 0.07s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:09:35,509 - INFO - Poison rate 0.07 completed in 16.35s
2024-12-28 15:09:35,509 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:09:35,510 - INFO - Label flipping details:
2024-12-28 15:09:35,510 - INFO - - Source class: 1
2024-12-28 15:09:35,510 - INFO - - Target class: 0
2024-12-28 15:09:35,511 - INFO - - Available samples in source class: 918
2024-12-28 15:09:35,511 - INFO - - Requested samples to poison: 1975
2024-12-28 15:09:35,511 - INFO - - Actual samples to flip: 917
2024-12-28 15:09:35,511 - INFO - - Samples remaining in source class: 1
2024-12-28 15:09:35,511 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:09:35,511 - INFO - Total number of labels flipped: 917
2024-12-28 15:09:35,511 - INFO - Label flipping completed in 0.00s
2024-12-28 15:09:35,511 - INFO - Training set processing completed in 0.00s
2024-12-28 15:09:35,511 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:09:35,512 - INFO - Memory usage at start_fit: CPU 1876.4 MB, GPU 103.7 MB
2024-12-28 15:09:35,512 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:09:35,518 - INFO - Number of unique classes: 43
2024-12-28 15:09:35,665 - INFO - Fitted scaler and transformed data
2024-12-28 15:09:35,666 - INFO - Scaling time: 0.15s
2024-12-28 15:09:36,132 - INFO - Epoch 1/1000, Train Loss: 1.8878, Val Loss: 1.3752
2024-12-28 15:09:36,558 - INFO - Epoch 2/1000, Train Loss: 1.1880, Val Loss: 1.1184
2024-12-28 15:09:37,013 - INFO - Epoch 3/1000, Train Loss: 1.0036, Val Loss: 1.0067
2024-12-28 15:09:37,478 - INFO - Epoch 4/1000, Train Loss: 0.9093, Val Loss: 0.9449
2024-12-28 15:09:37,989 - INFO - Epoch 5/1000, Train Loss: 0.8520, Val Loss: 0.9080
2024-12-28 15:09:38,490 - INFO - Epoch 6/1000, Train Loss: 0.8132, Val Loss: 0.8723
2024-12-28 15:09:38,987 - INFO - Epoch 7/1000, Train Loss: 0.7891, Val Loss: 0.8488
2024-12-28 15:09:39,487 - INFO - Epoch 8/1000, Train Loss: 0.7690, Val Loss: 0.8367
2024-12-28 15:09:39,973 - INFO - Epoch 9/1000, Train Loss: 0.7553, Val Loss: 0.8315
2024-12-28 15:09:40,485 - INFO - Epoch 10/1000, Train Loss: 0.7441, Val Loss: 0.8207
2024-12-28 15:09:40,995 - INFO - Epoch 11/1000, Train Loss: 0.7357, Val Loss: 0.8215
2024-12-28 15:09:41,451 - INFO - Epoch 12/1000, Train Loss: 0.7312, Val Loss: 0.8221
2024-12-28 15:09:41,931 - INFO - Epoch 13/1000, Train Loss: 0.7265, Val Loss: 0.8028
2024-12-28 15:09:42,380 - INFO - Epoch 14/1000, Train Loss: 0.7219, Val Loss: 0.7974
2024-12-28 15:09:42,840 - INFO - Epoch 15/1000, Train Loss: 0.7183, Val Loss: 0.8007
2024-12-28 15:09:43,289 - INFO - Epoch 16/1000, Train Loss: 0.7149, Val Loss: 0.8067
2024-12-28 15:09:43,787 - INFO - Epoch 17/1000, Train Loss: 0.7148, Val Loss: 0.7999
2024-12-28 15:09:44,277 - INFO - Epoch 18/1000, Train Loss: 0.7114, Val Loss: 0.7932
2024-12-28 15:09:44,794 - INFO - Epoch 19/1000, Train Loss: 0.7119, Val Loss: 0.7931
2024-12-28 15:09:45,266 - INFO - Epoch 20/1000, Train Loss: 0.7094, Val Loss: 0.7989
2024-12-28 15:09:45,730 - INFO - Epoch 21/1000, Train Loss: 0.7089, Val Loss: 0.7952
2024-12-28 15:09:46,214 - INFO - Epoch 22/1000, Train Loss: 0.7084, Val Loss: 0.7960
2024-12-28 15:09:46,671 - INFO - Epoch 23/1000, Train Loss: 0.7071, Val Loss: 0.7938
2024-12-28 15:09:46,671 - INFO - Early stopping triggered at epoch 23
2024-12-28 15:09:46,671 - INFO - Training completed in 11.16s
2024-12-28 15:09:46,672 - INFO - Final memory usage: CPU 1915.0 MB, GPU 103.8 MB
2024-12-28 15:09:46,672 - INFO - Model training completed in 11.16s
2024-12-28 15:09:46,762 - INFO - Prediction completed in 0.09s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:09:46,777 - INFO - Poison rate 0.1 completed in 11.27s
2024-12-28 15:09:46,778 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:09:46,779 - INFO - Label flipping details:
2024-12-28 15:09:46,780 - INFO - - Source class: 1
2024-12-28 15:09:46,780 - INFO - - Target class: 0
2024-12-28 15:09:46,780 - INFO - - Available samples in source class: 918
2024-12-28 15:09:46,780 - INFO - - Requested samples to poison: 3951
2024-12-28 15:09:46,780 - INFO - - Actual samples to flip: 917
2024-12-28 15:09:46,780 - INFO - - Samples remaining in source class: 1
2024-12-28 15:09:46,780 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:09:46,780 - INFO - Total number of labels flipped: 917
2024-12-28 15:09:46,780 - INFO - Label flipping completed in 0.00s
2024-12-28 15:09:46,780 - INFO - Training set processing completed in 0.00s
2024-12-28 15:09:46,780 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:09:46,781 - INFO - Memory usage at start_fit: CPU 1876.4 MB, GPU 103.7 MB
2024-12-28 15:09:46,781 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:09:46,788 - INFO - Number of unique classes: 43
2024-12-28 15:09:46,937 - INFO - Fitted scaler and transformed data
2024-12-28 15:09:46,938 - INFO - Scaling time: 0.15s
2024-12-28 15:09:47,414 - INFO - Epoch 1/1000, Train Loss: 1.8786, Val Loss: 1.3882
2024-12-28 15:09:47,867 - INFO - Epoch 2/1000, Train Loss: 1.1830, Val Loss: 1.1492
2024-12-28 15:09:48,329 - INFO - Epoch 3/1000, Train Loss: 1.0004, Val Loss: 1.0333
2024-12-28 15:09:48,853 - INFO - Epoch 4/1000, Train Loss: 0.9038, Val Loss: 0.9692
2024-12-28 15:09:49,289 - INFO - Epoch 5/1000, Train Loss: 0.8479, Val Loss: 0.9303
2024-12-28 15:09:49,746 - INFO - Epoch 6/1000, Train Loss: 0.8101, Val Loss: 0.9072
2024-12-28 15:09:50,216 - INFO - Epoch 7/1000, Train Loss: 0.7830, Val Loss: 0.8821
2024-12-28 15:09:50,701 - INFO - Epoch 8/1000, Train Loss: 0.7634, Val Loss: 0.8725
2024-12-28 15:09:51,173 - INFO - Epoch 9/1000, Train Loss: 0.7497, Val Loss: 0.8689
2024-12-28 15:09:51,609 - INFO - Epoch 10/1000, Train Loss: 0.7424, Val Loss: 0.8567
2024-12-28 15:09:52,046 - INFO - Epoch 11/1000, Train Loss: 0.7337, Val Loss: 0.8488
2024-12-28 15:09:52,494 - INFO - Epoch 12/1000, Train Loss: 0.7258, Val Loss: 0.8492
2024-12-28 15:09:52,941 - INFO - Epoch 13/1000, Train Loss: 0.7229, Val Loss: 0.8377
2024-12-28 15:09:53,380 - INFO - Epoch 14/1000, Train Loss: 0.7160, Val Loss: 0.8384
2024-12-28 15:09:53,821 - INFO - Epoch 15/1000, Train Loss: 0.7163, Val Loss: 0.8412
2024-12-28 15:09:54,279 - INFO - Epoch 16/1000, Train Loss: 0.7100, Val Loss: 0.8337
2024-12-28 15:09:54,719 - INFO - Epoch 17/1000, Train Loss: 0.7115, Val Loss: 0.8364
2024-12-28 15:09:55,156 - INFO - Epoch 18/1000, Train Loss: 0.7085, Val Loss: 0.8314
2024-12-28 15:09:55,591 - INFO - Epoch 19/1000, Train Loss: 0.7065, Val Loss: 0.8379
2024-12-28 15:09:56,022 - INFO - Epoch 20/1000, Train Loss: 0.7071, Val Loss: 0.8280
2024-12-28 15:09:56,469 - INFO - Epoch 21/1000, Train Loss: 0.7056, Val Loss: 0.8190
2024-12-28 15:09:56,884 - INFO - Epoch 22/1000, Train Loss: 0.7035, Val Loss: 0.8271
2024-12-28 15:09:57,338 - INFO - Epoch 23/1000, Train Loss: 0.7054, Val Loss: 0.8232
2024-12-28 15:09:57,767 - INFO - Epoch 24/1000, Train Loss: 0.7024, Val Loss: 0.8299
2024-12-28 15:09:58,189 - INFO - Epoch 25/1000, Train Loss: 0.7035, Val Loss: 0.8261
2024-12-28 15:09:58,640 - INFO - Epoch 26/1000, Train Loss: 0.7030, Val Loss: 0.8297
2024-12-28 15:09:58,640 - INFO - Early stopping triggered at epoch 26
2024-12-28 15:09:58,640 - INFO - Training completed in 11.86s
2024-12-28 15:09:58,641 - INFO - Final memory usage: CPU 1915.0 MB, GPU 103.8 MB
2024-12-28 15:09:58,642 - INFO - Model training completed in 11.86s
2024-12-28 15:09:58,724 - INFO - Prediction completed in 0.08s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:09:58,735 - INFO - Poison rate 0.2 completed in 11.96s
2024-12-28 15:09:58,739 - INFO - Loaded 126 existing results
2024-12-28 15:09:58,739 - INFO - Total results to save: 133
2024-12-28 15:09:58,739 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:09:58,748 - INFO - Saved 133 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:09:58,748 - INFO - Total evaluation time: 111.01s
2024-12-28 15:09:58,754 - INFO - 
Progress: 20.8% - Evaluating GTSRB with LogisticRegression (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:09:58,956 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:09:58,957 - INFO - Dataset type: image
2024-12-28 15:09:58,957 - INFO - Sample size: 39209
2024-12-28 15:09:58,957 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:09:58,957 - INFO - Loading datasets...
2024-12-28 15:10:18,000 - INFO - Dataset loading completed in 19.04s
2024-12-28 15:10:18,000 - INFO - Extracting validation features...
2024-12-28 15:10:18,000 - INFO - Extracting features from 4435 samples...
2024-12-28 15:10:18,753 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:10:18,759 - INFO - Validation feature extraction completed in 0.76s
2024-12-28 15:10:18,759 - INFO - Extracting training features...
2024-12-28 15:10:18,759 - INFO - Extracting features from 19755 samples...
2024-12-28 15:10:21,562 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:10:21,573 - INFO - Training feature extraction completed in 2.81s
2024-12-28 15:10:21,574 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 15:10:21,574 - INFO - Using device: cuda
2024-12-28 15:10:21,574 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:10:21,574 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:10:21,574 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:10:22,854 - INFO - Feature scaling completed in 1.28s
2024-12-28 15:10:22,854 - INFO - Starting feature selection (k=50)
2024-12-28 15:10:22,882 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:10:22,883 - INFO - Starting anomaly detection
2024-12-28 15:10:30,557 - INFO - Anomaly detection completed in 7.67s
2024-12-28 15:10:30,557 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:10:30,558 - INFO - Total fit_transform time: 8.98s
2024-12-28 15:10:30,558 - INFO - Training set processing completed in 8.98s
2024-12-28 15:10:30,558 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:10:30,560 - INFO - Memory usage at start_fit: CPU 1886.1 MB, GPU 104.0 MB
2024-12-28 15:10:30,560 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:10:30,569 - INFO - Number of unique classes: 43
2024-12-28 15:10:30,717 - INFO - Fitted scaler and transformed data
2024-12-28 15:10:30,718 - INFO - Scaling time: 0.15s
2024-12-28 15:10:31,134 - INFO - Epoch 1/1000, Train Loss: 1.9032, Val Loss: 1.3723
2024-12-28 15:10:31,577 - INFO - Epoch 2/1000, Train Loss: 1.2006, Val Loss: 1.1148
2024-12-28 15:10:32,009 - INFO - Epoch 3/1000, Train Loss: 1.0129, Val Loss: 1.0020
2024-12-28 15:10:32,456 - INFO - Epoch 4/1000, Train Loss: 0.9180, Val Loss: 0.9358
2024-12-28 15:10:32,888 - INFO - Epoch 5/1000, Train Loss: 0.8591, Val Loss: 0.9006
2024-12-28 15:10:33,292 - INFO - Epoch 6/1000, Train Loss: 0.8230, Val Loss: 0.8750
2024-12-28 15:10:33,719 - INFO - Epoch 7/1000, Train Loss: 0.7952, Val Loss: 0.8519
2024-12-28 15:10:34,151 - INFO - Epoch 8/1000, Train Loss: 0.7763, Val Loss: 0.8417
2024-12-28 15:10:34,613 - INFO - Epoch 9/1000, Train Loss: 0.7632, Val Loss: 0.8272
2024-12-28 15:10:35,051 - INFO - Epoch 10/1000, Train Loss: 0.7551, Val Loss: 0.8182
2024-12-28 15:10:35,503 - INFO - Epoch 11/1000, Train Loss: 0.7444, Val Loss: 0.8117
2024-12-28 15:10:35,955 - INFO - Epoch 12/1000, Train Loss: 0.7389, Val Loss: 0.8130
2024-12-28 15:10:36,384 - INFO - Epoch 13/1000, Train Loss: 0.7326, Val Loss: 0.8034
2024-12-28 15:10:36,813 - INFO - Epoch 14/1000, Train Loss: 0.7296, Val Loss: 0.8048
2024-12-28 15:10:37,248 - INFO - Epoch 15/1000, Train Loss: 0.7273, Val Loss: 0.8022
2024-12-28 15:10:37,698 - INFO - Epoch 16/1000, Train Loss: 0.7247, Val Loss: 0.8035
2024-12-28 15:10:38,126 - INFO - Epoch 17/1000, Train Loss: 0.7230, Val Loss: 0.8028
2024-12-28 15:10:38,552 - INFO - Epoch 18/1000, Train Loss: 0.7215, Val Loss: 0.7937
2024-12-28 15:10:38,980 - INFO - Epoch 19/1000, Train Loss: 0.7201, Val Loss: 0.7942
2024-12-28 15:10:39,432 - INFO - Epoch 20/1000, Train Loss: 0.7188, Val Loss: 0.7982
2024-12-28 15:10:39,872 - INFO - Epoch 21/1000, Train Loss: 0.7179, Val Loss: 0.7939
2024-12-28 15:10:40,277 - INFO - Epoch 22/1000, Train Loss: 0.7163, Val Loss: 0.7916
2024-12-28 15:10:40,709 - INFO - Epoch 23/1000, Train Loss: 0.7155, Val Loss: 0.7958
2024-12-28 15:10:41,156 - INFO - Epoch 24/1000, Train Loss: 0.7151, Val Loss: 0.8013
2024-12-28 15:10:41,587 - INFO - Epoch 25/1000, Train Loss: 0.7160, Val Loss: 0.7913
2024-12-28 15:10:42,014 - INFO - Epoch 26/1000, Train Loss: 0.7171, Val Loss: 0.7898
2024-12-28 15:10:42,445 - INFO - Epoch 27/1000, Train Loss: 0.7154, Val Loss: 0.7869
2024-12-28 15:10:42,855 - INFO - Epoch 28/1000, Train Loss: 0.7142, Val Loss: 0.7895
2024-12-28 15:10:43,295 - INFO - Epoch 29/1000, Train Loss: 0.7154, Val Loss: 0.7842
2024-12-28 15:10:43,709 - INFO - Epoch 30/1000, Train Loss: 0.7131, Val Loss: 0.7810
2024-12-28 15:10:44,143 - INFO - Epoch 31/1000, Train Loss: 0.7132, Val Loss: 0.7901
2024-12-28 15:10:44,573 - INFO - Epoch 32/1000, Train Loss: 0.7133, Val Loss: 0.7928
2024-12-28 15:10:45,018 - INFO - Epoch 33/1000, Train Loss: 0.7129, Val Loss: 0.7975
2024-12-28 15:10:45,455 - INFO - Epoch 34/1000, Train Loss: 0.7139, Val Loss: 0.7855
2024-12-28 15:10:45,877 - INFO - Epoch 35/1000, Train Loss: 0.7138, Val Loss: 0.7876
2024-12-28 15:10:45,877 - INFO - Early stopping triggered at epoch 35
2024-12-28 15:10:45,877 - INFO - Training completed in 15.32s
2024-12-28 15:10:45,878 - INFO - Final memory usage: CPU 1924.7 MB, GPU 104.5 MB
2024-12-28 15:10:45,878 - INFO - Model training completed in 15.32s
2024-12-28 15:10:45,973 - INFO - Prediction completed in 0.09s
2024-12-28 15:10:45,986 - INFO - Poison rate 0.0 completed in 24.41s
2024-12-28 15:10:45,986 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:10:45,988 - INFO - Label flipping details:
2024-12-28 15:10:45,989 - INFO - - Source class: 1
2024-12-28 15:10:45,989 - INFO - - Target class: 0
2024-12-28 15:10:45,989 - INFO - - Available samples in source class: 919
2024-12-28 15:10:45,989 - INFO - - Requested samples to poison: 197
2024-12-28 15:10:45,989 - INFO - - Actual samples to flip: 197
2024-12-28 15:10:45,989 - INFO - - Samples remaining in source class: 722
2024-12-28 15:10:45,989 - INFO - Successfully flipped 197 labels from class 1 to 0
2024-12-28 15:10:45,990 - INFO - Total number of labels flipped: 197
2024-12-28 15:10:45,990 - INFO - Label flipping completed in 0.00s
2024-12-28 15:10:45,990 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:10:45,990 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:10:47,256 - INFO - Feature scaling completed in 1.27s
2024-12-28 15:10:47,256 - INFO - Starting feature selection (k=50)
2024-12-28 15:10:47,289 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:10:47,289 - INFO - Starting anomaly detection
2024-12-28 15:10:55,400 - INFO - Anomaly detection completed in 8.11s
2024-12-28 15:10:55,400 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:10:55,401 - INFO - Total fit_transform time: 9.41s
2024-12-28 15:10:55,401 - INFO - Training set processing completed in 9.41s
2024-12-28 15:10:55,401 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:10:55,402 - INFO - Memory usage at start_fit: CPU 1886.1 MB, GPU 104.3 MB
2024-12-28 15:10:55,402 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:10:55,413 - INFO - Number of unique classes: 43
2024-12-28 15:10:55,564 - INFO - Fitted scaler and transformed data
2024-12-28 15:10:55,564 - INFO - Scaling time: 0.15s
2024-12-28 15:10:55,972 - INFO - Epoch 1/1000, Train Loss: 1.9230, Val Loss: 1.3759
2024-12-28 15:10:56,371 - INFO - Epoch 2/1000, Train Loss: 1.2204, Val Loss: 1.1292
2024-12-28 15:10:56,818 - INFO - Epoch 3/1000, Train Loss: 1.0371, Val Loss: 1.0199
2024-12-28 15:10:57,262 - INFO - Epoch 4/1000, Train Loss: 0.9400, Val Loss: 0.9516
2024-12-28 15:10:57,668 - INFO - Epoch 5/1000, Train Loss: 0.8831, Val Loss: 0.9173
2024-12-28 15:10:58,095 - INFO - Epoch 6/1000, Train Loss: 0.8461, Val Loss: 0.8867
2024-12-28 15:10:58,529 - INFO - Epoch 7/1000, Train Loss: 0.8195, Val Loss: 0.8778
2024-12-28 15:10:58,952 - INFO - Epoch 8/1000, Train Loss: 0.8016, Val Loss: 0.8549
2024-12-28 15:10:59,393 - INFO - Epoch 9/1000, Train Loss: 0.7861, Val Loss: 0.8501
2024-12-28 15:10:59,851 - INFO - Epoch 10/1000, Train Loss: 0.7769, Val Loss: 0.8420
2024-12-28 15:11:00,313 - INFO - Epoch 11/1000, Train Loss: 0.7688, Val Loss: 0.8360
2024-12-28 15:11:00,779 - INFO - Epoch 12/1000, Train Loss: 0.7626, Val Loss: 0.8282
2024-12-28 15:11:01,250 - INFO - Epoch 13/1000, Train Loss: 0.7559, Val Loss: 0.8218
2024-12-28 15:11:01,713 - INFO - Epoch 14/1000, Train Loss: 0.7516, Val Loss: 0.8253
2024-12-28 15:11:02,172 - INFO - Epoch 15/1000, Train Loss: 0.7514, Val Loss: 0.8169
2024-12-28 15:11:02,602 - INFO - Epoch 16/1000, Train Loss: 0.7489, Val Loss: 0.8201
2024-12-28 15:11:03,072 - INFO - Epoch 17/1000, Train Loss: 0.7461, Val Loss: 0.8145
2024-12-28 15:11:03,503 - INFO - Epoch 18/1000, Train Loss: 0.7452, Val Loss: 0.8174
2024-12-28 15:11:03,942 - INFO - Epoch 19/1000, Train Loss: 0.7423, Val Loss: 0.8088
2024-12-28 15:11:04,360 - INFO - Epoch 20/1000, Train Loss: 0.7420, Val Loss: 0.8097
2024-12-28 15:11:04,804 - INFO - Epoch 21/1000, Train Loss: 0.7391, Val Loss: 0.8113
2024-12-28 15:11:05,275 - INFO - Epoch 22/1000, Train Loss: 0.7401, Val Loss: 0.8086
2024-12-28 15:11:05,742 - INFO - Epoch 23/1000, Train Loss: 0.7390, Val Loss: 0.8070
2024-12-28 15:11:06,191 - INFO - Epoch 24/1000, Train Loss: 0.7376, Val Loss: 0.8088
2024-12-28 15:11:06,625 - INFO - Epoch 25/1000, Train Loss: 0.7386, Val Loss: 0.8076
2024-12-28 15:11:07,086 - INFO - Epoch 26/1000, Train Loss: 0.7376, Val Loss: 0.8109
2024-12-28 15:11:07,523 - INFO - Epoch 27/1000, Train Loss: 0.7366, Val Loss: 0.8119
2024-12-28 15:11:07,971 - INFO - Epoch 28/1000, Train Loss: 0.7374, Val Loss: 0.8127
2024-12-28 15:11:07,972 - INFO - Early stopping triggered at epoch 28
2024-12-28 15:11:07,972 - INFO - Training completed in 12.57s
2024-12-28 15:11:07,972 - INFO - Final memory usage: CPU 1924.7 MB, GPU 104.5 MB
2024-12-28 15:11:07,972 - INFO - Model training completed in 12.57s
2024-12-28 15:11:08,034 - INFO - Prediction completed in 0.06s
2024-12-28 15:11:08,056 - INFO - Poison rate 0.01 completed in 22.07s
2024-12-28 15:11:08,057 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:11:08,058 - INFO - Label flipping details:
2024-12-28 15:11:08,058 - INFO - - Source class: 1
2024-12-28 15:11:08,058 - INFO - - Target class: 0
2024-12-28 15:11:08,058 - INFO - - Available samples in source class: 919
2024-12-28 15:11:08,058 - INFO - - Requested samples to poison: 592
2024-12-28 15:11:08,058 - INFO - - Actual samples to flip: 592
2024-12-28 15:11:08,058 - INFO - - Samples remaining in source class: 327
2024-12-28 15:11:08,058 - INFO - Successfully flipped 592 labels from class 1 to 0
2024-12-28 15:11:08,058 - INFO - Total number of labels flipped: 592
2024-12-28 15:11:08,059 - INFO - Label flipping completed in 0.00s
2024-12-28 15:11:08,059 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:11:08,059 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:11:09,297 - INFO - Feature scaling completed in 1.24s
2024-12-28 15:11:09,297 - INFO - Starting feature selection (k=50)
2024-12-28 15:11:09,326 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:11:09,326 - INFO - Starting anomaly detection
2024-12-28 15:11:15,801 - INFO - Anomaly detection completed in 6.47s
2024-12-28 15:11:15,801 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:11:15,801 - INFO - Total fit_transform time: 7.74s
2024-12-28 15:11:15,801 - INFO - Training set processing completed in 7.74s
2024-12-28 15:11:15,802 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:11:15,803 - INFO - Memory usage at start_fit: CPU 1886.1 MB, GPU 104.3 MB
2024-12-28 15:11:15,803 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:11:15,810 - INFO - Number of unique classes: 43
2024-12-28 15:11:15,964 - INFO - Fitted scaler and transformed data
2024-12-28 15:11:15,964 - INFO - Scaling time: 0.15s
2024-12-28 15:11:16,452 - INFO - Epoch 1/1000, Train Loss: 1.9093, Val Loss: 1.4183
2024-12-28 15:11:16,864 - INFO - Epoch 2/1000, Train Loss: 1.2215, Val Loss: 1.1486
2024-12-28 15:11:17,244 - INFO - Epoch 3/1000, Train Loss: 1.0398, Val Loss: 1.0404
2024-12-28 15:11:17,648 - INFO - Epoch 4/1000, Train Loss: 0.9456, Val Loss: 0.9737
2024-12-28 15:11:18,080 - INFO - Epoch 5/1000, Train Loss: 0.8873, Val Loss: 0.9235
2024-12-28 15:11:18,522 - INFO - Epoch 6/1000, Train Loss: 0.8514, Val Loss: 0.8993
2024-12-28 15:11:18,942 - INFO - Epoch 7/1000, Train Loss: 0.8249, Val Loss: 0.8735
2024-12-28 15:11:19,361 - INFO - Epoch 8/1000, Train Loss: 0.8064, Val Loss: 0.8665
2024-12-28 15:11:19,778 - INFO - Epoch 9/1000, Train Loss: 0.7935, Val Loss: 0.8489
2024-12-28 15:11:20,191 - INFO - Epoch 10/1000, Train Loss: 0.7836, Val Loss: 0.8381
2024-12-28 15:11:20,656 - INFO - Epoch 11/1000, Train Loss: 0.7737, Val Loss: 0.8404
2024-12-28 15:11:21,070 - INFO - Epoch 12/1000, Train Loss: 0.7699, Val Loss: 0.8308
2024-12-28 15:11:21,499 - INFO - Epoch 13/1000, Train Loss: 0.7626, Val Loss: 0.8301
2024-12-28 15:11:21,924 - INFO - Epoch 14/1000, Train Loss: 0.7601, Val Loss: 0.8165
2024-12-28 15:11:22,647 - INFO - Epoch 15/1000, Train Loss: 0.7570, Val Loss: 0.8286
2024-12-28 15:11:23,498 - INFO - Epoch 16/1000, Train Loss: 0.7537, Val Loss: 0.8157
2024-12-28 15:11:24,379 - INFO - Epoch 17/1000, Train Loss: 0.7532, Val Loss: 0.8158
2024-12-28 15:11:25,200 - INFO - Epoch 18/1000, Train Loss: 0.7514, Val Loss: 0.8218
2024-12-28 15:11:25,939 - INFO - Epoch 19/1000, Train Loss: 0.7495, Val Loss: 0.8172
2024-12-28 15:11:25,940 - INFO - Early stopping triggered at epoch 19
2024-12-28 15:11:25,940 - INFO - Training completed in 10.14s
2024-12-28 15:11:25,940 - INFO - Final memory usage: CPU 1924.7 MB, GPU 104.5 MB
2024-12-28 15:11:25,941 - INFO - Model training completed in 10.14s
2024-12-28 15:11:26,002 - INFO - Prediction completed in 0.06s
2024-12-28 15:11:26,013 - INFO - Poison rate 0.03 completed in 17.96s
2024-12-28 15:11:26,013 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:11:26,014 - INFO - Label flipping details:
2024-12-28 15:11:26,014 - INFO - - Source class: 1
2024-12-28 15:11:26,014 - INFO - - Target class: 0
2024-12-28 15:11:26,014 - INFO - - Available samples in source class: 919
2024-12-28 15:11:26,014 - INFO - - Requested samples to poison: 987
2024-12-28 15:11:26,014 - INFO - - Actual samples to flip: 918
2024-12-28 15:11:26,014 - INFO - - Samples remaining in source class: 1
2024-12-28 15:11:26,015 - INFO - Successfully flipped 918 labels from class 1 to 0
2024-12-28 15:11:26,015 - INFO - Total number of labels flipped: 918
2024-12-28 15:11:26,015 - INFO - Label flipping completed in 0.00s
2024-12-28 15:11:26,015 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:11:26,015 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:11:27,246 - INFO - Feature scaling completed in 1.23s
2024-12-28 15:11:27,246 - INFO - Starting feature selection (k=50)
2024-12-28 15:11:27,276 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:11:27,276 - INFO - Starting anomaly detection
2024-12-28 15:11:35,418 - INFO - Anomaly detection completed in 8.14s
2024-12-28 15:11:35,418 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:11:35,418 - INFO - Total fit_transform time: 9.40s
2024-12-28 15:11:35,418 - INFO - Training set processing completed in 9.40s
2024-12-28 15:11:35,418 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:11:35,419 - INFO - Memory usage at start_fit: CPU 1886.1 MB, GPU 104.3 MB
2024-12-28 15:11:35,419 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:11:35,426 - INFO - Number of unique classes: 43
2024-12-28 15:11:35,564 - INFO - Fitted scaler and transformed data
2024-12-28 15:11:35,564 - INFO - Scaling time: 0.14s
2024-12-28 15:11:36,089 - INFO - Epoch 1/1000, Train Loss: 1.8847, Val Loss: 1.3694
2024-12-28 15:11:36,524 - INFO - Epoch 2/1000, Train Loss: 1.1889, Val Loss: 1.1270
2024-12-28 15:11:36,969 - INFO - Epoch 3/1000, Train Loss: 1.0038, Val Loss: 1.0052
2024-12-28 15:11:37,434 - INFO - Epoch 4/1000, Train Loss: 0.9096, Val Loss: 0.9428
2024-12-28 15:11:37,931 - INFO - Epoch 5/1000, Train Loss: 0.8505, Val Loss: 0.8997
2024-12-28 15:11:38,450 - INFO - Epoch 6/1000, Train Loss: 0.8163, Val Loss: 0.8712
2024-12-28 15:11:38,928 - INFO - Epoch 7/1000, Train Loss: 0.7869, Val Loss: 0.8497
2024-12-28 15:11:39,543 - INFO - Epoch 8/1000, Train Loss: 0.7689, Val Loss: 0.8287
2024-12-28 15:11:40,090 - INFO - Epoch 9/1000, Train Loss: 0.7564, Val Loss: 0.8248
2024-12-28 15:11:40,772 - INFO - Epoch 10/1000, Train Loss: 0.7448, Val Loss: 0.8103
2024-12-28 15:11:41,390 - INFO - Epoch 11/1000, Train Loss: 0.7362, Val Loss: 0.8065
2024-12-28 15:11:41,921 - INFO - Epoch 12/1000, Train Loss: 0.7327, Val Loss: 0.8007
2024-12-28 15:11:42,545 - INFO - Epoch 13/1000, Train Loss: 0.7263, Val Loss: 0.7946
2024-12-28 15:11:43,188 - INFO - Epoch 14/1000, Train Loss: 0.7237, Val Loss: 0.7945
2024-12-28 15:11:44,074 - INFO - Epoch 15/1000, Train Loss: 0.7193, Val Loss: 0.8002
2024-12-28 15:11:44,908 - INFO - Epoch 16/1000, Train Loss: 0.7178, Val Loss: 0.7945
2024-12-28 15:11:45,710 - INFO - Epoch 17/1000, Train Loss: 0.7155, Val Loss: 0.7901
2024-12-28 15:11:46,580 - INFO - Epoch 18/1000, Train Loss: 0.7145, Val Loss: 0.7922
2024-12-28 15:11:47,451 - INFO - Epoch 19/1000, Train Loss: 0.7125, Val Loss: 0.7856
2024-12-28 15:11:48,323 - INFO - Epoch 20/1000, Train Loss: 0.7106, Val Loss: 0.7866
2024-12-28 15:11:49,205 - INFO - Epoch 21/1000, Train Loss: 0.7102, Val Loss: 0.7977
2024-12-28 15:11:49,945 - INFO - Epoch 22/1000, Train Loss: 0.7114, Val Loss: 0.7862
2024-12-28 15:11:50,772 - INFO - Epoch 23/1000, Train Loss: 0.7080, Val Loss: 0.7882
2024-12-28 15:11:51,609 - INFO - Epoch 24/1000, Train Loss: 0.7102, Val Loss: 0.7888
2024-12-28 15:11:51,609 - INFO - Early stopping triggered at epoch 24
2024-12-28 15:11:51,609 - INFO - Training completed in 16.19s
2024-12-28 15:11:51,609 - INFO - Final memory usage: CPU 1924.7 MB, GPU 104.5 MB
2024-12-28 15:11:51,610 - INFO - Model training completed in 16.19s
2024-12-28 15:11:51,696 - INFO - Prediction completed in 0.09s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:11:51,716 - INFO - Poison rate 0.05 completed in 25.70s
2024-12-28 15:11:51,717 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:11:51,718 - INFO - Label flipping details:
2024-12-28 15:11:51,718 - INFO - - Source class: 1
2024-12-28 15:11:51,718 - INFO - - Target class: 0
2024-12-28 15:11:51,718 - INFO - - Available samples in source class: 919
2024-12-28 15:11:51,718 - INFO - - Requested samples to poison: 1382
2024-12-28 15:11:51,718 - INFO - - Actual samples to flip: 918
2024-12-28 15:11:51,718 - INFO - - Samples remaining in source class: 1
2024-12-28 15:11:51,718 - INFO - Successfully flipped 918 labels from class 1 to 0
2024-12-28 15:11:51,718 - INFO - Total number of labels flipped: 918
2024-12-28 15:11:51,718 - INFO - Label flipping completed in 0.00s
2024-12-28 15:11:51,718 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:11:51,718 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:11:53,005 - INFO - Feature scaling completed in 1.29s
2024-12-28 15:11:53,005 - INFO - Starting feature selection (k=50)
2024-12-28 15:11:53,033 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:11:53,034 - INFO - Starting anomaly detection
2024-12-28 15:12:00,368 - INFO - Anomaly detection completed in 7.33s
2024-12-28 15:12:00,368 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:12:00,368 - INFO - Total fit_transform time: 8.65s
2024-12-28 15:12:00,368 - INFO - Training set processing completed in 8.65s
2024-12-28 15:12:00,368 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:12:00,369 - INFO - Memory usage at start_fit: CPU 1886.1 MB, GPU 104.3 MB
2024-12-28 15:12:00,370 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:12:00,376 - INFO - Number of unique classes: 43
2024-12-28 15:12:00,518 - INFO - Fitted scaler and transformed data
2024-12-28 15:12:00,518 - INFO - Scaling time: 0.14s
2024-12-28 15:12:00,953 - INFO - Epoch 1/1000, Train Loss: 1.8771, Val Loss: 1.3615
2024-12-28 15:12:01,375 - INFO - Epoch 2/1000, Train Loss: 1.1934, Val Loss: 1.0999
2024-12-28 15:12:01,802 - INFO - Epoch 3/1000, Train Loss: 1.0088, Val Loss: 0.9805
2024-12-28 15:12:02,212 - INFO - Epoch 4/1000, Train Loss: 0.9162, Val Loss: 0.9183
2024-12-28 15:12:02,649 - INFO - Epoch 5/1000, Train Loss: 0.8564, Val Loss: 0.8712
2024-12-28 15:12:03,108 - INFO - Epoch 6/1000, Train Loss: 0.8180, Val Loss: 0.8381
2024-12-28 15:12:03,563 - INFO - Epoch 7/1000, Train Loss: 0.7923, Val Loss: 0.8210
2024-12-28 15:12:04,038 - INFO - Epoch 8/1000, Train Loss: 0.7727, Val Loss: 0.8099
2024-12-28 15:12:04,567 - INFO - Epoch 9/1000, Train Loss: 0.7588, Val Loss: 0.7958
2024-12-28 15:12:05,168 - INFO - Epoch 10/1000, Train Loss: 0.7483, Val Loss: 0.7953
2024-12-28 15:12:05,719 - INFO - Epoch 11/1000, Train Loss: 0.7406, Val Loss: 0.7824
2024-12-28 15:12:06,245 - INFO - Epoch 12/1000, Train Loss: 0.7331, Val Loss: 0.7836
2024-12-28 15:12:06,728 - INFO - Epoch 13/1000, Train Loss: 0.7292, Val Loss: 0.7758
2024-12-28 15:12:07,226 - INFO - Epoch 14/1000, Train Loss: 0.7243, Val Loss: 0.7741
2024-12-28 15:12:07,704 - INFO - Epoch 15/1000, Train Loss: 0.7218, Val Loss: 0.7733
2024-12-28 15:12:08,209 - INFO - Epoch 16/1000, Train Loss: 0.7197, Val Loss: 0.7810
2024-12-28 15:12:08,671 - INFO - Epoch 17/1000, Train Loss: 0.7194, Val Loss: 0.7790
2024-12-28 15:12:09,117 - INFO - Epoch 18/1000, Train Loss: 0.7173, Val Loss: 0.7693
2024-12-28 15:12:09,602 - INFO - Epoch 19/1000, Train Loss: 0.7142, Val Loss: 0.7634
2024-12-28 15:12:10,044 - INFO - Epoch 20/1000, Train Loss: 0.7132, Val Loss: 0.7668
2024-12-28 15:12:10,572 - INFO - Epoch 21/1000, Train Loss: 0.7135, Val Loss: 0.7669
2024-12-28 15:12:11,105 - INFO - Epoch 22/1000, Train Loss: 0.7125, Val Loss: 0.7657
2024-12-28 15:12:11,617 - INFO - Epoch 23/1000, Train Loss: 0.7122, Val Loss: 0.7602
2024-12-28 15:12:12,085 - INFO - Epoch 24/1000, Train Loss: 0.7098, Val Loss: 0.7649
2024-12-28 15:12:12,594 - INFO - Epoch 25/1000, Train Loss: 0.7095, Val Loss: 0.7678
2024-12-28 15:12:13,110 - INFO - Epoch 26/1000, Train Loss: 0.7103, Val Loss: 0.7617
2024-12-28 15:12:13,589 - INFO - Epoch 27/1000, Train Loss: 0.7082, Val Loss: 0.7606
2024-12-28 15:12:14,054 - INFO - Epoch 28/1000, Train Loss: 0.7106, Val Loss: 0.7606
2024-12-28 15:12:14,054 - INFO - Early stopping triggered at epoch 28
2024-12-28 15:12:14,054 - INFO - Training completed in 13.69s
2024-12-28 15:12:14,055 - INFO - Final memory usage: CPU 1924.7 MB, GPU 104.5 MB
2024-12-28 15:12:14,056 - INFO - Model training completed in 13.69s
2024-12-28 15:12:14,126 - INFO - Prediction completed in 0.07s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:12:14,137 - INFO - Poison rate 0.07 completed in 22.42s
2024-12-28 15:12:14,137 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:12:14,139 - INFO - Label flipping details:
2024-12-28 15:12:14,139 - INFO - - Source class: 1
2024-12-28 15:12:14,139 - INFO - - Target class: 0
2024-12-28 15:12:14,139 - INFO - - Available samples in source class: 919
2024-12-28 15:12:14,139 - INFO - - Requested samples to poison: 1975
2024-12-28 15:12:14,139 - INFO - - Actual samples to flip: 918
2024-12-28 15:12:14,139 - INFO - - Samples remaining in source class: 1
2024-12-28 15:12:14,139 - INFO - Successfully flipped 918 labels from class 1 to 0
2024-12-28 15:12:14,139 - INFO - Total number of labels flipped: 918
2024-12-28 15:12:14,139 - INFO - Label flipping completed in 0.00s
2024-12-28 15:12:14,139 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:12:14,139 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:12:15,319 - INFO - Feature scaling completed in 1.18s
2024-12-28 15:12:15,319 - INFO - Starting feature selection (k=50)
2024-12-28 15:12:15,347 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:12:15,347 - INFO - Starting anomaly detection
2024-12-28 15:12:23,336 - INFO - Anomaly detection completed in 7.99s
2024-12-28 15:12:23,336 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:12:23,336 - INFO - Total fit_transform time: 9.20s
2024-12-28 15:12:23,336 - INFO - Training set processing completed in 9.20s
2024-12-28 15:12:23,336 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:12:23,337 - INFO - Memory usage at start_fit: CPU 1886.1 MB, GPU 104.3 MB
2024-12-28 15:12:23,337 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:12:23,347 - INFO - Number of unique classes: 43
2024-12-28 15:12:23,496 - INFO - Fitted scaler and transformed data
2024-12-28 15:12:23,496 - INFO - Scaling time: 0.15s
2024-12-28 15:12:23,997 - INFO - Epoch 1/1000, Train Loss: 1.8672, Val Loss: 1.4242
2024-12-28 15:12:24,411 - INFO - Epoch 2/1000, Train Loss: 1.1803, Val Loss: 1.1601
2024-12-28 15:12:24,852 - INFO - Epoch 3/1000, Train Loss: 0.9984, Val Loss: 1.0586
2024-12-28 15:12:25,297 - INFO - Epoch 4/1000, Train Loss: 0.9066, Val Loss: 0.9768
2024-12-28 15:12:25,757 - INFO - Epoch 5/1000, Train Loss: 0.8477, Val Loss: 0.9320
2024-12-28 15:12:26,181 - INFO - Epoch 6/1000, Train Loss: 0.8104, Val Loss: 0.9077
2024-12-28 15:12:26,592 - INFO - Epoch 7/1000, Train Loss: 0.7850, Val Loss: 0.8926
2024-12-28 15:12:27,032 - INFO - Epoch 8/1000, Train Loss: 0.7655, Val Loss: 0.8728
2024-12-28 15:12:27,472 - INFO - Epoch 9/1000, Train Loss: 0.7540, Val Loss: 0.8706
2024-12-28 15:12:27,928 - INFO - Epoch 10/1000, Train Loss: 0.7423, Val Loss: 0.8586
2024-12-28 15:12:28,379 - INFO - Epoch 11/1000, Train Loss: 0.7353, Val Loss: 0.8542
2024-12-28 15:12:28,814 - INFO - Epoch 12/1000, Train Loss: 0.7278, Val Loss: 0.8395
2024-12-28 15:12:29,239 - INFO - Epoch 13/1000, Train Loss: 0.7255, Val Loss: 0.8370
2024-12-28 15:12:29,682 - INFO - Epoch 14/1000, Train Loss: 0.7201, Val Loss: 0.8317
2024-12-28 15:12:30,135 - INFO - Epoch 15/1000, Train Loss: 0.7166, Val Loss: 0.8318
2024-12-28 15:12:30,603 - INFO - Epoch 16/1000, Train Loss: 0.7153, Val Loss: 0.8322
2024-12-28 15:12:31,055 - INFO - Epoch 17/1000, Train Loss: 0.7127, Val Loss: 0.8356
2024-12-28 15:12:31,516 - INFO - Epoch 18/1000, Train Loss: 0.7118, Val Loss: 0.8294
2024-12-28 15:12:31,979 - INFO - Epoch 19/1000, Train Loss: 0.7117, Val Loss: 0.8286
2024-12-28 15:12:32,435 - INFO - Epoch 20/1000, Train Loss: 0.7104, Val Loss: 0.8191
2024-12-28 15:12:32,890 - INFO - Epoch 21/1000, Train Loss: 0.7072, Val Loss: 0.8299
2024-12-28 15:12:33,351 - INFO - Epoch 22/1000, Train Loss: 0.7082, Val Loss: 0.8264
2024-12-28 15:12:33,779 - INFO - Epoch 23/1000, Train Loss: 0.7075, Val Loss: 0.8254
2024-12-28 15:12:34,236 - INFO - Epoch 24/1000, Train Loss: 0.7048, Val Loss: 0.8230
2024-12-28 15:12:34,656 - INFO - Epoch 25/1000, Train Loss: 0.7053, Val Loss: 0.8247
2024-12-28 15:12:34,656 - INFO - Early stopping triggered at epoch 25
2024-12-28 15:12:34,656 - INFO - Training completed in 11.32s
2024-12-28 15:12:34,656 - INFO - Final memory usage: CPU 1924.7 MB, GPU 104.5 MB
2024-12-28 15:12:34,657 - INFO - Model training completed in 11.32s
2024-12-28 15:12:34,718 - INFO - Prediction completed in 0.06s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:12:34,729 - INFO - Poison rate 0.1 completed in 20.59s
2024-12-28 15:12:34,729 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:12:34,730 - INFO - Label flipping details:
2024-12-28 15:12:34,730 - INFO - - Source class: 1
2024-12-28 15:12:34,731 - INFO - - Target class: 0
2024-12-28 15:12:34,731 - INFO - - Available samples in source class: 919
2024-12-28 15:12:34,731 - INFO - - Requested samples to poison: 3951
2024-12-28 15:12:34,731 - INFO - - Actual samples to flip: 918
2024-12-28 15:12:34,731 - INFO - - Samples remaining in source class: 1
2024-12-28 15:12:34,731 - INFO - Successfully flipped 918 labels from class 1 to 0
2024-12-28 15:12:34,731 - INFO - Total number of labels flipped: 918
2024-12-28 15:12:34,731 - INFO - Label flipping completed in 0.00s
2024-12-28 15:12:34,731 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:12:34,731 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:12:36,048 - INFO - Feature scaling completed in 1.32s
2024-12-28 15:12:36,048 - INFO - Starting feature selection (k=50)
2024-12-28 15:12:36,076 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:12:36,077 - INFO - Starting anomaly detection
2024-12-28 15:12:43,510 - INFO - Anomaly detection completed in 7.43s
2024-12-28 15:12:43,510 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:12:43,510 - INFO - Total fit_transform time: 8.78s
2024-12-28 15:12:43,510 - INFO - Training set processing completed in 8.78s
2024-12-28 15:12:43,510 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:12:43,511 - INFO - Memory usage at start_fit: CPU 1886.1 MB, GPU 104.3 MB
2024-12-28 15:12:43,511 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:12:43,519 - INFO - Number of unique classes: 43
2024-12-28 15:12:43,675 - INFO - Fitted scaler and transformed data
2024-12-28 15:12:43,675 - INFO - Scaling time: 0.16s
2024-12-28 15:12:44,117 - INFO - Epoch 1/1000, Train Loss: 1.9150, Val Loss: 1.3767
2024-12-28 15:12:44,545 - INFO - Epoch 2/1000, Train Loss: 1.1933, Val Loss: 1.1274
2024-12-28 15:12:44,975 - INFO - Epoch 3/1000, Train Loss: 1.0078, Val Loss: 1.0083
2024-12-28 15:12:45,414 - INFO - Epoch 4/1000, Train Loss: 0.9108, Val Loss: 0.9433
2024-12-28 15:12:45,873 - INFO - Epoch 5/1000, Train Loss: 0.8528, Val Loss: 0.9045
2024-12-28 15:12:46,308 - INFO - Epoch 6/1000, Train Loss: 0.8138, Val Loss: 0.8819
2024-12-28 15:12:46,753 - INFO - Epoch 7/1000, Train Loss: 0.7885, Val Loss: 0.8534
2024-12-28 15:12:47,188 - INFO - Epoch 8/1000, Train Loss: 0.7707, Val Loss: 0.8462
2024-12-28 15:12:47,667 - INFO - Epoch 9/1000, Train Loss: 0.7563, Val Loss: 0.8327
2024-12-28 15:12:48,106 - INFO - Epoch 10/1000, Train Loss: 0.7448, Val Loss: 0.8177
2024-12-28 15:12:48,554 - INFO - Epoch 11/1000, Train Loss: 0.7364, Val Loss: 0.8176
2024-12-28 15:12:48,995 - INFO - Epoch 12/1000, Train Loss: 0.7316, Val Loss: 0.8118
2024-12-28 15:12:49,460 - INFO - Epoch 13/1000, Train Loss: 0.7272, Val Loss: 0.8066
2024-12-28 15:12:49,907 - INFO - Epoch 14/1000, Train Loss: 0.7221, Val Loss: 0.8055
2024-12-28 15:12:50,358 - INFO - Epoch 15/1000, Train Loss: 0.7208, Val Loss: 0.8023
2024-12-28 15:12:50,798 - INFO - Epoch 16/1000, Train Loss: 0.7170, Val Loss: 0.7990
2024-12-28 15:12:51,268 - INFO - Epoch 17/1000, Train Loss: 0.7136, Val Loss: 0.8105
2024-12-28 15:12:51,723 - INFO - Epoch 18/1000, Train Loss: 0.7136, Val Loss: 0.7951
2024-12-28 15:12:52,175 - INFO - Epoch 19/1000, Train Loss: 0.7117, Val Loss: 0.8003
2024-12-28 15:12:52,643 - INFO - Epoch 20/1000, Train Loss: 0.7103, Val Loss: 0.7978
2024-12-28 15:12:53,127 - INFO - Epoch 21/1000, Train Loss: 0.7109, Val Loss: 0.7917
2024-12-28 15:12:53,624 - INFO - Epoch 22/1000, Train Loss: 0.7083, Val Loss: 0.7877
2024-12-28 15:12:54,061 - INFO - Epoch 23/1000, Train Loss: 0.7071, Val Loss: 0.7898
2024-12-28 15:12:54,508 - INFO - Epoch 24/1000, Train Loss: 0.7077, Val Loss: 0.7878
2024-12-28 15:12:54,947 - INFO - Epoch 25/1000, Train Loss: 0.7089, Val Loss: 0.7915
2024-12-28 15:12:55,406 - INFO - Epoch 26/1000, Train Loss: 0.7066, Val Loss: 0.7915
2024-12-28 15:12:55,861 - INFO - Epoch 27/1000, Train Loss: 0.7072, Val Loss: 0.7898
2024-12-28 15:12:55,862 - INFO - Early stopping triggered at epoch 27
2024-12-28 15:12:55,862 - INFO - Training completed in 12.35s
2024-12-28 15:12:55,862 - INFO - Final memory usage: CPU 1924.7 MB, GPU 104.5 MB
2024-12-28 15:12:55,863 - INFO - Model training completed in 12.35s
2024-12-28 15:12:55,942 - INFO - Prediction completed in 0.08s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:12:55,954 - INFO - Poison rate 0.2 completed in 21.22s
2024-12-28 15:12:55,958 - INFO - Loaded 133 existing results
2024-12-28 15:12:55,958 - INFO - Total results to save: 140
2024-12-28 15:12:55,959 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:12:55,967 - INFO - Saved 140 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:12:55,967 - INFO - Total evaluation time: 177.01s
2024-12-28 15:12:55,974 - INFO - 
Progress: 21.9% - Evaluating GTSRB with RandomForest (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:12:56,189 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:12:56,189 - INFO - Dataset type: image
2024-12-28 15:12:56,189 - INFO - Sample size: 39209
2024-12-28 15:12:56,189 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:12:56,190 - INFO - Loading datasets...
2024-12-28 15:13:15,966 - INFO - Dataset loading completed in 19.78s
2024-12-28 15:13:15,966 - INFO - Extracting validation features...
2024-12-28 15:13:15,966 - INFO - Extracting features from 4435 samples...
2024-12-28 15:13:16,712 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:13:16,714 - INFO - Validation feature extraction completed in 0.75s
2024-12-28 15:13:16,714 - INFO - Extracting training features...
2024-12-28 15:13:16,715 - INFO - Extracting features from 19755 samples...
2024-12-28 15:13:19,387 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:13:19,395 - INFO - Training feature extraction completed in 2.68s
2024-12-28 15:13:19,395 - INFO - Creating model for classifier: RandomForest
2024-12-28 15:13:19,395 - INFO - Using device: cuda
2024-12-28 15:13:19,395 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:13:19,396 - INFO - Training set processing completed in 0.00s
2024-12-28 15:13:19,396 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:13:19,397 - INFO - Memory usage at start_fit: CPU 1927.4 MB, GPU 104.0 MB
2024-12-28 15:13:19,397 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:13:20,013 - INFO - Fitted scaler and transformed data
2024-12-28 15:13:20,014 - INFO - Scaling time: 0.62s
2024-12-28 15:13:20,036 - INFO - Number of unique classes: 43
2024-12-28 15:13:26,114 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:13:33,264 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 15:13:39,160 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 15:13:45,621 - INFO - Epoch 4/10, Train Loss: 3.7587, Val Loss: 3.7585
2024-12-28 15:13:45,621 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:13:45,621 - INFO - Training completed in 26.23s
2024-12-28 15:13:45,622 - INFO - Final memory usage: CPU 1927.4 MB, GPU 153.6 MB
2024-12-28 15:13:45,622 - INFO - Model training completed in 26.23s
2024-12-28 15:13:45,803 - INFO - Prediction completed in 0.18s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:13:45,815 - INFO - Poison rate 0.0 completed in 26.42s
2024-12-28 15:13:45,816 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:13:45,817 - INFO - Label flipping details:
2024-12-28 15:13:45,817 - INFO - - Source class: 1
2024-12-28 15:13:45,817 - INFO - - Target class: 0
2024-12-28 15:13:45,817 - INFO - - Available samples in source class: 918
2024-12-28 15:13:45,817 - INFO - - Requested samples to poison: 197
2024-12-28 15:13:45,817 - INFO - - Actual samples to flip: 197
2024-12-28 15:13:45,817 - INFO - - Samples remaining in source class: 721
2024-12-28 15:13:45,817 - INFO - Successfully flipped 197 labels from class 1 to 0
2024-12-28 15:13:45,817 - INFO - Total number of labels flipped: 197
2024-12-28 15:13:45,817 - INFO - Label flipping completed in 0.00s
2024-12-28 15:13:45,818 - INFO - Training set processing completed in 0.00s
2024-12-28 15:13:45,818 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:13:45,818 - INFO - Memory usage at start_fit: CPU 1927.4 MB, GPU 112.5 MB
2024-12-28 15:13:45,818 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:13:46,322 - INFO - Fitted scaler and transformed data
2024-12-28 15:13:46,322 - INFO - Scaling time: 0.50s
2024-12-28 15:13:46,342 - INFO - Number of unique classes: 43
2024-12-28 15:13:51,936 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:13:58,955 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 15:14:05,824 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 15:14:12,639 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7586
2024-12-28 15:14:12,639 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:14:12,640 - INFO - Training completed in 26.82s
2024-12-28 15:14:12,640 - INFO - Final memory usage: CPU 1927.4 MB, GPU 153.6 MB
2024-12-28 15:14:12,640 - INFO - Model training completed in 26.82s
2024-12-28 15:14:12,849 - INFO - Prediction completed in 0.21s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:14:12,861 - INFO - Poison rate 0.01 completed in 27.04s
2024-12-28 15:14:12,861 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:14:12,862 - INFO - Label flipping details:
2024-12-28 15:14:12,862 - INFO - - Source class: 1
2024-12-28 15:14:12,862 - INFO - - Target class: 0
2024-12-28 15:14:12,862 - INFO - - Available samples in source class: 918
2024-12-28 15:14:12,862 - INFO - - Requested samples to poison: 592
2024-12-28 15:14:12,862 - INFO - - Actual samples to flip: 592
2024-12-28 15:14:12,862 - INFO - - Samples remaining in source class: 326
2024-12-28 15:14:12,862 - INFO - Successfully flipped 592 labels from class 1 to 0
2024-12-28 15:14:12,862 - INFO - Total number of labels flipped: 592
2024-12-28 15:14:12,863 - INFO - Label flipping completed in 0.00s
2024-12-28 15:14:12,863 - INFO - Training set processing completed in 0.00s
2024-12-28 15:14:12,863 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:14:12,864 - INFO - Memory usage at start_fit: CPU 1927.4 MB, GPU 112.5 MB
2024-12-28 15:14:12,864 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:14:13,408 - INFO - Fitted scaler and transformed data
2024-12-28 15:14:13,408 - INFO - Scaling time: 0.54s
2024-12-28 15:14:13,429 - INFO - Number of unique classes: 43
2024-12-28 15:14:19,036 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:14:25,454 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 15:14:31,859 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 15:14:37,660 - INFO - Epoch 4/10, Train Loss: 3.7587, Val Loss: 3.7586
2024-12-28 15:14:37,660 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:14:37,660 - INFO - Training completed in 24.80s
2024-12-28 15:14:37,660 - INFO - Final memory usage: CPU 1927.4 MB, GPU 153.6 MB
2024-12-28 15:14:37,661 - INFO - Model training completed in 24.80s
2024-12-28 15:14:37,843 - INFO - Prediction completed in 0.18s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:14:37,855 - INFO - Poison rate 0.03 completed in 24.99s
2024-12-28 15:14:37,855 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:14:37,857 - INFO - Label flipping details:
2024-12-28 15:14:37,857 - INFO - - Source class: 1
2024-12-28 15:14:37,857 - INFO - - Target class: 0
2024-12-28 15:14:37,857 - INFO - - Available samples in source class: 918
2024-12-28 15:14:37,857 - INFO - - Requested samples to poison: 987
2024-12-28 15:14:37,857 - INFO - - Actual samples to flip: 917
2024-12-28 15:14:37,857 - INFO - - Samples remaining in source class: 1
2024-12-28 15:14:37,857 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:14:37,857 - INFO - Total number of labels flipped: 917
2024-12-28 15:14:37,857 - INFO - Label flipping completed in 0.00s
2024-12-28 15:14:37,857 - INFO - Training set processing completed in 0.00s
2024-12-28 15:14:37,857 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:14:37,858 - INFO - Memory usage at start_fit: CPU 1927.4 MB, GPU 112.5 MB
2024-12-28 15:14:37,858 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:14:38,368 - INFO - Fitted scaler and transformed data
2024-12-28 15:14:38,368 - INFO - Scaling time: 0.51s
2024-12-28 15:14:38,387 - INFO - Number of unique classes: 43
2024-12-28 15:14:44,613 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:14:51,390 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 15:14:57,220 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 15:15:02,845 - INFO - Epoch 4/10, Train Loss: 3.7587, Val Loss: 3.7584
2024-12-28 15:15:02,845 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:15:02,845 - INFO - Training completed in 24.99s
2024-12-28 15:15:02,846 - INFO - Final memory usage: CPU 1927.4 MB, GPU 153.6 MB
2024-12-28 15:15:02,846 - INFO - Model training completed in 24.99s
2024-12-28 15:15:03,118 - INFO - Prediction completed in 0.27s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:15:03,130 - INFO - Poison rate 0.05 completed in 25.27s
2024-12-28 15:15:03,130 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:15:03,132 - INFO - Label flipping details:
2024-12-28 15:15:03,132 - INFO - - Source class: 1
2024-12-28 15:15:03,132 - INFO - - Target class: 0
2024-12-28 15:15:03,132 - INFO - - Available samples in source class: 918
2024-12-28 15:15:03,132 - INFO - - Requested samples to poison: 1382
2024-12-28 15:15:03,132 - INFO - - Actual samples to flip: 917
2024-12-28 15:15:03,132 - INFO - - Samples remaining in source class: 1
2024-12-28 15:15:03,132 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:15:03,132 - INFO - Total number of labels flipped: 917
2024-12-28 15:15:03,132 - INFO - Label flipping completed in 0.00s
2024-12-28 15:15:03,132 - INFO - Training set processing completed in 0.00s
2024-12-28 15:15:03,132 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:15:03,133 - INFO - Memory usage at start_fit: CPU 1927.4 MB, GPU 112.5 MB
2024-12-28 15:15:03,134 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:15:03,662 - INFO - Fitted scaler and transformed data
2024-12-28 15:15:03,662 - INFO - Scaling time: 0.53s
2024-12-28 15:15:03,681 - INFO - Number of unique classes: 43
2024-12-28 15:15:10,253 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7605
2024-12-28 15:15:16,509 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 15:15:22,117 - INFO - Epoch 3/10, Train Loss: 3.7594, Val Loss: 3.7592
2024-12-28 15:15:28,147 - INFO - Epoch 4/10, Train Loss: 3.7587, Val Loss: 3.7584
2024-12-28 15:15:28,148 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:15:28,148 - INFO - Training completed in 25.01s
2024-12-28 15:15:28,148 - INFO - Final memory usage: CPU 1927.4 MB, GPU 153.6 MB
2024-12-28 15:15:28,148 - INFO - Model training completed in 25.02s
2024-12-28 15:15:28,328 - INFO - Prediction completed in 0.18s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:15:28,340 - INFO - Poison rate 0.07 completed in 25.21s
2024-12-28 15:15:28,340 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:15:28,341 - INFO - Label flipping details:
2024-12-28 15:15:28,341 - INFO - - Source class: 1
2024-12-28 15:15:28,341 - INFO - - Target class: 0
2024-12-28 15:15:28,341 - INFO - - Available samples in source class: 918
2024-12-28 15:15:28,341 - INFO - - Requested samples to poison: 1975
2024-12-28 15:15:28,341 - INFO - - Actual samples to flip: 917
2024-12-28 15:15:28,341 - INFO - - Samples remaining in source class: 1
2024-12-28 15:15:28,341 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:15:28,342 - INFO - Total number of labels flipped: 917
2024-12-28 15:15:28,342 - INFO - Label flipping completed in 0.00s
2024-12-28 15:15:28,342 - INFO - Training set processing completed in 0.00s
2024-12-28 15:15:28,342 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:15:28,343 - INFO - Memory usage at start_fit: CPU 1927.4 MB, GPU 112.5 MB
2024-12-28 15:15:28,343 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:15:28,847 - INFO - Fitted scaler and transformed data
2024-12-28 15:15:28,848 - INFO - Scaling time: 0.50s
2024-12-28 15:15:28,866 - INFO - Number of unique classes: 43
2024-12-28 15:15:34,312 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:15:40,718 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 15:15:46,920 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 15:15:52,757 - INFO - Epoch 4/10, Train Loss: 3.7587, Val Loss: 3.7585
2024-12-28 15:15:52,757 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:15:52,757 - INFO - Training completed in 24.41s
2024-12-28 15:15:52,757 - INFO - Final memory usage: CPU 1927.4 MB, GPU 153.6 MB
2024-12-28 15:15:52,758 - INFO - Model training completed in 24.42s
2024-12-28 15:15:52,934 - INFO - Prediction completed in 0.18s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:15:52,946 - INFO - Poison rate 0.1 completed in 24.61s
2024-12-28 15:15:52,946 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:15:52,947 - INFO - Label flipping details:
2024-12-28 15:15:52,947 - INFO - - Source class: 1
2024-12-28 15:15:52,947 - INFO - - Target class: 0
2024-12-28 15:15:52,947 - INFO - - Available samples in source class: 918
2024-12-28 15:15:52,947 - INFO - - Requested samples to poison: 3951
2024-12-28 15:15:52,947 - INFO - - Actual samples to flip: 917
2024-12-28 15:15:52,947 - INFO - - Samples remaining in source class: 1
2024-12-28 15:15:52,947 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:15:52,947 - INFO - Total number of labels flipped: 917
2024-12-28 15:15:52,948 - INFO - Label flipping completed in 0.00s
2024-12-28 15:15:52,948 - INFO - Training set processing completed in 0.00s
2024-12-28 15:15:52,948 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:15:52,949 - INFO - Memory usage at start_fit: CPU 1927.4 MB, GPU 112.5 MB
2024-12-28 15:15:52,949 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:15:53,451 - INFO - Fitted scaler and transformed data
2024-12-28 15:15:53,452 - INFO - Scaling time: 0.50s
2024-12-28 15:15:53,470 - INFO - Number of unique classes: 43
2024-12-28 15:15:58,945 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:16:05,117 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 15:16:11,157 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 15:16:17,939 - INFO - Epoch 4/10, Train Loss: 3.7587, Val Loss: 3.7585
2024-12-28 15:16:17,939 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:16:17,939 - INFO - Training completed in 24.99s
2024-12-28 15:16:17,940 - INFO - Final memory usage: CPU 1927.4 MB, GPU 153.6 MB
2024-12-28 15:16:17,940 - INFO - Model training completed in 24.99s
2024-12-28 15:16:18,108 - INFO - Prediction completed in 0.17s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:16:18,120 - INFO - Poison rate 0.2 completed in 25.17s
2024-12-28 15:16:18,124 - INFO - Loaded 140 existing results
2024-12-28 15:16:18,124 - INFO - Total results to save: 147
2024-12-28 15:16:18,125 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:16:18,134 - INFO - Saved 147 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:16:18,134 - INFO - Total evaluation time: 201.94s
2024-12-28 15:16:18,140 - INFO - 
Progress: 22.9% - Evaluating GTSRB with RandomForest (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:16:18,307 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:16:18,307 - INFO - Dataset type: image
2024-12-28 15:16:18,307 - INFO - Sample size: 39209
2024-12-28 15:16:18,307 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:16:18,308 - INFO - Loading datasets...
2024-12-28 15:16:37,001 - INFO - Dataset loading completed in 18.69s
2024-12-28 15:16:37,002 - INFO - Extracting validation features...
2024-12-28 15:16:37,002 - INFO - Extracting features from 4435 samples...
2024-12-28 15:16:37,756 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:16:37,761 - INFO - Validation feature extraction completed in 0.76s
2024-12-28 15:16:37,762 - INFO - Extracting training features...
2024-12-28 15:16:37,762 - INFO - Extracting features from 19755 samples...
2024-12-28 15:16:40,526 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:16:40,538 - INFO - Training feature extraction completed in 2.78s
2024-12-28 15:16:40,538 - INFO - Creating model for classifier: RandomForest
2024-12-28 15:16:40,538 - INFO - Using device: cuda
2024-12-28 15:16:40,538 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:16:40,538 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:16:40,538 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:16:41,828 - INFO - Feature scaling completed in 1.29s
2024-12-28 15:16:41,828 - INFO - Starting feature selection (k=50)
2024-12-28 15:16:41,857 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:16:41,857 - INFO - Starting anomaly detection
2024-12-28 15:16:49,403 - INFO - Anomaly detection completed in 7.55s
2024-12-28 15:16:49,403 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:16:49,403 - INFO - Total fit_transform time: 8.86s
2024-12-28 15:16:49,403 - INFO - Training set processing completed in 8.86s
2024-12-28 15:16:49,403 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:16:49,405 - INFO - Memory usage at start_fit: CPU 1927.5 MB, GPU 104.0 MB
2024-12-28 15:16:49,405 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:16:49,939 - INFO - Fitted scaler and transformed data
2024-12-28 15:16:49,939 - INFO - Scaling time: 0.53s
2024-12-28 15:16:49,951 - INFO - Number of unique classes: 43
2024-12-28 15:16:57,222 - INFO - Epoch 1/10, Train Loss: 3.5739, Val Loss: 3.7606
2024-12-28 15:17:04,565 - INFO - Epoch 2/10, Train Loss: 3.5732, Val Loss: 3.7599
2024-12-28 15:17:12,030 - INFO - Epoch 3/10, Train Loss: 3.5726, Val Loss: 3.7592
2024-12-28 15:17:19,882 - INFO - Epoch 4/10, Train Loss: 3.5719, Val Loss: 3.7585
2024-12-28 15:17:19,882 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:17:19,882 - INFO - Training completed in 30.48s
2024-12-28 15:17:19,882 - INFO - Final memory usage: CPU 1927.5 MB, GPU 153.6 MB
2024-12-28 15:17:19,882 - INFO - Model training completed in 30.48s
2024-12-28 15:17:20,091 - INFO - Prediction completed in 0.21s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:17:20,103 - INFO - Poison rate 0.0 completed in 39.56s
2024-12-28 15:17:20,103 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:17:20,104 - INFO - Label flipping details:
2024-12-28 15:17:20,105 - INFO - - Source class: 1
2024-12-28 15:17:20,105 - INFO - - Target class: 0
2024-12-28 15:17:20,105 - INFO - - Available samples in source class: 916
2024-12-28 15:17:20,105 - INFO - - Requested samples to poison: 197
2024-12-28 15:17:20,105 - INFO - - Actual samples to flip: 197
2024-12-28 15:17:20,105 - INFO - - Samples remaining in source class: 719
2024-12-28 15:17:20,105 - INFO - Successfully flipped 197 labels from class 1 to 0
2024-12-28 15:17:20,105 - INFO - Total number of labels flipped: 197
2024-12-28 15:17:20,105 - INFO - Label flipping completed in 0.00s
2024-12-28 15:17:20,106 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:17:20,106 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:17:21,325 - INFO - Feature scaling completed in 1.22s
2024-12-28 15:17:21,325 - INFO - Starting feature selection (k=50)
2024-12-28 15:17:21,348 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 15:17:21,348 - INFO - Starting anomaly detection
2024-12-28 15:17:29,457 - INFO - Anomaly detection completed in 8.11s
2024-12-28 15:17:29,457 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:17:29,457 - INFO - Total fit_transform time: 9.35s
2024-12-28 15:17:29,457 - INFO - Training set processing completed in 9.35s
2024-12-28 15:17:29,457 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:17:29,458 - INFO - Memory usage at start_fit: CPU 1927.5 MB, GPU 112.5 MB
2024-12-28 15:17:29,459 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:17:30,035 - INFO - Fitted scaler and transformed data
2024-12-28 15:17:30,035 - INFO - Scaling time: 0.58s
2024-12-28 15:17:30,047 - INFO - Number of unique classes: 43
2024-12-28 15:17:37,366 - INFO - Epoch 1/10, Train Loss: 3.5729, Val Loss: 3.7606
2024-12-28 15:17:43,900 - INFO - Epoch 2/10, Train Loss: 3.5722, Val Loss: 3.7599
2024-12-28 15:17:50,067 - INFO - Epoch 3/10, Train Loss: 3.5716, Val Loss: 3.7592
2024-12-28 15:17:55,630 - INFO - Epoch 4/10, Train Loss: 3.5709, Val Loss: 3.7585
2024-12-28 15:17:55,630 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:17:55,630 - INFO - Training completed in 26.17s
2024-12-28 15:17:55,631 - INFO - Final memory usage: CPU 1886.4 MB, GPU 153.6 MB
2024-12-28 15:17:55,631 - INFO - Model training completed in 26.17s
2024-12-28 15:17:55,828 - INFO - Prediction completed in 0.20s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:17:55,839 - INFO - Poison rate 0.01 completed in 35.74s
2024-12-28 15:17:55,839 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:17:55,841 - INFO - Label flipping details:
2024-12-28 15:17:55,841 - INFO - - Source class: 1
2024-12-28 15:17:55,841 - INFO - - Target class: 0
2024-12-28 15:17:55,841 - INFO - - Available samples in source class: 916
2024-12-28 15:17:55,841 - INFO - - Requested samples to poison: 592
2024-12-28 15:17:55,841 - INFO - - Actual samples to flip: 592
2024-12-28 15:17:55,841 - INFO - - Samples remaining in source class: 324
2024-12-28 15:17:55,841 - INFO - Successfully flipped 592 labels from class 1 to 0
2024-12-28 15:17:55,841 - INFO - Total number of labels flipped: 592
2024-12-28 15:17:55,841 - INFO - Label flipping completed in 0.00s
2024-12-28 15:17:55,841 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:17:55,841 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:17:57,020 - INFO - Feature scaling completed in 1.18s
2024-12-28 15:17:57,021 - INFO - Starting feature selection (k=50)
2024-12-28 15:17:57,046 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:17:57,046 - INFO - Starting anomaly detection
2024-12-28 15:18:05,251 - INFO - Anomaly detection completed in 8.20s
2024-12-28 15:18:05,251 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:18:05,251 - INFO - Total fit_transform time: 9.41s
2024-12-28 15:18:05,251 - INFO - Training set processing completed in 9.41s
2024-12-28 15:18:05,251 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:18:05,252 - INFO - Memory usage at start_fit: CPU 1886.4 MB, GPU 112.5 MB
2024-12-28 15:18:05,253 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:18:05,795 - INFO - Fitted scaler and transformed data
2024-12-28 15:18:05,796 - INFO - Scaling time: 0.54s
2024-12-28 15:18:05,809 - INFO - Number of unique classes: 43
2024-12-28 15:18:11,683 - INFO - Epoch 1/10, Train Loss: 3.5714, Val Loss: 3.7606
2024-12-28 15:18:18,108 - INFO - Epoch 2/10, Train Loss: 3.5708, Val Loss: 3.7600
2024-12-28 15:18:24,383 - INFO - Epoch 3/10, Train Loss: 3.5701, Val Loss: 3.7593
2024-12-28 15:18:31,616 - INFO - Epoch 4/10, Train Loss: 3.5694, Val Loss: 3.7586
2024-12-28 15:18:31,616 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:18:31,616 - INFO - Training completed in 26.36s
2024-12-28 15:18:31,617 - INFO - Final memory usage: CPU 1886.4 MB, GPU 153.6 MB
2024-12-28 15:18:31,617 - INFO - Model training completed in 26.37s
2024-12-28 15:18:31,894 - INFO - Prediction completed in 0.28s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:18:31,906 - INFO - Poison rate 0.03 completed in 36.07s
2024-12-28 15:18:31,906 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:18:31,907 - INFO - Label flipping details:
2024-12-28 15:18:31,908 - INFO - - Source class: 1
2024-12-28 15:18:31,908 - INFO - - Target class: 0
2024-12-28 15:18:31,908 - INFO - - Available samples in source class: 916
2024-12-28 15:18:31,908 - INFO - - Requested samples to poison: 987
2024-12-28 15:18:31,908 - INFO - - Actual samples to flip: 915
2024-12-28 15:18:31,908 - INFO - - Samples remaining in source class: 1
2024-12-28 15:18:31,908 - INFO - Successfully flipped 915 labels from class 1 to 0
2024-12-28 15:18:31,908 - INFO - Total number of labels flipped: 915
2024-12-28 15:18:31,908 - INFO - Label flipping completed in 0.00s
2024-12-28 15:18:31,908 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:18:31,908 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:18:33,096 - INFO - Feature scaling completed in 1.19s
2024-12-28 15:18:33,096 - INFO - Starting feature selection (k=50)
2024-12-28 15:18:33,122 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:18:33,123 - INFO - Starting anomaly detection
2024-12-28 15:18:39,484 - INFO - Anomaly detection completed in 6.36s
2024-12-28 15:18:39,485 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:18:39,485 - INFO - Total fit_transform time: 7.58s
2024-12-28 15:18:39,485 - INFO - Training set processing completed in 7.58s
2024-12-28 15:18:39,485 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:18:39,486 - INFO - Memory usage at start_fit: CPU 1886.4 MB, GPU 112.5 MB
2024-12-28 15:18:39,487 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:18:40,024 - INFO - Fitted scaler and transformed data
2024-12-28 15:18:40,024 - INFO - Scaling time: 0.54s
2024-12-28 15:18:40,039 - INFO - Number of unique classes: 43
2024-12-28 15:18:45,473 - INFO - Epoch 1/10, Train Loss: 3.5727, Val Loss: 3.7606
2024-12-28 15:18:51,766 - INFO - Epoch 2/10, Train Loss: 3.5721, Val Loss: 3.7599
2024-12-28 15:18:57,673 - INFO - Epoch 3/10, Train Loss: 3.5714, Val Loss: 3.7592
2024-12-28 15:19:02,940 - INFO - Epoch 4/10, Train Loss: 3.5707, Val Loss: 3.7585
2024-12-28 15:19:02,940 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:19:02,940 - INFO - Training completed in 23.45s
2024-12-28 15:19:02,941 - INFO - Final memory usage: CPU 1886.4 MB, GPU 153.6 MB
2024-12-28 15:19:02,941 - INFO - Model training completed in 23.46s
2024-12-28 15:19:03,196 - INFO - Prediction completed in 0.25s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:19:03,208 - INFO - Poison rate 0.05 completed in 31.30s
2024-12-28 15:19:03,208 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:19:03,210 - INFO - Label flipping details:
2024-12-28 15:19:03,210 - INFO - - Source class: 1
2024-12-28 15:19:03,210 - INFO - - Target class: 0
2024-12-28 15:19:03,210 - INFO - - Available samples in source class: 916
2024-12-28 15:19:03,210 - INFO - - Requested samples to poison: 1382
2024-12-28 15:19:03,210 - INFO - - Actual samples to flip: 915
2024-12-28 15:19:03,210 - INFO - - Samples remaining in source class: 1
2024-12-28 15:19:03,210 - INFO - Successfully flipped 915 labels from class 1 to 0
2024-12-28 15:19:03,210 - INFO - Total number of labels flipped: 915
2024-12-28 15:19:03,210 - INFO - Label flipping completed in 0.00s
2024-12-28 15:19:03,210 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:19:03,210 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:19:04,398 - INFO - Feature scaling completed in 1.19s
2024-12-28 15:19:04,398 - INFO - Starting feature selection (k=50)
2024-12-28 15:19:04,428 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:19:04,428 - INFO - Starting anomaly detection
2024-12-28 15:19:12,362 - INFO - Anomaly detection completed in 7.93s
2024-12-28 15:19:12,362 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:19:12,362 - INFO - Total fit_transform time: 9.15s
2024-12-28 15:19:12,362 - INFO - Training set processing completed in 9.15s
2024-12-28 15:19:12,362 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:19:12,363 - INFO - Memory usage at start_fit: CPU 1886.4 MB, GPU 112.5 MB
2024-12-28 15:19:12,364 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:19:12,928 - INFO - Fitted scaler and transformed data
2024-12-28 15:19:12,928 - INFO - Scaling time: 0.56s
2024-12-28 15:19:12,943 - INFO - Number of unique classes: 43
2024-12-28 15:19:19,465 - INFO - Epoch 1/10, Train Loss: 3.5740, Val Loss: 3.7606
2024-12-28 15:19:25,395 - INFO - Epoch 2/10, Train Loss: 3.5734, Val Loss: 3.7599
2024-12-28 15:19:31,230 - INFO - Epoch 3/10, Train Loss: 3.5727, Val Loss: 3.7592
2024-12-28 15:19:37,679 - INFO - Epoch 4/10, Train Loss: 3.5720, Val Loss: 3.7585
2024-12-28 15:19:37,680 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:19:37,680 - INFO - Training completed in 25.32s
2024-12-28 15:19:37,680 - INFO - Final memory usage: CPU 1886.4 MB, GPU 153.6 MB
2024-12-28 15:19:37,680 - INFO - Model training completed in 25.32s
2024-12-28 15:19:37,999 - INFO - Prediction completed in 0.32s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:19:38,010 - INFO - Poison rate 0.07 completed in 34.80s
2024-12-28 15:19:38,010 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:19:38,012 - INFO - Label flipping details:
2024-12-28 15:19:38,012 - INFO - - Source class: 1
2024-12-28 15:19:38,012 - INFO - - Target class: 0
2024-12-28 15:19:38,012 - INFO - - Available samples in source class: 916
2024-12-28 15:19:38,012 - INFO - - Requested samples to poison: 1975
2024-12-28 15:19:38,012 - INFO - - Actual samples to flip: 915
2024-12-28 15:19:38,012 - INFO - - Samples remaining in source class: 1
2024-12-28 15:19:38,012 - INFO - Successfully flipped 915 labels from class 1 to 0
2024-12-28 15:19:38,012 - INFO - Total number of labels flipped: 915
2024-12-28 15:19:38,012 - INFO - Label flipping completed in 0.00s
2024-12-28 15:19:38,012 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:19:38,012 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:19:39,320 - INFO - Feature scaling completed in 1.31s
2024-12-28 15:19:39,320 - INFO - Starting feature selection (k=50)
2024-12-28 15:19:39,346 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:19:39,346 - INFO - Starting anomaly detection
2024-12-28 15:19:47,754 - INFO - Anomaly detection completed in 8.41s
2024-12-28 15:19:47,754 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:19:47,754 - INFO - Total fit_transform time: 9.74s
2024-12-28 15:19:47,754 - INFO - Training set processing completed in 9.74s
2024-12-28 15:19:47,754 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:19:47,755 - INFO - Memory usage at start_fit: CPU 1886.4 MB, GPU 112.5 MB
2024-12-28 15:19:47,756 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:19:48,330 - INFO - Fitted scaler and transformed data
2024-12-28 15:19:48,330 - INFO - Scaling time: 0.57s
2024-12-28 15:19:48,345 - INFO - Number of unique classes: 43
2024-12-28 15:19:55,174 - INFO - Epoch 1/10, Train Loss: 3.5738, Val Loss: 3.7606
2024-12-28 15:20:01,195 - INFO - Epoch 2/10, Train Loss: 3.5731, Val Loss: 3.7599
2024-12-28 15:20:07,352 - INFO - Epoch 3/10, Train Loss: 3.5724, Val Loss: 3.7592
2024-12-28 15:20:13,361 - INFO - Epoch 4/10, Train Loss: 3.5717, Val Loss: 3.7585
2024-12-28 15:20:13,362 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:20:13,362 - INFO - Training completed in 25.61s
2024-12-28 15:20:13,362 - INFO - Final memory usage: CPU 1886.4 MB, GPU 153.6 MB
2024-12-28 15:20:13,362 - INFO - Model training completed in 25.61s
2024-12-28 15:20:13,637 - INFO - Prediction completed in 0.27s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:20:13,649 - INFO - Poison rate 0.1 completed in 35.64s
2024-12-28 15:20:13,649 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:20:13,651 - INFO - Label flipping details:
2024-12-28 15:20:13,651 - INFO - - Source class: 1
2024-12-28 15:20:13,651 - INFO - - Target class: 0
2024-12-28 15:20:13,651 - INFO - - Available samples in source class: 916
2024-12-28 15:20:13,651 - INFO - - Requested samples to poison: 3951
2024-12-28 15:20:13,651 - INFO - - Actual samples to flip: 915
2024-12-28 15:20:13,651 - INFO - - Samples remaining in source class: 1
2024-12-28 15:20:13,651 - INFO - Successfully flipped 915 labels from class 1 to 0
2024-12-28 15:20:13,651 - INFO - Total number of labels flipped: 915
2024-12-28 15:20:13,651 - INFO - Label flipping completed in 0.00s
2024-12-28 15:20:13,651 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:20:13,651 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:20:14,917 - INFO - Feature scaling completed in 1.27s
2024-12-28 15:20:14,917 - INFO - Starting feature selection (k=50)
2024-12-28 15:20:14,943 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:20:14,943 - INFO - Starting anomaly detection
2024-12-28 15:20:20,315 - INFO - Anomaly detection completed in 5.37s
2024-12-28 15:20:20,315 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:20:20,315 - INFO - Total fit_transform time: 6.66s
2024-12-28 15:20:20,316 - INFO - Training set processing completed in 6.66s
2024-12-28 15:20:20,316 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:20:20,317 - INFO - Memory usage at start_fit: CPU 1886.4 MB, GPU 112.5 MB
2024-12-28 15:20:20,318 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:20:20,865 - INFO - Fitted scaler and transformed data
2024-12-28 15:20:20,866 - INFO - Scaling time: 0.55s
2024-12-28 15:20:20,879 - INFO - Number of unique classes: 43
2024-12-28 15:20:28,851 - INFO - Epoch 1/10, Train Loss: 3.5732, Val Loss: 3.7605
2024-12-28 15:20:35,517 - INFO - Epoch 2/10, Train Loss: 3.5725, Val Loss: 3.7599
2024-12-28 15:20:42,658 - INFO - Epoch 3/10, Train Loss: 3.5718, Val Loss: 3.7592
2024-12-28 15:20:49,491 - INFO - Epoch 4/10, Train Loss: 3.5711, Val Loss: 3.7584
2024-12-28 15:20:49,491 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:20:49,491 - INFO - Training completed in 29.17s
2024-12-28 15:20:49,491 - INFO - Final memory usage: CPU 1886.4 MB, GPU 153.6 MB
2024-12-28 15:20:49,492 - INFO - Model training completed in 29.18s
2024-12-28 15:20:49,710 - INFO - Prediction completed in 0.22s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:20:49,722 - INFO - Poison rate 0.2 completed in 36.07s
2024-12-28 15:20:49,729 - INFO - Loaded 147 existing results
2024-12-28 15:20:49,729 - INFO - Total results to save: 154
2024-12-28 15:20:49,730 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:20:49,747 - INFO - Saved 154 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:20:49,747 - INFO - Total evaluation time: 271.44s
2024-12-28 15:20:49,753 - INFO - 
Progress: 24.0% - Evaluating GTSRB with KNeighbors (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:20:49,935 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:20:49,935 - INFO - Dataset type: image
2024-12-28 15:20:49,935 - INFO - Sample size: 39209
2024-12-28 15:20:49,935 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:20:49,936 - INFO - Loading datasets...
2024-12-28 15:21:09,152 - INFO - Dataset loading completed in 19.22s
2024-12-28 15:21:09,152 - INFO - Extracting validation features...
2024-12-28 15:21:09,152 - INFO - Extracting features from 4435 samples...
2024-12-28 15:21:09,870 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:21:09,875 - INFO - Validation feature extraction completed in 0.72s
2024-12-28 15:21:09,875 - INFO - Extracting training features...
2024-12-28 15:21:09,875 - INFO - Extracting features from 19755 samples...
2024-12-28 15:21:12,581 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:21:12,592 - INFO - Training feature extraction completed in 2.72s
2024-12-28 15:21:12,593 - INFO - Creating model for classifier: KNeighbors
2024-12-28 15:21:12,593 - INFO - Using device: cuda
2024-12-28 15:21:12,593 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:21:12,593 - INFO - Training set processing completed in 0.00s
2024-12-28 15:21:12,593 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:21:12,595 - INFO - Memory usage at start_fit: CPU 1867.3 MB, GPU 104.0 MB
2024-12-28 15:21:12,595 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:21:13,141 - INFO - Fitted scaler and transformed data
2024-12-28 15:21:13,141 - INFO - Scaling time: 0.55s
2024-12-28 15:21:13,161 - INFO - Training completed in 0.57s
2024-12-28 15:21:13,162 - INFO - Final memory usage: CPU 1886.5 MB, GPU 142.7 MB
2024-12-28 15:21:13,162 - INFO - Model training completed in 0.57s
2024-12-28 15:21:13,497 - INFO - Prediction completed in 0.33s
2024-12-28 15:21:13,509 - INFO - Poison rate 0.0 completed in 0.92s
2024-12-28 15:21:13,510 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:21:13,511 - INFO - Label flipping details:
2024-12-28 15:21:13,511 - INFO - - Source class: 1
2024-12-28 15:21:13,511 - INFO - - Target class: 0
2024-12-28 15:21:13,511 - INFO - - Available samples in source class: 916
2024-12-28 15:21:13,511 - INFO - - Requested samples to poison: 197
2024-12-28 15:21:13,511 - INFO - - Actual samples to flip: 197
2024-12-28 15:21:13,511 - INFO - - Samples remaining in source class: 719
2024-12-28 15:21:13,511 - INFO - Successfully flipped 197 labels from class 1 to 0
2024-12-28 15:21:13,512 - INFO - Total number of labels flipped: 197
2024-12-28 15:21:13,512 - INFO - Label flipping completed in 0.00s
2024-12-28 15:21:13,512 - INFO - Training set processing completed in 0.00s
2024-12-28 15:21:13,512 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:21:13,513 - INFO - Memory usage at start_fit: CPU 1886.5 MB, GPU 142.7 MB
2024-12-28 15:21:13,513 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:21:14,042 - INFO - Fitted scaler and transformed data
2024-12-28 15:21:14,043 - INFO - Scaling time: 0.53s
2024-12-28 15:21:14,055 - INFO - Training completed in 0.54s
2024-12-28 15:21:14,056 - INFO - Final memory usage: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:14,056 - INFO - Model training completed in 0.54s
2024-12-28 15:21:14,148 - INFO - Prediction completed in 0.09s
2024-12-28 15:21:14,159 - INFO - Poison rate 0.01 completed in 0.65s
2024-12-28 15:21:14,159 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:21:14,160 - INFO - Label flipping details:
2024-12-28 15:21:14,161 - INFO - - Source class: 1
2024-12-28 15:21:14,161 - INFO - - Target class: 0
2024-12-28 15:21:14,161 - INFO - - Available samples in source class: 916
2024-12-28 15:21:14,161 - INFO - - Requested samples to poison: 592
2024-12-28 15:21:14,161 - INFO - - Actual samples to flip: 592
2024-12-28 15:21:14,161 - INFO - - Samples remaining in source class: 324
2024-12-28 15:21:14,161 - INFO - Successfully flipped 592 labels from class 1 to 0
2024-12-28 15:21:14,161 - INFO - Total number of labels flipped: 592
2024-12-28 15:21:14,161 - INFO - Label flipping completed in 0.00s
2024-12-28 15:21:14,161 - INFO - Training set processing completed in 0.00s
2024-12-28 15:21:14,161 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:21:14,162 - INFO - Memory usage at start_fit: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:14,163 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:21:14,670 - INFO - Fitted scaler and transformed data
2024-12-28 15:21:14,671 - INFO - Scaling time: 0.51s
2024-12-28 15:21:14,705 - INFO - Training completed in 0.54s
2024-12-28 15:21:14,706 - INFO - Final memory usage: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:14,706 - INFO - Model training completed in 0.54s
2024-12-28 15:21:14,826 - INFO - Prediction completed in 0.12s
2024-12-28 15:21:14,838 - INFO - Poison rate 0.03 completed in 0.68s
2024-12-28 15:21:14,838 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:21:14,839 - INFO - Label flipping details:
2024-12-28 15:21:14,839 - INFO - - Source class: 1
2024-12-28 15:21:14,839 - INFO - - Target class: 0
2024-12-28 15:21:14,839 - INFO - - Available samples in source class: 916
2024-12-28 15:21:14,839 - INFO - - Requested samples to poison: 987
2024-12-28 15:21:14,839 - INFO - - Actual samples to flip: 915
2024-12-28 15:21:14,839 - INFO - - Samples remaining in source class: 1
2024-12-28 15:21:14,840 - INFO - Successfully flipped 915 labels from class 1 to 0
2024-12-28 15:21:14,840 - INFO - Total number of labels flipped: 915
2024-12-28 15:21:14,840 - INFO - Label flipping completed in 0.00s
2024-12-28 15:21:14,840 - INFO - Training set processing completed in 0.00s
2024-12-28 15:21:14,840 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:21:14,841 - INFO - Memory usage at start_fit: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:14,841 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:21:15,392 - INFO - Fitted scaler and transformed data
2024-12-28 15:21:15,392 - INFO - Scaling time: 0.55s
2024-12-28 15:21:15,404 - INFO - Training completed in 0.56s
2024-12-28 15:21:15,405 - INFO - Final memory usage: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:15,405 - INFO - Model training completed in 0.57s
2024-12-28 15:21:15,531 - INFO - Prediction completed in 0.13s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:21:15,555 - INFO - Poison rate 0.05 completed in 0.72s
2024-12-28 15:21:15,555 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:21:15,557 - INFO - Label flipping details:
2024-12-28 15:21:15,558 - INFO - - Source class: 1
2024-12-28 15:21:15,558 - INFO - - Target class: 0
2024-12-28 15:21:15,558 - INFO - - Available samples in source class: 916
2024-12-28 15:21:15,558 - INFO - - Requested samples to poison: 1382
2024-12-28 15:21:15,558 - INFO - - Actual samples to flip: 915
2024-12-28 15:21:15,558 - INFO - - Samples remaining in source class: 1
2024-12-28 15:21:15,558 - INFO - Successfully flipped 915 labels from class 1 to 0
2024-12-28 15:21:15,559 - INFO - Total number of labels flipped: 915
2024-12-28 15:21:15,559 - INFO - Label flipping completed in 0.00s
2024-12-28 15:21:15,559 - INFO - Training set processing completed in 0.00s
2024-12-28 15:21:15,559 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:21:15,560 - INFO - Memory usage at start_fit: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:15,560 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:21:16,094 - INFO - Fitted scaler and transformed data
2024-12-28 15:21:16,095 - INFO - Scaling time: 0.53s
2024-12-28 15:21:16,108 - INFO - Training completed in 0.55s
2024-12-28 15:21:16,108 - INFO - Final memory usage: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:16,108 - INFO - Model training completed in 0.55s
2024-12-28 15:21:16,232 - INFO - Prediction completed in 0.12s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:21:16,243 - INFO - Poison rate 0.07 completed in 0.69s
2024-12-28 15:21:16,243 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:21:16,244 - INFO - Label flipping details:
2024-12-28 15:21:16,245 - INFO - - Source class: 1
2024-12-28 15:21:16,245 - INFO - - Target class: 0
2024-12-28 15:21:16,245 - INFO - - Available samples in source class: 916
2024-12-28 15:21:16,245 - INFO - - Requested samples to poison: 1975
2024-12-28 15:21:16,245 - INFO - - Actual samples to flip: 915
2024-12-28 15:21:16,245 - INFO - - Samples remaining in source class: 1
2024-12-28 15:21:16,245 - INFO - Successfully flipped 915 labels from class 1 to 0
2024-12-28 15:21:16,245 - INFO - Total number of labels flipped: 915
2024-12-28 15:21:16,245 - INFO - Label flipping completed in 0.00s
2024-12-28 15:21:16,245 - INFO - Training set processing completed in 0.00s
2024-12-28 15:21:16,245 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:21:16,246 - INFO - Memory usage at start_fit: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:16,246 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:21:16,763 - INFO - Fitted scaler and transformed data
2024-12-28 15:21:16,763 - INFO - Scaling time: 0.52s
2024-12-28 15:21:16,790 - INFO - Training completed in 0.54s
2024-12-28 15:21:16,790 - INFO - Final memory usage: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:16,790 - INFO - Model training completed in 0.55s
2024-12-28 15:21:16,915 - INFO - Prediction completed in 0.12s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:21:16,927 - INFO - Poison rate 0.1 completed in 0.68s
2024-12-28 15:21:16,927 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:21:16,928 - INFO - Label flipping details:
2024-12-28 15:21:16,928 - INFO - - Source class: 1
2024-12-28 15:21:16,928 - INFO - - Target class: 0
2024-12-28 15:21:16,928 - INFO - - Available samples in source class: 916
2024-12-28 15:21:16,928 - INFO - - Requested samples to poison: 3951
2024-12-28 15:21:16,928 - INFO - - Actual samples to flip: 915
2024-12-28 15:21:16,928 - INFO - - Samples remaining in source class: 1
2024-12-28 15:21:16,929 - INFO - Successfully flipped 915 labels from class 1 to 0
2024-12-28 15:21:16,929 - INFO - Total number of labels flipped: 915
2024-12-28 15:21:16,929 - INFO - Label flipping completed in 0.00s
2024-12-28 15:21:16,929 - INFO - Training set processing completed in 0.00s
2024-12-28 15:21:16,929 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:21:16,930 - INFO - Memory usage at start_fit: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:16,931 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:21:17,487 - INFO - Fitted scaler and transformed data
2024-12-28 15:21:17,488 - INFO - Scaling time: 0.56s
2024-12-28 15:21:17,502 - INFO - Training completed in 0.57s
2024-12-28 15:21:17,503 - INFO - Final memory usage: CPU 1895.1 MB, GPU 142.7 MB
2024-12-28 15:21:17,503 - INFO - Model training completed in 0.57s
2024-12-28 15:21:17,638 - INFO - Prediction completed in 0.14s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:21:17,649 - INFO - Poison rate 0.2 completed in 0.72s
2024-12-28 15:21:17,655 - INFO - Loaded 154 existing results
2024-12-28 15:21:17,655 - INFO - Total results to save: 161
2024-12-28 15:21:17,656 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:21:17,666 - INFO - Saved 161 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:21:17,666 - INFO - Total evaluation time: 27.73s
2024-12-28 15:21:17,673 - INFO - 
Progress: 25.0% - Evaluating GTSRB with KNeighbors (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:21:17,848 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:21:17,848 - INFO - Dataset type: image
2024-12-28 15:21:17,848 - INFO - Sample size: 39209
2024-12-28 15:21:17,848 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:21:17,849 - INFO - Loading datasets...
2024-12-28 15:21:37,208 - INFO - Dataset loading completed in 19.36s
2024-12-28 15:21:37,209 - INFO - Extracting validation features...
2024-12-28 15:21:37,209 - INFO - Extracting features from 4435 samples...
2024-12-28 15:21:37,899 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:21:37,902 - INFO - Validation feature extraction completed in 0.69s
2024-12-28 15:21:37,902 - INFO - Extracting training features...
2024-12-28 15:21:37,902 - INFO - Extracting features from 19755 samples...
2024-12-28 15:21:40,461 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:21:40,468 - INFO - Training feature extraction completed in 2.57s
2024-12-28 15:21:40,468 - INFO - Creating model for classifier: KNeighbors
2024-12-28 15:21:40,468 - INFO - Using device: cuda
2024-12-28 15:21:40,469 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:21:40,469 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:21:40,469 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:21:41,764 - INFO - Feature scaling completed in 1.30s
2024-12-28 15:21:41,764 - INFO - Starting feature selection (k=50)
2024-12-28 15:21:41,794 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:21:41,794 - INFO - Starting anomaly detection
2024-12-28 15:21:49,812 - INFO - Anomaly detection completed in 8.02s
2024-12-28 15:21:49,812 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:21:49,812 - INFO - Total fit_transform time: 9.34s
2024-12-28 15:21:49,812 - INFO - Training set processing completed in 9.34s
2024-12-28 15:21:49,812 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:21:49,814 - INFO - Memory usage at start_fit: CPU 1886.5 MB, GPU 103.4 MB
2024-12-28 15:21:49,814 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:21:50,351 - INFO - Fitted scaler and transformed data
2024-12-28 15:21:50,351 - INFO - Scaling time: 0.54s
2024-12-28 15:21:50,364 - INFO - Training completed in 0.55s
2024-12-28 15:21:50,365 - INFO - Final memory usage: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:21:50,365 - INFO - Model training completed in 0.55s
2024-12-28 15:21:50,702 - INFO - Prediction completed in 0.34s
2024-12-28 15:21:50,713 - INFO - Poison rate 0.0 completed in 10.24s
2024-12-28 15:21:50,713 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:21:50,714 - INFO - Label flipping details:
2024-12-28 15:21:50,715 - INFO - - Source class: 1
2024-12-28 15:21:50,715 - INFO - - Target class: 0
2024-12-28 15:21:50,715 - INFO - - Available samples in source class: 918
2024-12-28 15:21:50,715 - INFO - - Requested samples to poison: 197
2024-12-28 15:21:50,715 - INFO - - Actual samples to flip: 197
2024-12-28 15:21:50,715 - INFO - - Samples remaining in source class: 721
2024-12-28 15:21:50,715 - INFO - Successfully flipped 197 labels from class 1 to 0
2024-12-28 15:21:50,715 - INFO - Total number of labels flipped: 197
2024-12-28 15:21:50,715 - INFO - Label flipping completed in 0.00s
2024-12-28 15:21:50,715 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:21:50,715 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:21:51,875 - INFO - Feature scaling completed in 1.16s
2024-12-28 15:21:51,875 - INFO - Starting feature selection (k=50)
2024-12-28 15:21:51,889 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 15:21:51,889 - INFO - Starting anomaly detection
2024-12-28 15:21:59,714 - INFO - Anomaly detection completed in 7.83s
2024-12-28 15:21:59,715 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:21:59,715 - INFO - Total fit_transform time: 9.00s
2024-12-28 15:21:59,715 - INFO - Training set processing completed in 9.00s
2024-12-28 15:21:59,715 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:21:59,716 - INFO - Memory usage at start_fit: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:21:59,716 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:22:00,240 - INFO - Fitted scaler and transformed data
2024-12-28 15:22:00,241 - INFO - Scaling time: 0.52s
2024-12-28 15:22:00,254 - INFO - Training completed in 0.54s
2024-12-28 15:22:00,255 - INFO - Final memory usage: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:00,255 - INFO - Model training completed in 0.54s
2024-12-28 15:22:00,571 - INFO - Prediction completed in 0.32s
2024-12-28 15:22:00,583 - INFO - Poison rate 0.01 completed in 9.87s
2024-12-28 15:22:00,583 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:22:00,584 - INFO - Label flipping details:
2024-12-28 15:22:00,584 - INFO - - Source class: 1
2024-12-28 15:22:00,584 - INFO - - Target class: 0
2024-12-28 15:22:00,584 - INFO - - Available samples in source class: 918
2024-12-28 15:22:00,584 - INFO - - Requested samples to poison: 592
2024-12-28 15:22:00,584 - INFO - - Actual samples to flip: 592
2024-12-28 15:22:00,584 - INFO - - Samples remaining in source class: 326
2024-12-28 15:22:00,585 - INFO - Successfully flipped 592 labels from class 1 to 0
2024-12-28 15:22:00,585 - INFO - Total number of labels flipped: 592
2024-12-28 15:22:00,585 - INFO - Label flipping completed in 0.00s
2024-12-28 15:22:00,585 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:22:00,585 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:22:01,811 - INFO - Feature scaling completed in 1.23s
2024-12-28 15:22:01,812 - INFO - Starting feature selection (k=50)
2024-12-28 15:22:01,829 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 15:22:01,829 - INFO - Starting anomaly detection
2024-12-28 15:22:09,973 - INFO - Anomaly detection completed in 8.14s
2024-12-28 15:22:09,973 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:22:09,973 - INFO - Total fit_transform time: 9.39s
2024-12-28 15:22:09,973 - INFO - Training set processing completed in 9.39s
2024-12-28 15:22:09,973 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:22:09,974 - INFO - Memory usage at start_fit: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:09,974 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:22:10,511 - INFO - Fitted scaler and transformed data
2024-12-28 15:22:10,511 - INFO - Scaling time: 0.54s
2024-12-28 15:22:10,529 - INFO - Training completed in 0.56s
2024-12-28 15:22:10,530 - INFO - Final memory usage: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:10,530 - INFO - Model training completed in 0.56s
2024-12-28 15:22:10,851 - INFO - Prediction completed in 0.32s
2024-12-28 15:22:10,862 - INFO - Poison rate 0.03 completed in 10.28s
2024-12-28 15:22:10,862 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:22:10,863 - INFO - Label flipping details:
2024-12-28 15:22:10,863 - INFO - - Source class: 1
2024-12-28 15:22:10,863 - INFO - - Target class: 0
2024-12-28 15:22:10,863 - INFO - - Available samples in source class: 918
2024-12-28 15:22:10,863 - INFO - - Requested samples to poison: 987
2024-12-28 15:22:10,863 - INFO - - Actual samples to flip: 917
2024-12-28 15:22:10,863 - INFO - - Samples remaining in source class: 1
2024-12-28 15:22:10,863 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:22:10,864 - INFO - Total number of labels flipped: 917
2024-12-28 15:22:10,864 - INFO - Label flipping completed in 0.00s
2024-12-28 15:22:10,864 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:22:10,864 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:22:12,080 - INFO - Feature scaling completed in 1.22s
2024-12-28 15:22:12,080 - INFO - Starting feature selection (k=50)
2024-12-28 15:22:12,099 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 15:22:12,100 - INFO - Starting anomaly detection
2024-12-28 15:22:19,235 - INFO - Anomaly detection completed in 7.14s
2024-12-28 15:22:19,236 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:22:19,236 - INFO - Total fit_transform time: 8.37s
2024-12-28 15:22:19,236 - INFO - Training set processing completed in 8.37s
2024-12-28 15:22:19,236 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:22:19,238 - INFO - Memory usage at start_fit: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:19,238 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:22:19,801 - INFO - Fitted scaler and transformed data
2024-12-28 15:22:19,802 - INFO - Scaling time: 0.56s
2024-12-28 15:22:19,815 - INFO - Training completed in 0.58s
2024-12-28 15:22:19,816 - INFO - Final memory usage: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:19,816 - INFO - Model training completed in 0.58s
2024-12-28 15:22:20,135 - INFO - Prediction completed in 0.32s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:22:20,147 - INFO - Poison rate 0.05 completed in 9.29s
2024-12-28 15:22:20,147 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:22:20,148 - INFO - Label flipping details:
2024-12-28 15:22:20,148 - INFO - - Source class: 1
2024-12-28 15:22:20,148 - INFO - - Target class: 0
2024-12-28 15:22:20,148 - INFO - - Available samples in source class: 918
2024-12-28 15:22:20,149 - INFO - - Requested samples to poison: 1382
2024-12-28 15:22:20,149 - INFO - - Actual samples to flip: 917
2024-12-28 15:22:20,149 - INFO - - Samples remaining in source class: 1
2024-12-28 15:22:20,149 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:22:20,149 - INFO - Total number of labels flipped: 917
2024-12-28 15:22:20,149 - INFO - Label flipping completed in 0.00s
2024-12-28 15:22:20,149 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:22:20,149 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:22:21,350 - INFO - Feature scaling completed in 1.20s
2024-12-28 15:22:21,350 - INFO - Starting feature selection (k=50)
2024-12-28 15:22:21,374 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 15:22:21,375 - INFO - Starting anomaly detection
2024-12-28 15:22:29,077 - INFO - Anomaly detection completed in 7.70s
2024-12-28 15:22:29,077 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:22:29,078 - INFO - Total fit_transform time: 8.93s
2024-12-28 15:22:29,078 - INFO - Training set processing completed in 8.93s
2024-12-28 15:22:29,078 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:22:29,079 - INFO - Memory usage at start_fit: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:29,079 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:22:29,648 - INFO - Fitted scaler and transformed data
2024-12-28 15:22:29,648 - INFO - Scaling time: 0.57s
2024-12-28 15:22:29,662 - INFO - Training completed in 0.58s
2024-12-28 15:22:29,663 - INFO - Final memory usage: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:29,663 - INFO - Model training completed in 0.58s
2024-12-28 15:22:29,979 - INFO - Prediction completed in 0.32s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:22:29,990 - INFO - Poison rate 0.07 completed in 9.84s
2024-12-28 15:22:29,990 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:22:29,992 - INFO - Label flipping details:
2024-12-28 15:22:29,992 - INFO - - Source class: 1
2024-12-28 15:22:29,992 - INFO - - Target class: 0
2024-12-28 15:22:29,992 - INFO - - Available samples in source class: 918
2024-12-28 15:22:29,992 - INFO - - Requested samples to poison: 1975
2024-12-28 15:22:29,992 - INFO - - Actual samples to flip: 917
2024-12-28 15:22:29,992 - INFO - - Samples remaining in source class: 1
2024-12-28 15:22:29,992 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:22:29,992 - INFO - Total number of labels flipped: 917
2024-12-28 15:22:29,992 - INFO - Label flipping completed in 0.00s
2024-12-28 15:22:29,992 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:22:29,992 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:22:31,241 - INFO - Feature scaling completed in 1.25s
2024-12-28 15:22:31,241 - INFO - Starting feature selection (k=50)
2024-12-28 15:22:31,255 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 15:22:31,255 - INFO - Starting anomaly detection
2024-12-28 15:22:39,401 - INFO - Anomaly detection completed in 8.15s
2024-12-28 15:22:39,401 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:22:39,401 - INFO - Total fit_transform time: 9.41s
2024-12-28 15:22:39,401 - INFO - Training set processing completed in 9.41s
2024-12-28 15:22:39,401 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:22:39,402 - INFO - Memory usage at start_fit: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:39,402 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:22:39,920 - INFO - Fitted scaler and transformed data
2024-12-28 15:22:39,920 - INFO - Scaling time: 0.52s
2024-12-28 15:22:39,936 - INFO - Training completed in 0.53s
2024-12-28 15:22:39,936 - INFO - Final memory usage: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:39,936 - INFO - Model training completed in 0.54s
2024-12-28 15:22:40,238 - INFO - Prediction completed in 0.30s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:22:40,250 - INFO - Poison rate 0.1 completed in 10.26s
2024-12-28 15:22:40,250 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:22:40,251 - INFO - Label flipping details:
2024-12-28 15:22:40,251 - INFO - - Source class: 1
2024-12-28 15:22:40,251 - INFO - - Target class: 0
2024-12-28 15:22:40,251 - INFO - - Available samples in source class: 918
2024-12-28 15:22:40,251 - INFO - - Requested samples to poison: 3951
2024-12-28 15:22:40,251 - INFO - - Actual samples to flip: 917
2024-12-28 15:22:40,251 - INFO - - Samples remaining in source class: 1
2024-12-28 15:22:40,252 - INFO - Successfully flipped 917 labels from class 1 to 0
2024-12-28 15:22:40,252 - INFO - Total number of labels flipped: 917
2024-12-28 15:22:40,252 - INFO - Label flipping completed in 0.00s
2024-12-28 15:22:40,252 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:22:40,252 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:22:41,449 - INFO - Feature scaling completed in 1.20s
2024-12-28 15:22:41,450 - INFO - Starting feature selection (k=50)
2024-12-28 15:22:41,467 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 15:22:41,468 - INFO - Starting anomaly detection
2024-12-28 15:22:48,463 - INFO - Anomaly detection completed in 6.99s
2024-12-28 15:22:48,463 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:22:48,463 - INFO - Total fit_transform time: 8.21s
2024-12-28 15:22:48,463 - INFO - Training set processing completed in 8.21s
2024-12-28 15:22:48,463 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:22:48,464 - INFO - Memory usage at start_fit: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:48,464 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:22:48,993 - INFO - Fitted scaler and transformed data
2024-12-28 15:22:48,993 - INFO - Scaling time: 0.53s
2024-12-28 15:22:49,007 - INFO - Training completed in 0.54s
2024-12-28 15:22:49,008 - INFO - Final memory usage: CPU 1886.5 MB, GPU 142.1 MB
2024-12-28 15:22:49,008 - INFO - Model training completed in 0.55s
2024-12-28 15:22:49,364 - INFO - Prediction completed in 0.36s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:22:49,383 - INFO - Poison rate 0.2 completed in 9.13s
2024-12-28 15:22:49,388 - INFO - Loaded 161 existing results
2024-12-28 15:22:49,388 - INFO - Total results to save: 168
2024-12-28 15:22:49,389 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:22:49,399 - INFO - Saved 168 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:22:49,400 - INFO - Total evaluation time: 91.55s
2024-12-28 15:22:49,407 - INFO - Completed evaluation for GTSRB
2024-12-28 15:22:49,407 - INFO - 
Processing dataset: GTSRB
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:22:49,575 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:22:49,576 - INFO - Dataset type: image
2024-12-28 15:22:49,576 - INFO - Sample size: 39209
2024-12-28 15:22:49,576 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:22:49,576 - INFO - 
Progress: 26.0% - Evaluating GTSRB with SVM (standard mode, iteration 1/1)
2024-12-28 15:22:49,742 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:22:49,742 - INFO - Dataset type: image
2024-12-28 15:22:49,742 - INFO - Sample size: 39209
2024-12-28 15:22:49,742 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:22:49,743 - INFO - Loading datasets...
2024-12-28 15:23:08,887 - INFO - Dataset loading completed in 19.14s
2024-12-28 15:23:08,887 - INFO - Extracting validation features...
2024-12-28 15:23:08,887 - INFO - Extracting features from 4435 samples...
2024-12-28 15:23:09,663 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:23:09,667 - INFO - Validation feature extraction completed in 0.78s
2024-12-28 15:23:09,668 - INFO - Extracting training features...
2024-12-28 15:23:09,668 - INFO - Extracting features from 19755 samples...
2024-12-28 15:23:12,365 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:23:12,372 - INFO - Training feature extraction completed in 2.70s
2024-12-28 15:23:12,373 - INFO - Creating model for classifier: SVM
2024-12-28 15:23:12,373 - INFO - Using device: cuda
2024-12-28 15:23:12,373 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 15:23:12,373 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:23:12,373 - INFO - Training set processing completed in 0.00s
2024-12-28 15:23:12,373 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:23:12,375 - INFO - Memory usage at start_fit: CPU 1867.3 MB, GPU 104.0 MB
2024-12-28 15:23:12,375 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:23:12,382 - INFO - Number of unique classes: 43
2024-12-28 15:23:12,527 - INFO - Fitted scaler and transformed data
2024-12-28 15:23:12,528 - INFO - Scaling time: 0.14s
2024-12-28 15:23:13,679 - INFO - Epoch 1/500, Train Loss: 6.9266, Val Loss: 3.3386
2024-12-28 15:23:14,782 - INFO - Epoch 2/500, Train Loss: 2.4958, Val Loss: 2.3636
2024-12-28 15:23:15,920 - INFO - Epoch 3/500, Train Loss: 1.8112, Val Loss: 1.8919
2024-12-28 15:23:17,034 - INFO - Epoch 4/500, Train Loss: 1.4739, Val Loss: 1.6518
2024-12-28 15:23:18,124 - INFO - Epoch 5/500, Train Loss: 1.2605, Val Loss: 1.4681
2024-12-28 15:23:19,296 - INFO - Epoch 6/500, Train Loss: 1.1084, Val Loss: 1.3611
2024-12-28 15:23:20,440 - INFO - Epoch 7/500, Train Loss: 0.9969, Val Loss: 1.2370
2024-12-28 15:23:21,690 - INFO - Epoch 8/500, Train Loss: 0.9149, Val Loss: 1.2120
2024-12-28 15:23:22,883 - INFO - Epoch 9/500, Train Loss: 0.8488, Val Loss: 1.1288
2024-12-28 15:23:24,005 - INFO - Epoch 10/500, Train Loss: 0.7905, Val Loss: 1.0855
2024-12-28 15:23:24,993 - INFO - Epoch 11/500, Train Loss: 0.7414, Val Loss: 1.0462
2024-12-28 15:23:26,009 - INFO - Epoch 12/500, Train Loss: 0.7055, Val Loss: 1.0445
2024-12-28 15:23:27,039 - INFO - Epoch 13/500, Train Loss: 0.6761, Val Loss: 0.9873
2024-12-28 15:23:28,137 - INFO - Epoch 14/500, Train Loss: 0.6465, Val Loss: 0.9595
2024-12-28 15:23:29,275 - INFO - Epoch 15/500, Train Loss: 0.6225, Val Loss: 0.9508
2024-12-28 15:23:30,375 - INFO - Epoch 16/500, Train Loss: 0.5960, Val Loss: 0.9600
2024-12-28 15:23:31,396 - INFO - Epoch 17/500, Train Loss: 0.5874, Val Loss: 0.9371
2024-12-28 15:23:32,495 - INFO - Epoch 18/500, Train Loss: 0.5671, Val Loss: 0.9321
2024-12-28 15:23:33,493 - INFO - Epoch 19/500, Train Loss: 0.5566, Val Loss: 0.9235
2024-12-28 15:23:34,622 - INFO - Epoch 20/500, Train Loss: 0.5445, Val Loss: 0.8950
2024-12-28 15:23:35,782 - INFO - Epoch 21/500, Train Loss: 0.5316, Val Loss: 0.8978
2024-12-28 15:23:36,819 - INFO - Epoch 22/500, Train Loss: 0.5206, Val Loss: 0.9135
2024-12-28 15:23:37,817 - INFO - Epoch 23/500, Train Loss: 0.5201, Val Loss: 0.8875
2024-12-28 15:23:38,842 - INFO - Epoch 24/500, Train Loss: 0.5092, Val Loss: 0.8609
2024-12-28 15:23:39,810 - INFO - Epoch 25/500, Train Loss: 0.5009, Val Loss: 0.8456
2024-12-28 15:23:40,753 - INFO - Epoch 26/500, Train Loss: 0.4980, Val Loss: 0.8629
2024-12-28 15:23:41,714 - INFO - Epoch 27/500, Train Loss: 0.4892, Val Loss: 0.8621
2024-12-28 15:23:42,689 - INFO - Epoch 28/500, Train Loss: 0.4859, Val Loss: 0.8368
2024-12-28 15:23:43,717 - INFO - Epoch 29/500, Train Loss: 0.4821, Val Loss: 0.8455
2024-12-28 15:23:44,748 - INFO - Epoch 30/500, Train Loss: 0.4783, Val Loss: 0.8834
2024-12-28 15:23:45,704 - INFO - Epoch 31/500, Train Loss: 0.4767, Val Loss: 0.8554
2024-12-28 15:23:46,704 - INFO - Epoch 32/500, Train Loss: 0.4678, Val Loss: 0.8427
2024-12-28 15:23:47,734 - INFO - Epoch 33/500, Train Loss: 0.4698, Val Loss: 0.8388
2024-12-28 15:23:47,735 - INFO - Early stopping triggered at epoch 33
2024-12-28 15:23:47,735 - INFO - Training completed in 35.36s
2024-12-28 15:23:47,735 - INFO - Final memory usage: CPU 1915.4 MB, GPU 104.5 MB
2024-12-28 15:23:47,736 - INFO - Model training completed in 35.36s
2024-12-28 15:23:47,815 - INFO - Prediction completed in 0.08s
2024-12-28 15:23:47,826 - INFO - Poison rate 0.0 completed in 35.45s
2024-12-28 15:23:47,827 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:23:47,832 - INFO - Total number of labels flipped: 197
2024-12-28 15:23:47,832 - INFO - Label flipping completed in 0.01s
2024-12-28 15:23:47,832 - INFO - Training set processing completed in 0.00s
2024-12-28 15:23:47,832 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:23:47,833 - INFO - Memory usage at start_fit: CPU 1876.8 MB, GPU 104.3 MB
2024-12-28 15:23:47,833 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:23:47,838 - INFO - Number of unique classes: 43
2024-12-28 15:23:47,977 - INFO - Fitted scaler and transformed data
2024-12-28 15:23:47,978 - INFO - Scaling time: 0.14s
2024-12-28 15:23:49,120 - INFO - Epoch 1/500, Train Loss: 7.5487, Val Loss: 4.1577
2024-12-28 15:23:50,319 - INFO - Epoch 2/500, Train Loss: 3.0105, Val Loss: 3.1570
2024-12-28 15:23:51,417 - INFO - Epoch 3/500, Train Loss: 2.2357, Val Loss: 2.7203
2024-12-28 15:23:52,391 - INFO - Epoch 4/500, Train Loss: 1.8149, Val Loss: 2.4676
2024-12-28 15:23:53,372 - INFO - Epoch 5/500, Train Loss: 1.5575, Val Loss: 2.3349
2024-12-28 15:23:54,359 - INFO - Epoch 6/500, Train Loss: 1.3633, Val Loss: 2.1936
2024-12-28 15:23:55,371 - INFO - Epoch 7/500, Train Loss: 1.2301, Val Loss: 2.1712
2024-12-28 15:23:56,322 - INFO - Epoch 8/500, Train Loss: 1.1240, Val Loss: 2.0853
2024-12-28 15:23:57,269 - INFO - Epoch 9/500, Train Loss: 1.0435, Val Loss: 2.0245
2024-12-28 15:23:58,285 - INFO - Epoch 10/500, Train Loss: 0.9841, Val Loss: 2.0110
2024-12-28 15:23:59,395 - INFO - Epoch 11/500, Train Loss: 0.9220, Val Loss: 1.9619
2024-12-28 15:24:00,521 - INFO - Epoch 12/500, Train Loss: 0.8745, Val Loss: 1.9551
2024-12-28 15:24:01,644 - INFO - Epoch 13/500, Train Loss: 0.8375, Val Loss: 1.8897
2024-12-28 15:24:02,768 - INFO - Epoch 14/500, Train Loss: 0.7941, Val Loss: 1.8900
2024-12-28 15:24:03,855 - INFO - Epoch 15/500, Train Loss: 0.7806, Val Loss: 1.8689
2024-12-28 15:24:04,878 - INFO - Epoch 16/500, Train Loss: 0.7429, Val Loss: 1.8705
2024-12-28 15:24:05,918 - INFO - Epoch 17/500, Train Loss: 0.7278, Val Loss: 1.8196
2024-12-28 15:24:07,092 - INFO - Epoch 18/500, Train Loss: 0.7045, Val Loss: 1.7906
2024-12-28 15:24:08,307 - INFO - Epoch 19/500, Train Loss: 0.6859, Val Loss: 1.8707
2024-12-28 15:24:09,572 - INFO - Epoch 20/500, Train Loss: 0.6731, Val Loss: 1.7861
2024-12-28 15:24:10,662 - INFO - Epoch 21/500, Train Loss: 0.6522, Val Loss: 1.7761
2024-12-28 15:24:11,732 - INFO - Epoch 22/500, Train Loss: 0.6462, Val Loss: 1.8314
2024-12-28 15:24:12,825 - INFO - Epoch 23/500, Train Loss: 0.6359, Val Loss: 1.7989
2024-12-28 15:24:13,894 - INFO - Epoch 24/500, Train Loss: 0.6291, Val Loss: 1.8278
2024-12-28 15:24:15,002 - INFO - Epoch 25/500, Train Loss: 0.6203, Val Loss: 1.7559
2024-12-28 15:24:16,182 - INFO - Epoch 26/500, Train Loss: 0.6096, Val Loss: 1.7572
2024-12-28 15:24:17,350 - INFO - Epoch 27/500, Train Loss: 0.6031, Val Loss: 1.7364
2024-12-28 15:24:18,525 - INFO - Epoch 28/500, Train Loss: 0.5979, Val Loss: 1.7292
2024-12-28 15:24:19,624 - INFO - Epoch 29/500, Train Loss: 0.5952, Val Loss: 1.7353
2024-12-28 15:24:20,697 - INFO - Epoch 30/500, Train Loss: 0.5852, Val Loss: 1.6871
2024-12-28 15:24:21,785 - INFO - Epoch 31/500, Train Loss: 0.5837, Val Loss: 1.7355
2024-12-28 15:24:22,913 - INFO - Epoch 32/500, Train Loss: 0.5883, Val Loss: 1.7125
2024-12-28 15:24:23,980 - INFO - Epoch 33/500, Train Loss: 0.5751, Val Loss: 1.7190
2024-12-28 15:24:25,086 - INFO - Epoch 34/500, Train Loss: 0.5741, Val Loss: 1.7316
2024-12-28 15:24:26,219 - INFO - Epoch 35/500, Train Loss: 0.5739, Val Loss: 1.7353
2024-12-28 15:24:26,219 - INFO - Early stopping triggered at epoch 35
2024-12-28 15:24:26,219 - INFO - Training completed in 38.39s
2024-12-28 15:24:26,220 - INFO - Final memory usage: CPU 1915.4 MB, GPU 104.5 MB
2024-12-28 15:24:26,220 - INFO - Model training completed in 38.39s
2024-12-28 15:24:26,283 - INFO - Prediction completed in 0.06s
2024-12-28 15:24:26,294 - INFO - Poison rate 0.01 completed in 38.47s
2024-12-28 15:24:26,294 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:24:26,306 - INFO - Total number of labels flipped: 592
2024-12-28 15:24:26,306 - INFO - Label flipping completed in 0.01s
2024-12-28 15:24:26,306 - INFO - Training set processing completed in 0.00s
2024-12-28 15:24:26,306 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:24:26,307 - INFO - Memory usage at start_fit: CPU 1876.8 MB, GPU 104.3 MB
2024-12-28 15:24:26,307 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:24:26,313 - INFO - Number of unique classes: 43
2024-12-28 15:24:26,457 - INFO - Fitted scaler and transformed data
2024-12-28 15:24:26,457 - INFO - Scaling time: 0.14s
2024-12-28 15:24:27,666 - INFO - Epoch 1/500, Train Loss: 8.3146, Val Loss: 4.9687
2024-12-28 15:24:28,803 - INFO - Epoch 2/500, Train Loss: 3.9876, Val Loss: 4.0029
2024-12-28 15:24:30,053 - INFO - Epoch 3/500, Train Loss: 3.0513, Val Loss: 3.7035
2024-12-28 15:24:31,231 - INFO - Epoch 4/500, Train Loss: 2.5394, Val Loss: 3.4827
2024-12-28 15:24:32,488 - INFO - Epoch 5/500, Train Loss: 2.1761, Val Loss: 3.3841
2024-12-28 15:24:33,719 - INFO - Epoch 6/500, Train Loss: 1.9492, Val Loss: 3.2839
2024-12-28 15:24:34,849 - INFO - Epoch 7/500, Train Loss: 1.7502, Val Loss: 3.1527
2024-12-28 15:24:36,065 - INFO - Epoch 8/500, Train Loss: 1.6057, Val Loss: 3.0851
2024-12-28 15:24:37,326 - INFO - Epoch 9/500, Train Loss: 1.4914, Val Loss: 3.0639
2024-12-28 15:24:38,554 - INFO - Epoch 10/500, Train Loss: 1.3965, Val Loss: 3.0340
2024-12-28 15:24:39,780 - INFO - Epoch 11/500, Train Loss: 1.3181, Val Loss: 2.9733
2024-12-28 15:24:41,008 - INFO - Epoch 12/500, Train Loss: 1.2493, Val Loss: 2.9658
2024-12-28 15:24:42,200 - INFO - Epoch 13/500, Train Loss: 1.1892, Val Loss: 2.9543
2024-12-28 15:24:43,349 - INFO - Epoch 14/500, Train Loss: 1.1475, Val Loss: 3.0089
2024-12-28 15:24:44,561 - INFO - Epoch 15/500, Train Loss: 1.0989, Val Loss: 2.9001
2024-12-28 15:24:45,750 - INFO - Epoch 16/500, Train Loss: 1.0664, Val Loss: 2.9182
2024-12-28 15:24:46,812 - INFO - Epoch 17/500, Train Loss: 1.0349, Val Loss: 2.9298
2024-12-28 15:24:47,898 - INFO - Epoch 18/500, Train Loss: 1.0094, Val Loss: 2.8851
2024-12-28 15:24:48,939 - INFO - Epoch 19/500, Train Loss: 0.9847, Val Loss: 2.9139
2024-12-28 15:24:50,011 - INFO - Epoch 20/500, Train Loss: 0.9555, Val Loss: 2.8250
2024-12-28 15:24:51,063 - INFO - Epoch 21/500, Train Loss: 0.9422, Val Loss: 2.8511
2024-12-28 15:24:52,050 - INFO - Epoch 22/500, Train Loss: 0.9239, Val Loss: 2.8343
2024-12-28 15:24:53,066 - INFO - Epoch 23/500, Train Loss: 0.9051, Val Loss: 2.8053
2024-12-28 15:24:54,057 - INFO - Epoch 24/500, Train Loss: 0.8930, Val Loss: 2.8297
2024-12-28 15:24:55,066 - INFO - Epoch 25/500, Train Loss: 0.8829, Val Loss: 2.7785
2024-12-28 15:24:56,113 - INFO - Epoch 26/500, Train Loss: 0.8703, Val Loss: 2.7889
2024-12-28 15:24:57,197 - INFO - Epoch 27/500, Train Loss: 0.8467, Val Loss: 2.8161
2024-12-28 15:24:58,265 - INFO - Epoch 28/500, Train Loss: 0.8424, Val Loss: 2.7493
2024-12-28 15:24:59,268 - INFO - Epoch 29/500, Train Loss: 0.8297, Val Loss: 2.8049
2024-12-28 15:25:00,267 - INFO - Epoch 30/500, Train Loss: 0.8255, Val Loss: 2.7427
2024-12-28 15:25:01,303 - INFO - Epoch 31/500, Train Loss: 0.8231, Val Loss: 2.7813
2024-12-28 15:25:02,373 - INFO - Epoch 32/500, Train Loss: 0.8216, Val Loss: 2.7688
2024-12-28 15:25:03,411 - INFO - Epoch 33/500, Train Loss: 0.8004, Val Loss: 2.7730
2024-12-28 15:25:04,506 - INFO - Epoch 34/500, Train Loss: 0.8088, Val Loss: 2.7314
2024-12-28 15:25:05,539 - INFO - Epoch 35/500, Train Loss: 0.8030, Val Loss: 2.7966
2024-12-28 15:25:06,637 - INFO - Epoch 36/500, Train Loss: 0.7945, Val Loss: 2.7530
2024-12-28 15:25:07,789 - INFO - Epoch 37/500, Train Loss: 0.7831, Val Loss: 2.7877
2024-12-28 15:25:09,037 - INFO - Epoch 38/500, Train Loss: 0.7817, Val Loss: 2.7725
2024-12-28 15:25:10,183 - INFO - Epoch 39/500, Train Loss: 0.7805, Val Loss: 2.7538
2024-12-28 15:25:10,183 - INFO - Early stopping triggered at epoch 39
2024-12-28 15:25:10,183 - INFO - Training completed in 43.88s
2024-12-28 15:25:10,183 - INFO - Final memory usage: CPU 1915.4 MB, GPU 104.5 MB
2024-12-28 15:25:10,184 - INFO - Model training completed in 43.88s
2024-12-28 15:25:10,248 - INFO - Prediction completed in 0.06s
2024-12-28 15:25:10,259 - INFO - Poison rate 0.03 completed in 43.97s
2024-12-28 15:25:10,259 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:25:10,278 - INFO - Total number of labels flipped: 987
2024-12-28 15:25:10,278 - INFO - Label flipping completed in 0.02s
2024-12-28 15:25:10,278 - INFO - Training set processing completed in 0.00s
2024-12-28 15:25:10,278 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:25:10,279 - INFO - Memory usage at start_fit: CPU 1876.8 MB, GPU 104.3 MB
2024-12-28 15:25:10,279 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:25:10,285 - INFO - Number of unique classes: 43
2024-12-28 15:25:10,438 - INFO - Fitted scaler and transformed data
2024-12-28 15:25:10,438 - INFO - Scaling time: 0.15s
2024-12-28 15:25:11,546 - INFO - Epoch 1/500, Train Loss: 9.8824, Val Loss: 6.1982
2024-12-28 15:25:12,643 - INFO - Epoch 2/500, Train Loss: 4.9370, Val Loss: 5.3013
2024-12-28 15:25:13,635 - INFO - Epoch 3/500, Train Loss: 3.8764, Val Loss: 4.7752
2024-12-28 15:25:14,687 - INFO - Epoch 4/500, Train Loss: 3.2594, Val Loss: 4.5584
2024-12-28 15:25:15,712 - INFO - Epoch 5/500, Train Loss: 2.8380, Val Loss: 4.3273
2024-12-28 15:25:16,819 - INFO - Epoch 6/500, Train Loss: 2.5279, Val Loss: 4.2038
2024-12-28 15:25:17,903 - INFO - Epoch 7/500, Train Loss: 2.2965, Val Loss: 4.1626
2024-12-28 15:25:18,895 - INFO - Epoch 8/500, Train Loss: 2.1148, Val Loss: 4.0975
2024-12-28 15:25:19,895 - INFO - Epoch 9/500, Train Loss: 1.9739, Val Loss: 4.0011
2024-12-28 15:25:20,905 - INFO - Epoch 10/500, Train Loss: 1.8574, Val Loss: 4.0348
2024-12-28 15:25:22,005 - INFO - Epoch 11/500, Train Loss: 1.7537, Val Loss: 3.9627
2024-12-28 15:25:23,169 - INFO - Epoch 12/500, Train Loss: 1.6798, Val Loss: 3.9422
2024-12-28 15:25:24,344 - INFO - Epoch 13/500, Train Loss: 1.5924, Val Loss: 3.8576
2024-12-28 15:25:25,547 - INFO - Epoch 14/500, Train Loss: 1.5367, Val Loss: 3.8754
2024-12-28 15:25:26,836 - INFO - Epoch 15/500, Train Loss: 1.4882, Val Loss: 3.9536
2024-12-28 15:25:27,964 - INFO - Epoch 16/500, Train Loss: 1.4292, Val Loss: 3.8899
2024-12-28 15:25:29,134 - INFO - Epoch 17/500, Train Loss: 1.3955, Val Loss: 3.8238
2024-12-28 15:25:30,192 - INFO - Epoch 18/500, Train Loss: 1.3549, Val Loss: 3.8977
2024-12-28 15:25:31,279 - INFO - Epoch 19/500, Train Loss: 1.3257, Val Loss: 3.7809
2024-12-28 15:25:32,395 - INFO - Epoch 20/500, Train Loss: 1.2859, Val Loss: 3.9004
2024-12-28 15:25:33,611 - INFO - Epoch 21/500, Train Loss: 1.2739, Val Loss: 3.7699
2024-12-28 15:25:34,736 - INFO - Epoch 22/500, Train Loss: 1.2403, Val Loss: 3.7978
2024-12-28 15:25:35,844 - INFO - Epoch 23/500, Train Loss: 1.2220, Val Loss: 3.7979
2024-12-28 15:25:36,990 - INFO - Epoch 24/500, Train Loss: 1.2031, Val Loss: 3.7420
2024-12-28 15:25:38,230 - INFO - Epoch 25/500, Train Loss: 1.1870, Val Loss: 3.7091
2024-12-28 15:25:39,290 - INFO - Epoch 26/500, Train Loss: 1.1669, Val Loss: 3.7609
2024-12-28 15:25:40,435 - INFO - Epoch 27/500, Train Loss: 1.1483, Val Loss: 3.7839
2024-12-28 15:25:41,588 - INFO - Epoch 28/500, Train Loss: 1.1385, Val Loss: 3.7237
2024-12-28 15:25:42,612 - INFO - Epoch 29/500, Train Loss: 1.1210, Val Loss: 3.8180
2024-12-28 15:25:43,627 - INFO - Epoch 30/500, Train Loss: 1.1090, Val Loss: 3.6791
2024-12-28 15:25:44,617 - INFO - Epoch 31/500, Train Loss: 1.1091, Val Loss: 3.9009
2024-12-28 15:25:45,646 - INFO - Epoch 32/500, Train Loss: 1.0889, Val Loss: 3.7568
2024-12-28 15:25:46,745 - INFO - Epoch 33/500, Train Loss: 1.0777, Val Loss: 3.7644
2024-12-28 15:25:47,883 - INFO - Epoch 34/500, Train Loss: 1.0769, Val Loss: 3.7824
2024-12-28 15:25:48,968 - INFO - Epoch 35/500, Train Loss: 1.0650, Val Loss: 3.7499
2024-12-28 15:25:48,969 - INFO - Early stopping triggered at epoch 35
2024-12-28 15:25:48,969 - INFO - Training completed in 38.69s
2024-12-28 15:25:48,969 - INFO - Final memory usage: CPU 1915.4 MB, GPU 104.5 MB
2024-12-28 15:25:48,969 - INFO - Model training completed in 38.69s
2024-12-28 15:25:49,032 - INFO - Prediction completed in 0.06s
2024-12-28 15:25:49,043 - INFO - Poison rate 0.05 completed in 38.78s
2024-12-28 15:25:49,043 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:25:49,070 - INFO - Total number of labels flipped: 1382
2024-12-28 15:25:49,070 - INFO - Label flipping completed in 0.03s
2024-12-28 15:25:49,070 - INFO - Training set processing completed in 0.00s
2024-12-28 15:25:49,070 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:25:49,071 - INFO - Memory usage at start_fit: CPU 1876.8 MB, GPU 104.3 MB
2024-12-28 15:25:49,071 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:25:49,077 - INFO - Number of unique classes: 43
2024-12-28 15:25:49,228 - INFO - Fitted scaler and transformed data
2024-12-28 15:25:49,228 - INFO - Scaling time: 0.15s
2024-12-28 15:25:50,458 - INFO - Epoch 1/500, Train Loss: 10.4168, Val Loss: 7.2414
2024-12-28 15:25:51,745 - INFO - Epoch 2/500, Train Loss: 5.8431, Val Loss: 6.3337
2024-12-28 15:25:52,971 - INFO - Epoch 3/500, Train Loss: 4.7184, Val Loss: 6.0612
2024-12-28 15:25:54,185 - INFO - Epoch 4/500, Train Loss: 4.0188, Val Loss: 5.8857
2024-12-28 15:25:55,345 - INFO - Epoch 5/500, Train Loss: 3.5486, Val Loss: 5.7709
2024-12-28 15:25:56,443 - INFO - Epoch 6/500, Train Loss: 3.2070, Val Loss: 5.6088
2024-12-28 15:25:57,586 - INFO - Epoch 7/500, Train Loss: 2.9310, Val Loss: 5.5163
2024-12-28 15:25:58,629 - INFO - Epoch 8/500, Train Loss: 2.7391, Val Loss: 5.5149
2024-12-28 15:25:59,747 - INFO - Epoch 9/500, Train Loss: 2.5506, Val Loss: 5.4427
2024-12-28 15:26:00,910 - INFO - Epoch 10/500, Train Loss: 2.4113, Val Loss: 5.4091
2024-12-28 15:26:02,163 - INFO - Epoch 11/500, Train Loss: 2.2867, Val Loss: 5.4626
2024-12-28 15:26:03,217 - INFO - Epoch 12/500, Train Loss: 2.1895, Val Loss: 5.3294
2024-12-28 15:26:04,304 - INFO - Epoch 13/500, Train Loss: 2.0896, Val Loss: 5.3940
2024-12-28 15:26:05,322 - INFO - Epoch 14/500, Train Loss: 2.0319, Val Loss: 5.3674
2024-12-28 15:26:06,466 - INFO - Epoch 15/500, Train Loss: 1.9483, Val Loss: 5.4586
2024-12-28 15:26:07,562 - INFO - Epoch 16/500, Train Loss: 1.9035, Val Loss: 5.4306
2024-12-28 15:26:08,637 - INFO - Epoch 17/500, Train Loss: 1.8342, Val Loss: 5.3280
2024-12-28 15:26:09,769 - INFO - Epoch 18/500, Train Loss: 1.7913, Val Loss: 5.3434
2024-12-28 15:26:10,881 - INFO - Epoch 19/500, Train Loss: 1.7451, Val Loss: 5.3325
2024-12-28 15:26:12,056 - INFO - Epoch 20/500, Train Loss: 1.7148, Val Loss: 5.2379
2024-12-28 15:26:13,232 - INFO - Epoch 21/500, Train Loss: 1.6729, Val Loss: 5.4147
2024-12-28 15:26:14,291 - INFO - Epoch 22/500, Train Loss: 1.6415, Val Loss: 5.3526
2024-12-28 15:26:15,346 - INFO - Epoch 23/500, Train Loss: 1.6066, Val Loss: 5.4169
2024-12-28 15:26:16,387 - INFO - Epoch 24/500, Train Loss: 1.5876, Val Loss: 5.3110
2024-12-28 15:26:17,633 - INFO - Epoch 25/500, Train Loss: 1.5596, Val Loss: 5.3628
2024-12-28 15:26:17,633 - INFO - Early stopping triggered at epoch 25
2024-12-28 15:26:17,634 - INFO - Training completed in 28.56s
2024-12-28 15:26:17,634 - INFO - Final memory usage: CPU 1915.4 MB, GPU 104.5 MB
2024-12-28 15:26:17,635 - INFO - Model training completed in 28.56s
2024-12-28 15:26:17,708 - INFO - Prediction completed in 0.07s
2024-12-28 15:26:17,720 - INFO - Poison rate 0.07 completed in 28.68s
2024-12-28 15:26:17,720 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:26:17,756 - INFO - Total number of labels flipped: 1975
2024-12-28 15:26:17,756 - INFO - Label flipping completed in 0.04s
2024-12-28 15:26:17,757 - INFO - Training set processing completed in 0.00s
2024-12-28 15:26:17,757 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:26:17,757 - INFO - Memory usage at start_fit: CPU 1876.8 MB, GPU 104.3 MB
2024-12-28 15:26:17,758 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:26:17,764 - INFO - Number of unique classes: 43
2024-12-28 15:26:17,923 - INFO - Fitted scaler and transformed data
2024-12-28 15:26:17,923 - INFO - Scaling time: 0.16s
2024-12-28 15:26:19,171 - INFO - Epoch 1/500, Train Loss: 12.1333, Val Loss: 8.6672
2024-12-28 15:26:20,362 - INFO - Epoch 2/500, Train Loss: 7.2714, Val Loss: 7.7829
2024-12-28 15:26:21,598 - INFO - Epoch 3/500, Train Loss: 5.9812, Val Loss: 7.3213
2024-12-28 15:26:22,683 - INFO - Epoch 4/500, Train Loss: 5.2118, Val Loss: 7.1171
2024-12-28 15:26:23,886 - INFO - Epoch 5/500, Train Loss: 4.6643, Val Loss: 7.0281
2024-12-28 15:26:25,112 - INFO - Epoch 6/500, Train Loss: 4.2505, Val Loss: 7.0068
2024-12-28 15:26:26,315 - INFO - Epoch 7/500, Train Loss: 3.9323, Val Loss: 6.8635
2024-12-28 15:26:27,510 - INFO - Epoch 8/500, Train Loss: 3.6932, Val Loss: 6.9176
2024-12-28 15:26:28,689 - INFO - Epoch 9/500, Train Loss: 3.4787, Val Loss: 6.7928
2024-12-28 15:26:29,797 - INFO - Epoch 10/500, Train Loss: 3.3039, Val Loss: 6.8861
2024-12-28 15:26:30,855 - INFO - Epoch 11/500, Train Loss: 3.1486, Val Loss: 6.7709
2024-12-28 15:26:32,042 - INFO - Epoch 12/500, Train Loss: 3.0310, Val Loss: 6.6994
2024-12-28 15:26:33,148 - INFO - Epoch 13/500, Train Loss: 2.9007, Val Loss: 6.8551
2024-12-28 15:26:34,259 - INFO - Epoch 14/500, Train Loss: 2.8289, Val Loss: 6.6449
2024-12-28 15:26:35,458 - INFO - Epoch 15/500, Train Loss: 2.7060, Val Loss: 6.7118
2024-12-28 15:26:36,532 - INFO - Epoch 16/500, Train Loss: 2.6445, Val Loss: 6.7428
2024-12-28 15:26:37,601 - INFO - Epoch 17/500, Train Loss: 2.5678, Val Loss: 6.5901
2024-12-28 15:26:38,571 - INFO - Epoch 18/500, Train Loss: 2.5162, Val Loss: 6.7182
2024-12-28 15:26:39,607 - INFO - Epoch 19/500, Train Loss: 2.4437, Val Loss: 6.6570
2024-12-28 15:26:40,728 - INFO - Epoch 20/500, Train Loss: 2.3991, Val Loss: 6.6702
2024-12-28 15:26:41,863 - INFO - Epoch 21/500, Train Loss: 2.3495, Val Loss: 6.6320
2024-12-28 15:26:43,040 - INFO - Epoch 22/500, Train Loss: 2.3031, Val Loss: 6.7015
2024-12-28 15:26:43,040 - INFO - Early stopping triggered at epoch 22
2024-12-28 15:26:43,040 - INFO - Training completed in 25.28s
2024-12-28 15:26:43,041 - INFO - Final memory usage: CPU 1915.4 MB, GPU 104.5 MB
2024-12-28 15:26:43,041 - INFO - Model training completed in 25.28s
2024-12-28 15:26:43,104 - INFO - Prediction completed in 0.06s
2024-12-28 15:26:43,115 - INFO - Poison rate 0.1 completed in 25.40s
2024-12-28 15:26:43,115 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:26:43,187 - INFO - Total number of labels flipped: 3951
2024-12-28 15:26:43,188 - INFO - Label flipping completed in 0.07s
2024-12-28 15:26:43,188 - INFO - Training set processing completed in 0.00s
2024-12-28 15:26:43,188 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:26:43,189 - INFO - Memory usage at start_fit: CPU 1876.8 MB, GPU 104.3 MB
2024-12-28 15:26:43,189 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:26:43,195 - INFO - Number of unique classes: 43
2024-12-28 15:26:43,343 - INFO - Fitted scaler and transformed data
2024-12-28 15:26:43,344 - INFO - Scaling time: 0.15s
2024-12-28 15:26:44,515 - INFO - Epoch 1/500, Train Loss: 16.7992, Val Loss: 12.9703
2024-12-28 15:26:45,656 - INFO - Epoch 2/500, Train Loss: 11.6197, Val Loss: 12.2217
2024-12-28 15:26:46,689 - INFO - Epoch 3/500, Train Loss: 10.0313, Val Loss: 11.9809
2024-12-28 15:26:47,805 - INFO - Epoch 4/500, Train Loss: 9.0788, Val Loss: 11.6677
2024-12-28 15:26:48,944 - INFO - Epoch 5/500, Train Loss: 8.3699, Val Loss: 11.4853
2024-12-28 15:26:50,084 - INFO - Epoch 6/500, Train Loss: 7.8441, Val Loss: 11.5706
2024-12-28 15:26:51,166 - INFO - Epoch 7/500, Train Loss: 7.4245, Val Loss: 11.7095
2024-12-28 15:26:52,254 - INFO - Epoch 8/500, Train Loss: 7.0529, Val Loss: 11.5669
2024-12-28 15:26:53,331 - INFO - Epoch 9/500, Train Loss: 6.7887, Val Loss: 11.6012
2024-12-28 15:26:54,387 - INFO - Epoch 10/500, Train Loss: 6.5196, Val Loss: 11.6937
2024-12-28 15:26:54,387 - INFO - Early stopping triggered at epoch 10
2024-12-28 15:26:54,387 - INFO - Training completed in 11.20s
2024-12-28 15:26:54,388 - INFO - Final memory usage: CPU 1915.4 MB, GPU 104.5 MB
2024-12-28 15:26:54,389 - INFO - Model training completed in 11.20s
2024-12-28 15:26:54,454 - INFO - Prediction completed in 0.06s
2024-12-28 15:26:54,465 - INFO - Poison rate 0.2 completed in 11.35s
2024-12-28 15:26:54,471 - INFO - Loaded 168 existing results
2024-12-28 15:26:54,471 - INFO - Total results to save: 175
2024-12-28 15:26:54,472 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:26:54,482 - INFO - Saved 175 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:26:54,482 - INFO - Total evaluation time: 244.74s
2024-12-28 15:26:54,489 - INFO - 
Progress: 27.1% - Evaluating GTSRB with SVM (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:26:54,671 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:26:54,671 - INFO - Dataset type: image
2024-12-28 15:26:54,671 - INFO - Sample size: 39209
2024-12-28 15:26:54,672 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:26:54,672 - INFO - Loading datasets...
2024-12-28 15:27:13,243 - INFO - Dataset loading completed in 18.57s
2024-12-28 15:27:13,243 - INFO - Extracting validation features...
2024-12-28 15:27:13,243 - INFO - Extracting features from 4435 samples...
2024-12-28 15:27:14,021 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:27:14,027 - INFO - Validation feature extraction completed in 0.78s
2024-12-28 15:27:14,027 - INFO - Extracting training features...
2024-12-28 15:27:14,027 - INFO - Extracting features from 19755 samples...
2024-12-28 15:27:16,754 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:27:16,766 - INFO - Training feature extraction completed in 2.74s
2024-12-28 15:27:16,766 - INFO - Creating model for classifier: SVM
2024-12-28 15:27:16,766 - INFO - Using device: cuda
2024-12-28 15:27:16,766 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 15:27:16,766 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:27:16,766 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:27:16,766 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:27:17,924 - INFO - Feature scaling completed in 1.16s
2024-12-28 15:27:17,925 - INFO - Starting feature selection (k=50)
2024-12-28 15:27:17,953 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:27:17,953 - INFO - Starting anomaly detection
2024-12-28 15:27:23,268 - INFO - Anomaly detection completed in 5.31s
2024-12-28 15:27:23,268 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:27:23,268 - INFO - Total fit_transform time: 6.50s
2024-12-28 15:27:23,268 - INFO - Training set processing completed in 6.50s
2024-12-28 15:27:23,268 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:27:23,270 - INFO - Memory usage at start_fit: CPU 1925.5 MB, GPU 104.0 MB
2024-12-28 15:27:23,270 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:27:23,279 - INFO - Number of unique classes: 43
2024-12-28 15:27:23,435 - INFO - Fitted scaler and transformed data
2024-12-28 15:27:23,435 - INFO - Scaling time: 0.15s
2024-12-28 15:27:24,455 - INFO - Epoch 1/500, Train Loss: 6.7179, Val Loss: 3.2318
2024-12-28 15:27:25,524 - INFO - Epoch 2/500, Train Loss: 2.3981, Val Loss: 2.2506
2024-12-28 15:27:26,555 - INFO - Epoch 3/500, Train Loss: 1.7416, Val Loss: 1.8267
2024-12-28 15:27:27,521 - INFO - Epoch 4/500, Train Loss: 1.4076, Val Loss: 1.5436
2024-12-28 15:27:28,527 - INFO - Epoch 5/500, Train Loss: 1.1993, Val Loss: 1.3669
2024-12-28 15:27:29,624 - INFO - Epoch 6/500, Train Loss: 1.0592, Val Loss: 1.2596
2024-12-28 15:27:30,644 - INFO - Epoch 7/500, Train Loss: 0.9556, Val Loss: 1.1745
2024-12-28 15:27:31,699 - INFO - Epoch 8/500, Train Loss: 0.8739, Val Loss: 1.1280
2024-12-28 15:27:32,814 - INFO - Epoch 9/500, Train Loss: 0.8103, Val Loss: 1.0542
2024-12-28 15:27:33,879 - INFO - Epoch 10/500, Train Loss: 0.7595, Val Loss: 1.0165
2024-12-28 15:27:34,912 - INFO - Epoch 11/500, Train Loss: 0.7172, Val Loss: 0.9492
2024-12-28 15:27:36,127 - INFO - Epoch 12/500, Train Loss: 0.6800, Val Loss: 0.9280
2024-12-28 15:27:37,292 - INFO - Epoch 13/500, Train Loss: 0.6471, Val Loss: 0.9063
2024-12-28 15:27:38,518 - INFO - Epoch 14/500, Train Loss: 0.6294, Val Loss: 0.8805
2024-12-28 15:27:39,725 - INFO - Epoch 15/500, Train Loss: 0.6030, Val Loss: 0.8616
2024-12-28 15:27:40,944 - INFO - Epoch 16/500, Train Loss: 0.5865, Val Loss: 0.8792
2024-12-28 15:27:42,241 - INFO - Epoch 17/500, Train Loss: 0.5674, Val Loss: 0.8582
2024-12-28 15:27:43,352 - INFO - Epoch 18/500, Train Loss: 0.5537, Val Loss: 0.8354
2024-12-28 15:27:44,418 - INFO - Epoch 19/500, Train Loss: 0.5368, Val Loss: 0.8381
2024-12-28 15:27:45,475 - INFO - Epoch 20/500, Train Loss: 0.5306, Val Loss: 0.8069
2024-12-28 15:27:46,536 - INFO - Epoch 21/500, Train Loss: 0.5206, Val Loss: 0.7935
2024-12-28 15:27:47,595 - INFO - Epoch 22/500, Train Loss: 0.5039, Val Loss: 0.7815
2024-12-28 15:27:48,632 - INFO - Epoch 23/500, Train Loss: 0.5041, Val Loss: 0.8199
2024-12-28 15:27:49,673 - INFO - Epoch 24/500, Train Loss: 0.4953, Val Loss: 0.7900
2024-12-28 15:27:50,789 - INFO - Epoch 25/500, Train Loss: 0.4898, Val Loss: 0.7894
2024-12-28 15:27:51,982 - INFO - Epoch 26/500, Train Loss: 0.4861, Val Loss: 0.7782
2024-12-28 15:27:53,156 - INFO - Epoch 27/500, Train Loss: 0.4811, Val Loss: 0.7687
2024-12-28 15:27:54,355 - INFO - Epoch 28/500, Train Loss: 0.4753, Val Loss: 0.7819
2024-12-28 15:27:55,574 - INFO - Epoch 29/500, Train Loss: 0.4697, Val Loss: 0.7740
2024-12-28 15:27:56,841 - INFO - Epoch 30/500, Train Loss: 0.4692, Val Loss: 0.7837
2024-12-28 15:27:58,020 - INFO - Epoch 31/500, Train Loss: 0.4646, Val Loss: 0.7644
2024-12-28 15:27:59,207 - INFO - Epoch 32/500, Train Loss: 0.4547, Val Loss: 0.7734
2024-12-28 15:28:00,384 - INFO - Epoch 33/500, Train Loss: 0.4563, Val Loss: 0.7480
2024-12-28 15:28:01,585 - INFO - Epoch 34/500, Train Loss: 0.4518, Val Loss: 0.7746
2024-12-28 15:28:02,691 - INFO - Epoch 35/500, Train Loss: 0.4575, Val Loss: 0.7519
2024-12-28 15:28:03,834 - INFO - Epoch 36/500, Train Loss: 0.4474, Val Loss: 0.7799
2024-12-28 15:28:05,039 - INFO - Epoch 37/500, Train Loss: 0.4480, Val Loss: 0.7787
2024-12-28 15:28:06,202 - INFO - Epoch 38/500, Train Loss: 0.4501, Val Loss: 0.7530
2024-12-28 15:28:06,203 - INFO - Early stopping triggered at epoch 38
2024-12-28 15:28:06,203 - INFO - Training completed in 42.93s
2024-12-28 15:28:06,203 - INFO - Final memory usage: CPU 1906.0 MB, GPU 104.5 MB
2024-12-28 15:28:06,203 - INFO - Model training completed in 42.94s
2024-12-28 15:28:06,281 - INFO - Prediction completed in 0.08s
2024-12-28 15:28:06,292 - INFO - Poison rate 0.0 completed in 49.53s
2024-12-28 15:28:06,292 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:28:06,297 - INFO - Total number of labels flipped: 197
2024-12-28 15:28:06,297 - INFO - Label flipping completed in 0.01s
2024-12-28 15:28:06,297 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:28:06,297 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:28:07,558 - INFO - Feature scaling completed in 1.26s
2024-12-28 15:28:07,558 - INFO - Starting feature selection (k=50)
2024-12-28 15:28:07,594 - INFO - Feature selection completed in 0.04s. Output shape: (19755, 50)
2024-12-28 15:28:07,595 - INFO - Starting anomaly detection
2024-12-28 15:28:15,133 - INFO - Anomaly detection completed in 7.54s
2024-12-28 15:28:15,134 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:28:15,134 - INFO - Total fit_transform time: 8.84s
2024-12-28 15:28:15,134 - INFO - Training set processing completed in 8.84s
2024-12-28 15:28:15,134 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:28:15,135 - INFO - Memory usage at start_fit: CPU 1886.6 MB, GPU 104.3 MB
2024-12-28 15:28:15,135 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:28:15,144 - INFO - Number of unique classes: 43
2024-12-28 15:28:15,300 - INFO - Fitted scaler and transformed data
2024-12-28 15:28:15,301 - INFO - Scaling time: 0.15s
2024-12-28 15:28:16,333 - INFO - Epoch 1/500, Train Loss: 7.1424, Val Loss: 3.7778
2024-12-28 15:28:17,309 - INFO - Epoch 2/500, Train Loss: 2.8847, Val Loss: 2.8849
2024-12-28 15:28:18,213 - INFO - Epoch 3/500, Train Loss: 2.1446, Val Loss: 2.4652
2024-12-28 15:28:19,199 - INFO - Epoch 4/500, Train Loss: 1.7376, Val Loss: 2.2368
2024-12-28 15:28:20,190 - INFO - Epoch 5/500, Train Loss: 1.4868, Val Loss: 2.0638
2024-12-28 15:28:21,261 - INFO - Epoch 6/500, Train Loss: 1.3099, Val Loss: 2.0218
2024-12-28 15:28:22,352 - INFO - Epoch 7/500, Train Loss: 1.1806, Val Loss: 1.9101
2024-12-28 15:28:23,421 - INFO - Epoch 8/500, Train Loss: 1.0776, Val Loss: 1.9031
2024-12-28 15:28:24,454 - INFO - Epoch 9/500, Train Loss: 1.0025, Val Loss: 1.8502
2024-12-28 15:28:25,431 - INFO - Epoch 10/500, Train Loss: 0.9416, Val Loss: 1.7916
2024-12-28 15:28:26,420 - INFO - Epoch 11/500, Train Loss: 0.8863, Val Loss: 1.7755
2024-12-28 15:28:27,411 - INFO - Epoch 12/500, Train Loss: 0.8443, Val Loss: 1.7323
2024-12-28 15:28:28,399 - INFO - Epoch 13/500, Train Loss: 0.8018, Val Loss: 1.7397
2024-12-28 15:28:29,469 - INFO - Epoch 14/500, Train Loss: 0.7755, Val Loss: 1.7097
2024-12-28 15:28:30,551 - INFO - Epoch 15/500, Train Loss: 0.7471, Val Loss: 1.6967
2024-12-28 15:28:31,535 - INFO - Epoch 16/500, Train Loss: 0.7201, Val Loss: 1.6500
2024-12-28 15:28:32,518 - INFO - Epoch 17/500, Train Loss: 0.6930, Val Loss: 1.6069
2024-12-28 15:28:33,497 - INFO - Epoch 18/500, Train Loss: 0.6824, Val Loss: 1.6218
2024-12-28 15:28:34,606 - INFO - Epoch 19/500, Train Loss: 0.6657, Val Loss: 1.6173
2024-12-28 15:28:35,764 - INFO - Epoch 20/500, Train Loss: 0.6512, Val Loss: 1.6007
2024-12-28 15:28:36,758 - INFO - Epoch 21/500, Train Loss: 0.6421, Val Loss: 1.5928
2024-12-28 15:28:37,739 - INFO - Epoch 22/500, Train Loss: 0.6321, Val Loss: 1.5973
2024-12-28 15:28:38,748 - INFO - Epoch 23/500, Train Loss: 0.6162, Val Loss: 1.5911
2024-12-28 15:28:39,835 - INFO - Epoch 24/500, Train Loss: 0.6106, Val Loss: 1.5627
2024-12-28 15:28:41,058 - INFO - Epoch 25/500, Train Loss: 0.6032, Val Loss: 1.5572
2024-12-28 15:28:42,332 - INFO - Epoch 26/500, Train Loss: 0.5941, Val Loss: 1.6002
2024-12-28 15:28:43,525 - INFO - Epoch 27/500, Train Loss: 0.5875, Val Loss: 1.5142
2024-12-28 15:28:44,773 - INFO - Epoch 28/500, Train Loss: 0.5821, Val Loss: 1.5363
2024-12-28 15:28:45,982 - INFO - Epoch 29/500, Train Loss: 0.5755, Val Loss: 1.5626
2024-12-28 15:28:47,204 - INFO - Epoch 30/500, Train Loss: 0.5725, Val Loss: 1.5915
2024-12-28 15:28:48,357 - INFO - Epoch 31/500, Train Loss: 0.5715, Val Loss: 1.5968
2024-12-28 15:28:49,498 - INFO - Epoch 32/500, Train Loss: 0.5612, Val Loss: 1.5511
2024-12-28 15:28:49,499 - INFO - Early stopping triggered at epoch 32
2024-12-28 15:28:49,499 - INFO - Training completed in 34.36s
2024-12-28 15:28:49,499 - INFO - Final memory usage: CPU 1925.1 MB, GPU 104.5 MB
2024-12-28 15:28:49,499 - INFO - Model training completed in 34.37s
2024-12-28 15:28:49,579 - INFO - Prediction completed in 0.08s
2024-12-28 15:28:49,590 - INFO - Poison rate 0.01 completed in 43.30s
2024-12-28 15:28:49,590 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:28:49,602 - INFO - Total number of labels flipped: 592
2024-12-28 15:28:49,603 - INFO - Label flipping completed in 0.01s
2024-12-28 15:28:49,603 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:28:49,603 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:28:50,944 - INFO - Feature scaling completed in 1.34s
2024-12-28 15:28:50,945 - INFO - Starting feature selection (k=50)
2024-12-28 15:28:50,974 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:28:50,974 - INFO - Starting anomaly detection
2024-12-28 15:28:57,938 - INFO - Anomaly detection completed in 6.96s
2024-12-28 15:28:57,938 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:28:57,939 - INFO - Total fit_transform time: 8.34s
2024-12-28 15:28:57,939 - INFO - Training set processing completed in 8.34s
2024-12-28 15:28:57,939 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:28:57,940 - INFO - Memory usage at start_fit: CPU 1886.6 MB, GPU 104.3 MB
2024-12-28 15:28:57,940 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:28:57,951 - INFO - Number of unique classes: 43
2024-12-28 15:28:58,092 - INFO - Fitted scaler and transformed data
2024-12-28 15:28:58,092 - INFO - Scaling time: 0.14s
2024-12-28 15:28:59,066 - INFO - Epoch 1/500, Train Loss: 8.3129, Val Loss: 4.9826
2024-12-28 15:29:00,104 - INFO - Epoch 2/500, Train Loss: 3.8075, Val Loss: 4.0007
2024-12-28 15:29:01,107 - INFO - Epoch 3/500, Train Loss: 2.9253, Val Loss: 3.5779
2024-12-28 15:29:02,051 - INFO - Epoch 4/500, Train Loss: 2.4052, Val Loss: 3.4208
2024-12-28 15:29:03,083 - INFO - Epoch 5/500, Train Loss: 2.0744, Val Loss: 3.2037
2024-12-28 15:29:04,078 - INFO - Epoch 6/500, Train Loss: 1.8490, Val Loss: 3.0980
2024-12-28 15:29:05,072 - INFO - Epoch 7/500, Train Loss: 1.6730, Val Loss: 3.0387
2024-12-28 15:29:06,258 - INFO - Epoch 8/500, Train Loss: 1.5408, Val Loss: 2.9789
2024-12-28 15:29:07,469 - INFO - Epoch 9/500, Train Loss: 1.4323, Val Loss: 2.9059
2024-12-28 15:29:08,609 - INFO - Epoch 10/500, Train Loss: 1.3312, Val Loss: 2.8336
2024-12-28 15:29:09,671 - INFO - Epoch 11/500, Train Loss: 1.2567, Val Loss: 2.7882
2024-12-28 15:29:10,709 - INFO - Epoch 12/500, Train Loss: 1.2032, Val Loss: 2.7906
2024-12-28 15:29:11,706 - INFO - Epoch 13/500, Train Loss: 1.1597, Val Loss: 2.7968
2024-12-28 15:29:12,733 - INFO - Epoch 14/500, Train Loss: 1.1014, Val Loss: 2.7304
2024-12-28 15:29:13,888 - INFO - Epoch 15/500, Train Loss: 1.0724, Val Loss: 2.7101
2024-12-28 15:29:15,058 - INFO - Epoch 16/500, Train Loss: 1.0284, Val Loss: 2.6945
2024-12-28 15:29:16,212 - INFO - Epoch 17/500, Train Loss: 0.9996, Val Loss: 2.6922
2024-12-28 15:29:17,315 - INFO - Epoch 18/500, Train Loss: 0.9792, Val Loss: 2.6719
2024-12-28 15:29:18,497 - INFO - Epoch 19/500, Train Loss: 0.9479, Val Loss: 2.6383
2024-12-28 15:29:19,656 - INFO - Epoch 20/500, Train Loss: 0.9328, Val Loss: 2.6752
2024-12-28 15:29:20,688 - INFO - Epoch 21/500, Train Loss: 0.9126, Val Loss: 2.6071
2024-12-28 15:29:21,776 - INFO - Epoch 22/500, Train Loss: 0.8936, Val Loss: 2.6346
2024-12-28 15:29:22,955 - INFO - Epoch 23/500, Train Loss: 0.8864, Val Loss: 2.6005
2024-12-28 15:29:24,001 - INFO - Epoch 24/500, Train Loss: 0.8695, Val Loss: 2.6739
2024-12-28 15:29:25,036 - INFO - Epoch 25/500, Train Loss: 0.8567, Val Loss: 2.6275
2024-12-28 15:29:26,189 - INFO - Epoch 26/500, Train Loss: 0.8534, Val Loss: 2.6194
2024-12-28 15:29:27,345 - INFO - Epoch 27/500, Train Loss: 0.8333, Val Loss: 2.5758
2024-12-28 15:29:28,499 - INFO - Epoch 28/500, Train Loss: 0.8289, Val Loss: 2.5883
2024-12-28 15:29:29,641 - INFO - Epoch 29/500, Train Loss: 0.8194, Val Loss: 2.5920
2024-12-28 15:29:30,711 - INFO - Epoch 30/500, Train Loss: 0.8099, Val Loss: 2.6795
2024-12-28 15:29:31,721 - INFO - Epoch 31/500, Train Loss: 0.8098, Val Loss: 2.5786
2024-12-28 15:29:32,821 - INFO - Epoch 32/500, Train Loss: 0.7936, Val Loss: 2.5974
2024-12-28 15:29:32,821 - INFO - Early stopping triggered at epoch 32
2024-12-28 15:29:32,821 - INFO - Training completed in 34.88s
2024-12-28 15:29:32,821 - INFO - Final memory usage: CPU 1925.1 MB, GPU 104.5 MB
2024-12-28 15:29:32,822 - INFO - Model training completed in 34.88s
2024-12-28 15:29:32,883 - INFO - Prediction completed in 0.06s
2024-12-28 15:29:32,894 - INFO - Poison rate 0.03 completed in 43.30s
2024-12-28 15:29:32,895 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:29:32,917 - INFO - Total number of labels flipped: 987
2024-12-28 15:29:32,917 - INFO - Label flipping completed in 0.02s
2024-12-28 15:29:32,917 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:29:32,918 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:29:34,154 - INFO - Feature scaling completed in 1.24s
2024-12-28 15:29:34,154 - INFO - Starting feature selection (k=50)
2024-12-28 15:29:34,182 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:29:34,182 - INFO - Starting anomaly detection
2024-12-28 15:29:42,149 - INFO - Anomaly detection completed in 7.97s
2024-12-28 15:29:42,149 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:29:42,149 - INFO - Total fit_transform time: 9.23s
2024-12-28 15:29:42,149 - INFO - Training set processing completed in 9.23s
2024-12-28 15:29:42,149 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:29:42,150 - INFO - Memory usage at start_fit: CPU 1886.6 MB, GPU 104.3 MB
2024-12-28 15:29:42,151 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:29:42,162 - INFO - Number of unique classes: 43
2024-12-28 15:29:42,323 - INFO - Fitted scaler and transformed data
2024-12-28 15:29:42,324 - INFO - Scaling time: 0.16s
2024-12-28 15:29:43,289 - INFO - Epoch 1/500, Train Loss: 9.3291, Val Loss: 5.5087
2024-12-28 15:29:44,374 - INFO - Epoch 2/500, Train Loss: 4.7101, Val Loss: 4.6256
2024-12-28 15:29:45,419 - INFO - Epoch 3/500, Train Loss: 3.7230, Val Loss: 4.3084
2024-12-28 15:29:46,520 - INFO - Epoch 4/500, Train Loss: 3.1296, Val Loss: 4.0793
2024-12-28 15:29:47,604 - INFO - Epoch 5/500, Train Loss: 2.7299, Val Loss: 3.9428
2024-12-28 15:29:48,711 - INFO - Epoch 6/500, Train Loss: 2.4443, Val Loss: 3.8112
2024-12-28 15:29:49,853 - INFO - Epoch 7/500, Train Loss: 2.2237, Val Loss: 3.7176
2024-12-28 15:29:50,986 - INFO - Epoch 8/500, Train Loss: 2.0580, Val Loss: 3.6913
2024-12-28 15:29:52,090 - INFO - Epoch 9/500, Train Loss: 1.9187, Val Loss: 3.6622
2024-12-28 15:29:53,209 - INFO - Epoch 10/500, Train Loss: 1.8178, Val Loss: 3.6740
2024-12-28 15:29:54,333 - INFO - Epoch 11/500, Train Loss: 1.7146, Val Loss: 3.6304
2024-12-28 15:29:55,438 - INFO - Epoch 12/500, Train Loss: 1.6276, Val Loss: 3.5889
2024-12-28 15:29:56,559 - INFO - Epoch 13/500, Train Loss: 1.5571, Val Loss: 3.5592
2024-12-28 15:29:57,677 - INFO - Epoch 14/500, Train Loss: 1.4942, Val Loss: 3.5439
2024-12-28 15:29:58,681 - INFO - Epoch 15/500, Train Loss: 1.4503, Val Loss: 3.5542
2024-12-28 15:29:59,675 - INFO - Epoch 16/500, Train Loss: 1.3979, Val Loss: 3.4718
2024-12-28 15:30:00,655 - INFO - Epoch 17/500, Train Loss: 1.3573, Val Loss: 3.4415
2024-12-28 15:30:01,713 - INFO - Epoch 18/500, Train Loss: 1.3153, Val Loss: 3.5014
2024-12-28 15:30:02,816 - INFO - Epoch 19/500, Train Loss: 1.2973, Val Loss: 3.5506
2024-12-28 15:30:03,895 - INFO - Epoch 20/500, Train Loss: 1.2596, Val Loss: 3.3942
2024-12-28 15:30:04,918 - INFO - Epoch 21/500, Train Loss: 1.2459, Val Loss: 3.5074
2024-12-28 15:30:05,937 - INFO - Epoch 22/500, Train Loss: 1.2136, Val Loss: 3.5034
2024-12-28 15:30:06,990 - INFO - Epoch 23/500, Train Loss: 1.1874, Val Loss: 3.4852
2024-12-28 15:30:08,119 - INFO - Epoch 24/500, Train Loss: 1.1682, Val Loss: 3.4503
2024-12-28 15:30:09,350 - INFO - Epoch 25/500, Train Loss: 1.1682, Val Loss: 3.4966
2024-12-28 15:30:09,350 - INFO - Early stopping triggered at epoch 25
2024-12-28 15:30:09,350 - INFO - Training completed in 27.20s
2024-12-28 15:30:09,350 - INFO - Final memory usage: CPU 1925.1 MB, GPU 104.5 MB
2024-12-28 15:30:09,351 - INFO - Model training completed in 27.20s
2024-12-28 15:30:09,413 - INFO - Prediction completed in 0.06s
2024-12-28 15:30:09,424 - INFO - Poison rate 0.05 completed in 36.53s
2024-12-28 15:30:09,424 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:30:09,454 - INFO - Total number of labels flipped: 1382
2024-12-28 15:30:09,455 - INFO - Label flipping completed in 0.03s
2024-12-28 15:30:09,455 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:30:09,455 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:30:10,728 - INFO - Feature scaling completed in 1.27s
2024-12-28 15:30:10,728 - INFO - Starting feature selection (k=50)
2024-12-28 15:30:10,756 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:30:10,756 - INFO - Starting anomaly detection
2024-12-28 15:30:18,679 - INFO - Anomaly detection completed in 7.92s
2024-12-28 15:30:18,680 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:30:18,680 - INFO - Total fit_transform time: 9.23s
2024-12-28 15:30:18,680 - INFO - Training set processing completed in 9.23s
2024-12-28 15:30:18,680 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:30:18,681 - INFO - Memory usage at start_fit: CPU 1886.6 MB, GPU 104.3 MB
2024-12-28 15:30:18,681 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:30:18,693 - INFO - Number of unique classes: 43
2024-12-28 15:30:18,837 - INFO - Fitted scaler and transformed data
2024-12-28 15:30:18,837 - INFO - Scaling time: 0.14s
2024-12-28 15:30:19,957 - INFO - Epoch 1/500, Train Loss: 10.4560, Val Loss: 6.8500
2024-12-28 15:30:20,999 - INFO - Epoch 2/500, Train Loss: 5.6721, Val Loss: 5.9258
2024-12-28 15:30:22,159 - INFO - Epoch 3/500, Train Loss: 4.5575, Val Loss: 5.4235
2024-12-28 15:30:23,394 - INFO - Epoch 4/500, Train Loss: 3.9007, Val Loss: 5.3259
2024-12-28 15:30:24,626 - INFO - Epoch 5/500, Train Loss: 3.4404, Val Loss: 5.1966
2024-12-28 15:30:25,764 - INFO - Epoch 6/500, Train Loss: 3.1000, Val Loss: 4.9420
2024-12-28 15:30:26,872 - INFO - Epoch 7/500, Train Loss: 2.8265, Val Loss: 4.9986
2024-12-28 15:30:28,042 - INFO - Epoch 8/500, Train Loss: 2.6247, Val Loss: 4.9186
2024-12-28 15:30:29,120 - INFO - Epoch 9/500, Train Loss: 2.4787, Val Loss: 4.8413
2024-12-28 15:30:30,291 - INFO - Epoch 10/500, Train Loss: 2.3340, Val Loss: 4.8848
2024-12-28 15:30:31,463 - INFO - Epoch 11/500, Train Loss: 2.2257, Val Loss: 4.8294
2024-12-28 15:30:32,614 - INFO - Epoch 12/500, Train Loss: 2.1137, Val Loss: 4.7888
2024-12-28 15:30:33,820 - INFO - Epoch 13/500, Train Loss: 2.0316, Val Loss: 4.8313
2024-12-28 15:30:34,944 - INFO - Epoch 14/500, Train Loss: 1.9585, Val Loss: 4.8196
2024-12-28 15:30:36,151 - INFO - Epoch 15/500, Train Loss: 1.8913, Val Loss: 4.8967
2024-12-28 15:30:37,363 - INFO - Epoch 16/500, Train Loss: 1.8405, Val Loss: 4.7368
2024-12-28 15:30:38,526 - INFO - Epoch 17/500, Train Loss: 1.7844, Val Loss: 4.7821
2024-12-28 15:30:39,687 - INFO - Epoch 18/500, Train Loss: 1.7382, Val Loss: 4.7169
2024-12-28 15:30:40,905 - INFO - Epoch 19/500, Train Loss: 1.6958, Val Loss: 4.7637
2024-12-28 15:30:42,061 - INFO - Epoch 20/500, Train Loss: 1.6569, Val Loss: 4.7523
2024-12-28 15:30:43,181 - INFO - Epoch 21/500, Train Loss: 1.6368, Val Loss: 4.7474
2024-12-28 15:30:44,361 - INFO - Epoch 22/500, Train Loss: 1.5974, Val Loss: 4.7152
2024-12-28 15:30:45,538 - INFO - Epoch 23/500, Train Loss: 1.5698, Val Loss: 4.6967
2024-12-28 15:30:46,618 - INFO - Epoch 24/500, Train Loss: 1.5394, Val Loss: 4.7493
2024-12-28 15:30:47,735 - INFO - Epoch 25/500, Train Loss: 1.5200, Val Loss: 4.6978
2024-12-28 15:30:48,805 - INFO - Epoch 26/500, Train Loss: 1.4887, Val Loss: 4.7083
2024-12-28 15:30:49,928 - INFO - Epoch 27/500, Train Loss: 1.4745, Val Loss: 4.7311
2024-12-28 15:30:50,970 - INFO - Epoch 28/500, Train Loss: 1.4508, Val Loss: 4.8836
2024-12-28 15:30:50,971 - INFO - Early stopping triggered at epoch 28
2024-12-28 15:30:50,971 - INFO - Training completed in 32.29s
2024-12-28 15:30:50,971 - INFO - Final memory usage: CPU 1925.1 MB, GPU 104.5 MB
2024-12-28 15:30:50,972 - INFO - Model training completed in 32.29s
2024-12-28 15:30:51,053 - INFO - Prediction completed in 0.08s
2024-12-28 15:30:51,064 - INFO - Poison rate 0.07 completed in 41.64s
2024-12-28 15:30:51,064 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:30:51,099 - INFO - Total number of labels flipped: 1975
2024-12-28 15:30:51,099 - INFO - Label flipping completed in 0.03s
2024-12-28 15:30:51,099 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:30:51,099 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:30:52,408 - INFO - Feature scaling completed in 1.31s
2024-12-28 15:30:52,408 - INFO - Starting feature selection (k=50)
2024-12-28 15:30:52,438 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:30:52,438 - INFO - Starting anomaly detection
2024-12-28 15:30:59,026 - INFO - Anomaly detection completed in 6.59s
2024-12-28 15:30:59,026 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:30:59,026 - INFO - Total fit_transform time: 7.93s
2024-12-28 15:30:59,026 - INFO - Training set processing completed in 7.93s
2024-12-28 15:30:59,026 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:30:59,027 - INFO - Memory usage at start_fit: CPU 1886.6 MB, GPU 104.3 MB
2024-12-28 15:30:59,027 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:30:59,038 - INFO - Number of unique classes: 43
2024-12-28 15:30:59,182 - INFO - Fitted scaler and transformed data
2024-12-28 15:30:59,182 - INFO - Scaling time: 0.14s
2024-12-28 15:31:00,195 - INFO - Epoch 1/500, Train Loss: 11.4988, Val Loss: 9.2831
2024-12-28 15:31:01,313 - INFO - Epoch 2/500, Train Loss: 6.8164, Val Loss: 8.3591
2024-12-28 15:31:02,502 - INFO - Epoch 3/500, Train Loss: 5.5731, Val Loss: 7.9498
2024-12-28 15:31:03,692 - INFO - Epoch 4/500, Train Loss: 4.8442, Val Loss: 7.7689
2024-12-28 15:31:04,876 - INFO - Epoch 5/500, Train Loss: 4.3085, Val Loss: 7.7290
2024-12-28 15:31:06,134 - INFO - Epoch 6/500, Train Loss: 3.9678, Val Loss: 7.4721
2024-12-28 15:31:07,366 - INFO - Epoch 7/500, Train Loss: 3.6678, Val Loss: 7.5434
2024-12-28 15:31:08,596 - INFO - Epoch 8/500, Train Loss: 3.4180, Val Loss: 7.6134
2024-12-28 15:31:09,713 - INFO - Epoch 9/500, Train Loss: 3.2149, Val Loss: 7.6345
2024-12-28 15:31:10,845 - INFO - Epoch 10/500, Train Loss: 3.0647, Val Loss: 7.5873
2024-12-28 15:31:11,899 - INFO - Epoch 11/500, Train Loss: 2.9211, Val Loss: 7.6042
2024-12-28 15:31:11,899 - INFO - Early stopping triggered at epoch 11
2024-12-28 15:31:11,899 - INFO - Training completed in 12.87s
2024-12-28 15:31:11,899 - INFO - Final memory usage: CPU 1925.1 MB, GPU 104.5 MB
2024-12-28 15:31:11,901 - INFO - Model training completed in 12.87s
2024-12-28 15:31:11,963 - INFO - Prediction completed in 0.06s
2024-12-28 15:31:11,974 - INFO - Poison rate 0.1 completed in 20.91s
2024-12-28 15:31:11,974 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:31:12,044 - INFO - Total number of labels flipped: 3951
2024-12-28 15:31:12,044 - INFO - Label flipping completed in 0.07s
2024-12-28 15:31:12,044 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:31:12,044 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:31:13,355 - INFO - Feature scaling completed in 1.31s
2024-12-28 15:31:13,355 - INFO - Starting feature selection (k=50)
2024-12-28 15:31:13,387 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:31:13,388 - INFO - Starting anomaly detection
2024-12-28 15:31:20,973 - INFO - Anomaly detection completed in 7.58s
2024-12-28 15:31:20,973 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:31:20,973 - INFO - Total fit_transform time: 8.93s
2024-12-28 15:31:20,973 - INFO - Training set processing completed in 8.93s
2024-12-28 15:31:20,973 - INFO - Fitting SVMWrapper model with data shape: (19755, 512)
2024-12-28 15:31:20,974 - INFO - Memory usage at start_fit: CPU 1886.6 MB, GPU 104.3 MB
2024-12-28 15:31:20,974 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:31:20,986 - INFO - Number of unique classes: 43
2024-12-28 15:31:21,123 - INFO - Fitted scaler and transformed data
2024-12-28 15:31:21,124 - INFO - Scaling time: 0.14s
2024-12-28 15:31:22,177 - INFO - Epoch 1/500, Train Loss: 15.9887, Val Loss: 13.8875
2024-12-28 15:31:23,260 - INFO - Epoch 2/500, Train Loss: 10.9082, Val Loss: 12.9631
2024-12-28 15:31:24,343 - INFO - Epoch 3/500, Train Loss: 9.4229, Val Loss: 12.6064
2024-12-28 15:31:25,480 - INFO - Epoch 4/500, Train Loss: 8.5054, Val Loss: 12.4956
2024-12-28 15:31:26,555 - INFO - Epoch 5/500, Train Loss: 7.8494, Val Loss: 12.3705
2024-12-28 15:31:27,706 - INFO - Epoch 6/500, Train Loss: 7.3341, Val Loss: 12.4788
2024-12-28 15:31:28,876 - INFO - Epoch 7/500, Train Loss: 6.9085, Val Loss: 12.3816
2024-12-28 15:31:30,108 - INFO - Epoch 8/500, Train Loss: 6.6067, Val Loss: 12.4754
2024-12-28 15:31:31,355 - INFO - Epoch 9/500, Train Loss: 6.3109, Val Loss: 12.4867
2024-12-28 15:31:32,492 - INFO - Epoch 10/500, Train Loss: 6.0601, Val Loss: 12.4567
2024-12-28 15:31:32,492 - INFO - Early stopping triggered at epoch 10
2024-12-28 15:31:32,492 - INFO - Training completed in 11.52s
2024-12-28 15:31:32,493 - INFO - Final memory usage: CPU 1925.1 MB, GPU 104.5 MB
2024-12-28 15:31:32,493 - INFO - Model training completed in 11.52s
2024-12-28 15:31:32,556 - INFO - Prediction completed in 0.06s
2024-12-28 15:31:32,575 - INFO - Poison rate 0.2 completed in 20.60s
2024-12-28 15:31:32,581 - INFO - Loaded 175 existing results
2024-12-28 15:31:32,581 - INFO - Total results to save: 182
2024-12-28 15:31:32,582 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:31:32,594 - INFO - Saved 182 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:31:32,595 - INFO - Total evaluation time: 277.92s
2024-12-28 15:31:32,601 - INFO - 
Progress: 28.1% - Evaluating GTSRB with LogisticRegression (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:31:32,783 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:31:32,784 - INFO - Dataset type: image
2024-12-28 15:31:32,784 - INFO - Sample size: 39209
2024-12-28 15:31:32,784 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:31:32,785 - INFO - Loading datasets...
2024-12-28 15:31:52,355 - INFO - Dataset loading completed in 19.57s
2024-12-28 15:31:52,355 - INFO - Extracting validation features...
2024-12-28 15:31:52,355 - INFO - Extracting features from 4435 samples...
2024-12-28 15:31:53,073 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:31:53,077 - INFO - Validation feature extraction completed in 0.72s
2024-12-28 15:31:53,078 - INFO - Extracting training features...
2024-12-28 15:31:53,078 - INFO - Extracting features from 19755 samples...
2024-12-28 15:31:55,742 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:31:55,751 - INFO - Training feature extraction completed in 2.67s
2024-12-28 15:31:55,751 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 15:31:55,751 - INFO - Using device: cuda
2024-12-28 15:31:55,751 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:31:55,751 - INFO - Training set processing completed in 0.00s
2024-12-28 15:31:55,752 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:31:55,753 - INFO - Memory usage at start_fit: CPU 1925.5 MB, GPU 104.0 MB
2024-12-28 15:31:55,754 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:31:55,763 - INFO - Number of unique classes: 43
2024-12-28 15:31:55,912 - INFO - Fitted scaler and transformed data
2024-12-28 15:31:55,912 - INFO - Scaling time: 0.15s
2024-12-28 15:31:56,473 - INFO - Epoch 1/1000, Train Loss: 1.8965, Val Loss: 1.3789
2024-12-28 15:31:56,975 - INFO - Epoch 2/1000, Train Loss: 1.1938, Val Loss: 1.1274
2024-12-28 15:31:57,492 - INFO - Epoch 3/1000, Train Loss: 1.0099, Val Loss: 1.0188
2024-12-28 15:31:57,979 - INFO - Epoch 4/1000, Train Loss: 0.9150, Val Loss: 0.9468
2024-12-28 15:31:58,531 - INFO - Epoch 5/1000, Train Loss: 0.8578, Val Loss: 0.9081
2024-12-28 15:31:59,037 - INFO - Epoch 6/1000, Train Loss: 0.8176, Val Loss: 0.8851
2024-12-28 15:31:59,488 - INFO - Epoch 7/1000, Train Loss: 0.7929, Val Loss: 0.8629
2024-12-28 15:31:59,969 - INFO - Epoch 8/1000, Train Loss: 0.7740, Val Loss: 0.8441
2024-12-28 15:32:00,425 - INFO - Epoch 9/1000, Train Loss: 0.7614, Val Loss: 0.8398
2024-12-28 15:32:00,889 - INFO - Epoch 10/1000, Train Loss: 0.7502, Val Loss: 0.8336
2024-12-28 15:32:01,346 - INFO - Epoch 11/1000, Train Loss: 0.7442, Val Loss: 0.8223
2024-12-28 15:32:01,823 - INFO - Epoch 12/1000, Train Loss: 0.7359, Val Loss: 0.8225
2024-12-28 15:32:02,271 - INFO - Epoch 13/1000, Train Loss: 0.7314, Val Loss: 0.8199
2024-12-28 15:32:02,728 - INFO - Epoch 14/1000, Train Loss: 0.7268, Val Loss: 0.8128
2024-12-28 15:32:03,164 - INFO - Epoch 15/1000, Train Loss: 0.7260, Val Loss: 0.8143
2024-12-28 15:32:03,625 - INFO - Epoch 16/1000, Train Loss: 0.7224, Val Loss: 0.8052
2024-12-28 15:32:04,125 - INFO - Epoch 17/1000, Train Loss: 0.7207, Val Loss: 0.8094
2024-12-28 15:32:04,586 - INFO - Epoch 18/1000, Train Loss: 0.7171, Val Loss: 0.8078
2024-12-28 15:32:05,067 - INFO - Epoch 19/1000, Train Loss: 0.7172, Val Loss: 0.8063
2024-12-28 15:32:05,528 - INFO - Epoch 20/1000, Train Loss: 0.7147, Val Loss: 0.8048
2024-12-28 15:32:06,006 - INFO - Epoch 21/1000, Train Loss: 0.7150, Val Loss: 0.7984
2024-12-28 15:32:06,469 - INFO - Epoch 22/1000, Train Loss: 0.7146, Val Loss: 0.8025
2024-12-28 15:32:06,955 - INFO - Epoch 23/1000, Train Loss: 0.7134, Val Loss: 0.8036
2024-12-28 15:32:07,449 - INFO - Epoch 24/1000, Train Loss: 0.7139, Val Loss: 0.8027
2024-12-28 15:32:07,912 - INFO - Epoch 25/1000, Train Loss: 0.7121, Val Loss: 0.8088
2024-12-28 15:32:08,338 - INFO - Epoch 26/1000, Train Loss: 0.7120, Val Loss: 0.7973
2024-12-28 15:32:08,771 - INFO - Epoch 27/1000, Train Loss: 0.7116, Val Loss: 0.8005
2024-12-28 15:32:09,191 - INFO - Epoch 28/1000, Train Loss: 0.7116, Val Loss: 0.8025
2024-12-28 15:32:09,660 - INFO - Epoch 29/1000, Train Loss: 0.7115, Val Loss: 0.7971
2024-12-28 15:32:10,181 - INFO - Epoch 30/1000, Train Loss: 0.7116, Val Loss: 0.8032
2024-12-28 15:32:10,631 - INFO - Epoch 31/1000, Train Loss: 0.7116, Val Loss: 0.7994
2024-12-28 15:32:10,631 - INFO - Early stopping triggered at epoch 31
2024-12-28 15:32:10,631 - INFO - Training completed in 14.88s
2024-12-28 15:32:10,632 - INFO - Final memory usage: CPU 1906.1 MB, GPU 104.5 MB
2024-12-28 15:32:10,632 - INFO - Model training completed in 14.88s
2024-12-28 15:32:10,707 - INFO - Prediction completed in 0.07s
2024-12-28 15:32:10,719 - INFO - Poison rate 0.0 completed in 14.97s
2024-12-28 15:32:10,719 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:32:10,724 - INFO - Total number of labels flipped: 197
2024-12-28 15:32:10,724 - INFO - Label flipping completed in 0.01s
2024-12-28 15:32:10,724 - INFO - Training set processing completed in 0.00s
2024-12-28 15:32:10,724 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:32:10,725 - INFO - Memory usage at start_fit: CPU 1867.5 MB, GPU 104.3 MB
2024-12-28 15:32:10,725 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:32:10,733 - INFO - Number of unique classes: 43
2024-12-28 15:32:10,874 - INFO - Fitted scaler and transformed data
2024-12-28 15:32:10,874 - INFO - Scaling time: 0.14s
2024-12-28 15:32:11,352 - INFO - Epoch 1/1000, Train Loss: 1.9422, Val Loss: 1.3932
2024-12-28 15:32:11,788 - INFO - Epoch 2/1000, Train Loss: 1.2665, Val Loss: 1.1620
2024-12-28 15:32:12,243 - INFO - Epoch 3/1000, Train Loss: 1.0861, Val Loss: 1.0457
2024-12-28 15:32:12,688 - INFO - Epoch 4/1000, Train Loss: 0.9892, Val Loss: 0.9827
2024-12-28 15:32:13,160 - INFO - Epoch 5/1000, Train Loss: 0.9329, Val Loss: 0.9473
2024-12-28 15:32:13,622 - INFO - Epoch 6/1000, Train Loss: 0.8951, Val Loss: 0.9211
2024-12-28 15:32:14,054 - INFO - Epoch 7/1000, Train Loss: 0.8694, Val Loss: 0.9041
2024-12-28 15:32:14,525 - INFO - Epoch 8/1000, Train Loss: 0.8508, Val Loss: 0.8875
2024-12-28 15:32:14,974 - INFO - Epoch 9/1000, Train Loss: 0.8378, Val Loss: 0.8717
2024-12-28 15:32:15,483 - INFO - Epoch 10/1000, Train Loss: 0.8258, Val Loss: 0.8633
2024-12-28 15:32:15,939 - INFO - Epoch 11/1000, Train Loss: 0.8183, Val Loss: 0.8656
2024-12-28 15:32:16,447 - INFO - Epoch 12/1000, Train Loss: 0.8130, Val Loss: 0.8660
2024-12-28 15:32:16,919 - INFO - Epoch 13/1000, Train Loss: 0.8088, Val Loss: 0.8642
2024-12-28 15:32:17,377 - INFO - Epoch 14/1000, Train Loss: 0.8047, Val Loss: 0.8567
2024-12-28 15:32:17,830 - INFO - Epoch 15/1000, Train Loss: 0.8017, Val Loss: 0.8547
2024-12-28 15:32:18,285 - INFO - Epoch 16/1000, Train Loss: 0.7981, Val Loss: 0.8526
2024-12-28 15:32:18,757 - INFO - Epoch 17/1000, Train Loss: 0.7958, Val Loss: 0.8421
2024-12-28 15:32:19,173 - INFO - Epoch 18/1000, Train Loss: 0.7948, Val Loss: 0.8498
2024-12-28 15:32:19,584 - INFO - Epoch 19/1000, Train Loss: 0.7946, Val Loss: 0.8371
2024-12-28 15:32:20,013 - INFO - Epoch 20/1000, Train Loss: 0.7908, Val Loss: 0.8419
2024-12-28 15:32:20,464 - INFO - Epoch 21/1000, Train Loss: 0.7923, Val Loss: 0.8369
2024-12-28 15:32:20,904 - INFO - Epoch 22/1000, Train Loss: 0.7894, Val Loss: 0.8501
2024-12-28 15:32:21,388 - INFO - Epoch 23/1000, Train Loss: 0.7887, Val Loss: 0.8463
2024-12-28 15:32:21,891 - INFO - Epoch 24/1000, Train Loss: 0.7868, Val Loss: 0.8399
2024-12-28 15:32:21,891 - INFO - Early stopping triggered at epoch 24
2024-12-28 15:32:21,891 - INFO - Training completed in 11.17s
2024-12-28 15:32:21,892 - INFO - Final memory usage: CPU 1915.6 MB, GPU 104.5 MB
2024-12-28 15:32:21,893 - INFO - Model training completed in 11.17s
2024-12-28 15:32:21,997 - INFO - Prediction completed in 0.10s
2024-12-28 15:32:22,013 - INFO - Poison rate 0.01 completed in 11.29s
2024-12-28 15:32:22,014 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:32:22,039 - INFO - Total number of labels flipped: 592
2024-12-28 15:32:22,039 - INFO - Label flipping completed in 0.03s
2024-12-28 15:32:22,039 - INFO - Training set processing completed in 0.00s
2024-12-28 15:32:22,039 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:32:22,040 - INFO - Memory usage at start_fit: CPU 1877.0 MB, GPU 104.3 MB
2024-12-28 15:32:22,040 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:32:22,053 - INFO - Number of unique classes: 43
2024-12-28 15:32:22,195 - INFO - Fitted scaler and transformed data
2024-12-28 15:32:22,195 - INFO - Scaling time: 0.14s
2024-12-28 15:32:22,680 - INFO - Epoch 1/1000, Train Loss: 2.0396, Val Loss: 1.5366
2024-12-28 15:32:23,187 - INFO - Epoch 2/1000, Train Loss: 1.3943, Val Loss: 1.2933
2024-12-28 15:32:23,679 - INFO - Epoch 3/1000, Train Loss: 1.2216, Val Loss: 1.2004
2024-12-28 15:32:24,157 - INFO - Epoch 4/1000, Train Loss: 1.1322, Val Loss: 1.1363
2024-12-28 15:32:24,585 - INFO - Epoch 5/1000, Train Loss: 1.0748, Val Loss: 1.0940
2024-12-28 15:32:25,026 - INFO - Epoch 6/1000, Train Loss: 1.0390, Val Loss: 1.0628
2024-12-28 15:32:25,455 - INFO - Epoch 7/1000, Train Loss: 1.0124, Val Loss: 1.0421
2024-12-28 15:32:25,903 - INFO - Epoch 8/1000, Train Loss: 0.9936, Val Loss: 1.0321
2024-12-28 15:32:26,362 - INFO - Epoch 9/1000, Train Loss: 0.9791, Val Loss: 1.0179
2024-12-28 15:32:26,796 - INFO - Epoch 10/1000, Train Loss: 0.9665, Val Loss: 1.0108
2024-12-28 15:32:27,241 - INFO - Epoch 11/1000, Train Loss: 0.9585, Val Loss: 1.0133
2024-12-28 15:32:27,691 - INFO - Epoch 12/1000, Train Loss: 0.9527, Val Loss: 0.9993
2024-12-28 15:32:28,151 - INFO - Epoch 13/1000, Train Loss: 0.9467, Val Loss: 0.9943
2024-12-28 15:32:28,612 - INFO - Epoch 14/1000, Train Loss: 0.9432, Val Loss: 0.9904
2024-12-28 15:32:29,029 - INFO - Epoch 15/1000, Train Loss: 0.9391, Val Loss: 0.9911
2024-12-28 15:32:29,462 - INFO - Epoch 16/1000, Train Loss: 0.9372, Val Loss: 0.9901
2024-12-28 15:32:29,896 - INFO - Epoch 17/1000, Train Loss: 0.9358, Val Loss: 0.9922
2024-12-28 15:32:30,355 - INFO - Epoch 18/1000, Train Loss: 0.9333, Val Loss: 0.9855
2024-12-28 15:32:30,792 - INFO - Epoch 19/1000, Train Loss: 0.9312, Val Loss: 0.9963
2024-12-28 15:32:31,276 - INFO - Epoch 20/1000, Train Loss: 0.9316, Val Loss: 0.9862
2024-12-28 15:32:31,758 - INFO - Epoch 21/1000, Train Loss: 0.9300, Val Loss: 0.9810
2024-12-28 15:32:32,253 - INFO - Epoch 22/1000, Train Loss: 0.9282, Val Loss: 0.9896
2024-12-28 15:32:32,699 - INFO - Epoch 23/1000, Train Loss: 0.9262, Val Loss: 0.9869
2024-12-28 15:32:33,183 - INFO - Epoch 24/1000, Train Loss: 0.9269, Val Loss: 0.9836
2024-12-28 15:32:33,662 - INFO - Epoch 25/1000, Train Loss: 0.9266, Val Loss: 0.9768
2024-12-28 15:32:34,114 - INFO - Epoch 26/1000, Train Loss: 0.9269, Val Loss: 0.9823
2024-12-28 15:32:34,583 - INFO - Epoch 27/1000, Train Loss: 0.9255, Val Loss: 0.9835
2024-12-28 15:32:35,046 - INFO - Epoch 28/1000, Train Loss: 0.9251, Val Loss: 0.9852
2024-12-28 15:32:35,480 - INFO - Epoch 29/1000, Train Loss: 0.9252, Val Loss: 0.9821
2024-12-28 15:32:35,936 - INFO - Epoch 30/1000, Train Loss: 0.9272, Val Loss: 0.9899
2024-12-28 15:32:35,937 - INFO - Early stopping triggered at epoch 30
2024-12-28 15:32:35,937 - INFO - Training completed in 13.90s
2024-12-28 15:32:35,938 - INFO - Final memory usage: CPU 1915.6 MB, GPU 104.5 MB
2024-12-28 15:32:35,939 - INFO - Model training completed in 13.90s
2024-12-28 15:32:36,019 - INFO - Prediction completed in 0.08s
2024-12-28 15:32:36,030 - INFO - Poison rate 0.03 completed in 14.02s
2024-12-28 15:32:36,030 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:32:36,048 - INFO - Total number of labels flipped: 987
2024-12-28 15:32:36,049 - INFO - Label flipping completed in 0.02s
2024-12-28 15:32:36,049 - INFO - Training set processing completed in 0.00s
2024-12-28 15:32:36,049 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:32:36,050 - INFO - Memory usage at start_fit: CPU 1877.0 MB, GPU 104.3 MB
2024-12-28 15:32:36,050 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:32:36,056 - INFO - Number of unique classes: 43
2024-12-28 15:32:36,192 - INFO - Fitted scaler and transformed data
2024-12-28 15:32:36,193 - INFO - Scaling time: 0.14s
2024-12-28 15:32:36,653 - INFO - Epoch 1/1000, Train Loss: 2.0942, Val Loss: 1.6752
2024-12-28 15:32:37,134 - INFO - Epoch 2/1000, Train Loss: 1.4913, Val Loss: 1.4644
2024-12-28 15:32:37,629 - INFO - Epoch 3/1000, Train Loss: 1.3263, Val Loss: 1.3832
2024-12-28 15:32:38,108 - INFO - Epoch 4/1000, Train Loss: 1.2412, Val Loss: 1.3191
2024-12-28 15:32:38,591 - INFO - Epoch 5/1000, Train Loss: 1.1850, Val Loss: 1.2864
2024-12-28 15:32:39,117 - INFO - Epoch 6/1000, Train Loss: 1.1506, Val Loss: 1.2560
2024-12-28 15:32:39,581 - INFO - Epoch 7/1000, Train Loss: 1.1237, Val Loss: 1.2432
2024-12-28 15:32:40,062 - INFO - Epoch 8/1000, Train Loss: 1.1035, Val Loss: 1.2287
2024-12-28 15:32:40,540 - INFO - Epoch 9/1000, Train Loss: 1.0884, Val Loss: 1.2208
2024-12-28 15:32:41,008 - INFO - Epoch 10/1000, Train Loss: 1.0789, Val Loss: 1.2177
2024-12-28 15:32:41,467 - INFO - Epoch 11/1000, Train Loss: 1.0702, Val Loss: 1.2081
2024-12-28 15:32:41,922 - INFO - Epoch 12/1000, Train Loss: 1.0652, Val Loss: 1.2079
2024-12-28 15:32:42,388 - INFO - Epoch 13/1000, Train Loss: 1.0573, Val Loss: 1.2046
2024-12-28 15:32:42,851 - INFO - Epoch 14/1000, Train Loss: 1.0536, Val Loss: 1.2032
2024-12-28 15:32:43,333 - INFO - Epoch 15/1000, Train Loss: 1.0510, Val Loss: 1.1974
2024-12-28 15:32:43,802 - INFO - Epoch 16/1000, Train Loss: 1.0471, Val Loss: 1.1951
2024-12-28 15:32:44,247 - INFO - Epoch 17/1000, Train Loss: 1.0467, Val Loss: 1.1910
2024-12-28 15:32:44,741 - INFO - Epoch 18/1000, Train Loss: 1.0441, Val Loss: 1.1940
2024-12-28 15:32:45,223 - INFO - Epoch 19/1000, Train Loss: 1.0423, Val Loss: 1.1896
2024-12-28 15:32:45,685 - INFO - Epoch 20/1000, Train Loss: 1.0408, Val Loss: 1.1909
2024-12-28 15:32:46,153 - INFO - Epoch 21/1000, Train Loss: 1.0401, Val Loss: 1.1935
2024-12-28 15:32:46,607 - INFO - Epoch 22/1000, Train Loss: 1.0383, Val Loss: 1.1973
2024-12-28 15:32:47,067 - INFO - Epoch 23/1000, Train Loss: 1.0374, Val Loss: 1.1872
2024-12-28 15:32:47,544 - INFO - Epoch 24/1000, Train Loss: 1.0374, Val Loss: 1.2004
2024-12-28 15:32:48,001 - INFO - Epoch 25/1000, Train Loss: 1.0369, Val Loss: 1.1862
2024-12-28 15:32:48,448 - INFO - Epoch 26/1000, Train Loss: 1.0378, Val Loss: 1.1814
2024-12-28 15:32:48,909 - INFO - Epoch 27/1000, Train Loss: 1.0371, Val Loss: 1.1843
2024-12-28 15:32:49,357 - INFO - Epoch 28/1000, Train Loss: 1.0365, Val Loss: 1.1831
2024-12-28 15:32:49,836 - INFO - Epoch 29/1000, Train Loss: 1.0363, Val Loss: 1.1959
2024-12-28 15:32:50,332 - INFO - Epoch 30/1000, Train Loss: 1.0351, Val Loss: 1.1863
2024-12-28 15:32:50,819 - INFO - Epoch 31/1000, Train Loss: 1.0329, Val Loss: 1.1977
2024-12-28 15:32:50,819 - INFO - Early stopping triggered at epoch 31
2024-12-28 15:32:50,819 - INFO - Training completed in 14.77s
2024-12-28 15:32:50,819 - INFO - Final memory usage: CPU 1915.6 MB, GPU 104.5 MB
2024-12-28 15:32:50,820 - INFO - Model training completed in 14.77s
2024-12-28 15:32:50,893 - INFO - Prediction completed in 0.07s
2024-12-28 15:32:50,907 - INFO - Poison rate 0.05 completed in 14.88s
2024-12-28 15:32:50,908 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:32:50,942 - INFO - Total number of labels flipped: 1382
2024-12-28 15:32:50,942 - INFO - Label flipping completed in 0.03s
2024-12-28 15:32:50,942 - INFO - Training set processing completed in 0.00s
2024-12-28 15:32:50,942 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:32:50,943 - INFO - Memory usage at start_fit: CPU 1877.0 MB, GPU 104.3 MB
2024-12-28 15:32:50,944 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:32:50,950 - INFO - Number of unique classes: 43
2024-12-28 15:32:51,086 - INFO - Fitted scaler and transformed data
2024-12-28 15:32:51,087 - INFO - Scaling time: 0.13s
2024-12-28 15:32:51,584 - INFO - Epoch 1/1000, Train Loss: 2.1851, Val Loss: 1.8183
2024-12-28 15:32:52,056 - INFO - Epoch 2/1000, Train Loss: 1.5936, Val Loss: 1.6191
2024-12-28 15:32:52,573 - INFO - Epoch 3/1000, Train Loss: 1.4303, Val Loss: 1.5322
2024-12-28 15:32:53,071 - INFO - Epoch 4/1000, Train Loss: 1.3436, Val Loss: 1.4845
2024-12-28 15:32:53,526 - INFO - Epoch 5/1000, Train Loss: 1.2895, Val Loss: 1.4502
2024-12-28 15:32:53,993 - INFO - Epoch 6/1000, Train Loss: 1.2513, Val Loss: 1.4206
2024-12-28 15:32:54,435 - INFO - Epoch 7/1000, Train Loss: 1.2265, Val Loss: 1.4087
2024-12-28 15:32:54,895 - INFO - Epoch 8/1000, Train Loss: 1.2043, Val Loss: 1.3919
2024-12-28 15:32:55,350 - INFO - Epoch 9/1000, Train Loss: 1.1924, Val Loss: 1.3851
2024-12-28 15:32:55,803 - INFO - Epoch 10/1000, Train Loss: 1.1810, Val Loss: 1.3813
2024-12-28 15:32:56,227 - INFO - Epoch 11/1000, Train Loss: 1.1708, Val Loss: 1.3736
2024-12-28 15:32:56,670 - INFO - Epoch 12/1000, Train Loss: 1.1655, Val Loss: 1.3669
2024-12-28 15:32:57,125 - INFO - Epoch 13/1000, Train Loss: 1.1601, Val Loss: 1.3716
2024-12-28 15:32:57,592 - INFO - Epoch 14/1000, Train Loss: 1.1563, Val Loss: 1.3678
2024-12-28 15:32:58,035 - INFO - Epoch 15/1000, Train Loss: 1.1530, Val Loss: 1.3588
2024-12-28 15:32:58,476 - INFO - Epoch 16/1000, Train Loss: 1.1499, Val Loss: 1.3598
2024-12-28 15:32:58,900 - INFO - Epoch 17/1000, Train Loss: 1.1461, Val Loss: 1.3614
2024-12-28 15:32:59,341 - INFO - Epoch 18/1000, Train Loss: 1.1463, Val Loss: 1.3675
2024-12-28 15:32:59,760 - INFO - Epoch 19/1000, Train Loss: 1.1460, Val Loss: 1.3566
2024-12-28 15:33:00,196 - INFO - Epoch 20/1000, Train Loss: 1.1412, Val Loss: 1.3566
2024-12-28 15:33:00,644 - INFO - Epoch 21/1000, Train Loss: 1.1400, Val Loss: 1.3579
2024-12-28 15:33:01,261 - INFO - Epoch 22/1000, Train Loss: 1.1431, Val Loss: 1.3463
2024-12-28 15:33:01,828 - INFO - Epoch 23/1000, Train Loss: 1.1408, Val Loss: 1.3572
2024-12-28 15:33:02,290 - INFO - Epoch 24/1000, Train Loss: 1.1383, Val Loss: 1.3546
2024-12-28 15:33:02,760 - INFO - Epoch 25/1000, Train Loss: 1.1379, Val Loss: 1.3514
2024-12-28 15:33:03,362 - INFO - Epoch 26/1000, Train Loss: 1.1375, Val Loss: 1.3496
2024-12-28 15:33:04,005 - INFO - Epoch 27/1000, Train Loss: 1.1381, Val Loss: 1.3490
2024-12-28 15:33:04,005 - INFO - Early stopping triggered at epoch 27
2024-12-28 15:33:04,005 - INFO - Training completed in 13.06s
2024-12-28 15:33:04,005 - INFO - Final memory usage: CPU 1915.6 MB, GPU 104.5 MB
2024-12-28 15:33:04,006 - INFO - Model training completed in 13.06s
2024-12-28 15:33:04,081 - INFO - Prediction completed in 0.07s
2024-12-28 15:33:04,092 - INFO - Poison rate 0.07 completed in 13.18s
2024-12-28 15:33:04,093 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:33:04,129 - INFO - Total number of labels flipped: 1975
2024-12-28 15:33:04,129 - INFO - Label flipping completed in 0.04s
2024-12-28 15:33:04,129 - INFO - Training set processing completed in 0.00s
2024-12-28 15:33:04,129 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:33:04,130 - INFO - Memory usage at start_fit: CPU 1877.0 MB, GPU 104.3 MB
2024-12-28 15:33:04,130 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:33:04,135 - INFO - Number of unique classes: 43
2024-12-28 15:33:04,293 - INFO - Fitted scaler and transformed data
2024-12-28 15:33:04,293 - INFO - Scaling time: 0.16s
2024-12-28 15:33:04,900 - INFO - Epoch 1/1000, Train Loss: 2.2960, Val Loss: 1.9163
2024-12-28 15:33:05,368 - INFO - Epoch 2/1000, Train Loss: 1.7467, Val Loss: 1.7459
2024-12-28 15:33:05,892 - INFO - Epoch 3/1000, Train Loss: 1.5971, Val Loss: 1.6566
2024-12-28 15:33:06,410 - INFO - Epoch 4/1000, Train Loss: 1.5114, Val Loss: 1.6155
2024-12-28 15:33:06,938 - INFO - Epoch 5/1000, Train Loss: 1.4590, Val Loss: 1.5932
2024-12-28 15:33:07,427 - INFO - Epoch 6/1000, Train Loss: 1.4217, Val Loss: 1.5729
2024-12-28 15:33:07,936 - INFO - Epoch 7/1000, Train Loss: 1.3970, Val Loss: 1.5453
2024-12-28 15:33:08,411 - INFO - Epoch 8/1000, Train Loss: 1.3734, Val Loss: 1.5384
2024-12-28 15:33:08,908 - INFO - Epoch 9/1000, Train Loss: 1.3617, Val Loss: 1.5240
2024-12-28 15:33:09,387 - INFO - Epoch 10/1000, Train Loss: 1.3495, Val Loss: 1.5187
2024-12-28 15:33:09,878 - INFO - Epoch 11/1000, Train Loss: 1.3408, Val Loss: 1.5186
2024-12-28 15:33:10,356 - INFO - Epoch 12/1000, Train Loss: 1.3323, Val Loss: 1.5165
2024-12-28 15:33:10,866 - INFO - Epoch 13/1000, Train Loss: 1.3274, Val Loss: 1.5128
2024-12-28 15:33:11,393 - INFO - Epoch 14/1000, Train Loss: 1.3221, Val Loss: 1.5027
2024-12-28 15:33:11,970 - INFO - Epoch 15/1000, Train Loss: 1.3192, Val Loss: 1.4999
2024-12-28 15:33:12,491 - INFO - Epoch 16/1000, Train Loss: 1.3149, Val Loss: 1.5042
2024-12-28 15:33:13,006 - INFO - Epoch 17/1000, Train Loss: 1.3133, Val Loss: 1.5037
2024-12-28 15:33:13,500 - INFO - Epoch 18/1000, Train Loss: 1.3117, Val Loss: 1.5067
2024-12-28 15:33:13,985 - INFO - Epoch 19/1000, Train Loss: 1.3113, Val Loss: 1.5002
2024-12-28 15:33:14,470 - INFO - Epoch 20/1000, Train Loss: 1.3089, Val Loss: 1.4954
2024-12-28 15:33:14,961 - INFO - Epoch 21/1000, Train Loss: 1.3072, Val Loss: 1.4883
2024-12-28 15:33:15,492 - INFO - Epoch 22/1000, Train Loss: 1.3060, Val Loss: 1.5002
2024-12-28 15:33:15,995 - INFO - Epoch 23/1000, Train Loss: 1.3050, Val Loss: 1.4997
2024-12-28 15:33:16,492 - INFO - Epoch 24/1000, Train Loss: 1.3055, Val Loss: 1.4972
2024-12-28 15:33:16,960 - INFO - Epoch 25/1000, Train Loss: 1.3046, Val Loss: 1.4970
2024-12-28 15:33:17,417 - INFO - Epoch 26/1000, Train Loss: 1.3049, Val Loss: 1.4898
2024-12-28 15:33:17,417 - INFO - Early stopping triggered at epoch 26
2024-12-28 15:33:17,417 - INFO - Training completed in 13.29s
2024-12-28 15:33:17,417 - INFO - Final memory usage: CPU 1915.6 MB, GPU 104.5 MB
2024-12-28 15:33:17,418 - INFO - Model training completed in 13.29s
2024-12-28 15:33:17,483 - INFO - Prediction completed in 0.06s
2024-12-28 15:33:17,494 - INFO - Poison rate 0.1 completed in 13.40s
2024-12-28 15:33:17,494 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:33:17,577 - INFO - Total number of labels flipped: 3951
2024-12-28 15:33:17,577 - INFO - Label flipping completed in 0.08s
2024-12-28 15:33:17,577 - INFO - Training set processing completed in 0.00s
2024-12-28 15:33:17,577 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:33:17,578 - INFO - Memory usage at start_fit: CPU 1877.0 MB, GPU 104.3 MB
2024-12-28 15:33:17,578 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:33:17,585 - INFO - Number of unique classes: 43
2024-12-28 15:33:17,743 - INFO - Fitted scaler and transformed data
2024-12-28 15:33:17,744 - INFO - Scaling time: 0.16s
2024-12-28 15:33:18,218 - INFO - Epoch 1/1000, Train Loss: 2.6278, Val Loss: 2.2728
2024-12-28 15:33:18,680 - INFO - Epoch 2/1000, Train Loss: 2.1736, Val Loss: 2.1267
2024-12-28 15:33:19,112 - INFO - Epoch 3/1000, Train Loss: 2.0401, Val Loss: 2.0626
2024-12-28 15:33:19,529 - INFO - Epoch 4/1000, Train Loss: 1.9623, Val Loss: 2.0208
2024-12-28 15:33:19,994 - INFO - Epoch 5/1000, Train Loss: 1.9120, Val Loss: 2.0063
2024-12-28 15:33:20,447 - INFO - Epoch 6/1000, Train Loss: 1.8782, Val Loss: 1.9886
2024-12-28 15:33:20,880 - INFO - Epoch 7/1000, Train Loss: 1.8509, Val Loss: 1.9774
2024-12-28 15:33:21,352 - INFO - Epoch 8/1000, Train Loss: 1.8328, Val Loss: 1.9667
2024-12-28 15:33:21,803 - INFO - Epoch 9/1000, Train Loss: 1.8198, Val Loss: 1.9588
2024-12-28 15:33:22,237 - INFO - Epoch 10/1000, Train Loss: 1.8064, Val Loss: 1.9545
2024-12-28 15:33:22,702 - INFO - Epoch 11/1000, Train Loss: 1.7972, Val Loss: 1.9439
2024-12-28 15:33:23,142 - INFO - Epoch 12/1000, Train Loss: 1.7891, Val Loss: 1.9532
2024-12-28 15:33:23,586 - INFO - Epoch 13/1000, Train Loss: 1.7828, Val Loss: 1.9364
2024-12-28 15:33:23,999 - INFO - Epoch 14/1000, Train Loss: 1.7819, Val Loss: 1.9409
2024-12-28 15:33:24,449 - INFO - Epoch 15/1000, Train Loss: 1.7752, Val Loss: 1.9465
2024-12-28 15:33:24,963 - INFO - Epoch 16/1000, Train Loss: 1.7716, Val Loss: 1.9465
2024-12-28 15:33:25,467 - INFO - Epoch 17/1000, Train Loss: 1.7687, Val Loss: 1.9390
2024-12-28 15:33:25,910 - INFO - Epoch 18/1000, Train Loss: 1.7674, Val Loss: 1.9368
2024-12-28 15:33:25,911 - INFO - Early stopping triggered at epoch 18
2024-12-28 15:33:25,911 - INFO - Training completed in 8.33s
2024-12-28 15:33:25,911 - INFO - Final memory usage: CPU 1915.6 MB, GPU 104.5 MB
2024-12-28 15:33:25,912 - INFO - Model training completed in 8.34s
2024-12-28 15:33:26,000 - INFO - Prediction completed in 0.09s
2024-12-28 15:33:26,011 - INFO - Poison rate 0.2 completed in 8.52s
2024-12-28 15:33:26,016 - INFO - Loaded 182 existing results
2024-12-28 15:33:26,016 - INFO - Total results to save: 189
2024-12-28 15:33:26,017 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:33:26,028 - INFO - Saved 189 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:33:26,028 - INFO - Total evaluation time: 113.24s
2024-12-28 15:33:26,035 - INFO - 
Progress: 29.2% - Evaluating GTSRB with LogisticRegression (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:33:26,232 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:33:26,232 - INFO - Dataset type: image
2024-12-28 15:33:26,232 - INFO - Sample size: 39209
2024-12-28 15:33:26,232 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:33:26,233 - INFO - Loading datasets...
2024-12-28 15:33:45,562 - INFO - Dataset loading completed in 19.33s
2024-12-28 15:33:45,562 - INFO - Extracting validation features...
2024-12-28 15:33:45,562 - INFO - Extracting features from 4435 samples...
2024-12-28 15:33:46,324 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:33:46,328 - INFO - Validation feature extraction completed in 0.77s
2024-12-28 15:33:46,328 - INFO - Extracting training features...
2024-12-28 15:33:46,328 - INFO - Extracting features from 19755 samples...
2024-12-28 15:33:49,050 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:33:49,061 - INFO - Training feature extraction completed in 2.73s
2024-12-28 15:33:49,062 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 15:33:49,062 - INFO - Using device: cuda
2024-12-28 15:33:49,062 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:33:49,062 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:33:49,062 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:33:50,381 - INFO - Feature scaling completed in 1.32s
2024-12-28 15:33:50,382 - INFO - Starting feature selection (k=50)
2024-12-28 15:33:50,412 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:33:50,412 - INFO - Starting anomaly detection
2024-12-28 15:33:55,807 - INFO - Anomaly detection completed in 5.39s
2024-12-28 15:33:55,807 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:33:55,807 - INFO - Total fit_transform time: 6.75s
2024-12-28 15:33:55,807 - INFO - Training set processing completed in 6.75s
2024-12-28 15:33:55,807 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:33:55,809 - INFO - Memory usage at start_fit: CPU 1927.3 MB, GPU 104.0 MB
2024-12-28 15:33:55,809 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:33:55,814 - INFO - Number of unique classes: 43
2024-12-28 15:33:55,952 - INFO - Fitted scaler and transformed data
2024-12-28 15:33:55,952 - INFO - Scaling time: 0.14s
2024-12-28 15:33:56,401 - INFO - Epoch 1/1000, Train Loss: 1.9081, Val Loss: 1.3490
2024-12-28 15:33:56,835 - INFO - Epoch 2/1000, Train Loss: 1.1997, Val Loss: 1.1009
2024-12-28 15:33:57,289 - INFO - Epoch 3/1000, Train Loss: 1.0147, Val Loss: 0.9998
2024-12-28 15:33:57,733 - INFO - Epoch 4/1000, Train Loss: 0.9198, Val Loss: 0.9361
2024-12-28 15:33:58,217 - INFO - Epoch 5/1000, Train Loss: 0.8621, Val Loss: 0.8948
2024-12-28 15:33:58,657 - INFO - Epoch 6/1000, Train Loss: 0.8233, Val Loss: 0.8647
2024-12-28 15:33:59,120 - INFO - Epoch 7/1000, Train Loss: 0.7980, Val Loss: 0.8529
2024-12-28 15:33:59,587 - INFO - Epoch 8/1000, Train Loss: 0.7801, Val Loss: 0.8402
2024-12-28 15:34:00,041 - INFO - Epoch 9/1000, Train Loss: 0.7651, Val Loss: 0.8305
2024-12-28 15:34:00,463 - INFO - Epoch 10/1000, Train Loss: 0.7564, Val Loss: 0.8181
2024-12-28 15:34:00,894 - INFO - Epoch 11/1000, Train Loss: 0.7467, Val Loss: 0.8236
2024-12-28 15:34:01,364 - INFO - Epoch 12/1000, Train Loss: 0.7424, Val Loss: 0.8134
2024-12-28 15:34:01,847 - INFO - Epoch 13/1000, Train Loss: 0.7366, Val Loss: 0.8045
2024-12-28 15:34:02,288 - INFO - Epoch 14/1000, Train Loss: 0.7329, Val Loss: 0.8056
2024-12-28 15:34:02,738 - INFO - Epoch 15/1000, Train Loss: 0.7298, Val Loss: 0.8095
2024-12-28 15:34:03,193 - INFO - Epoch 16/1000, Train Loss: 0.7264, Val Loss: 0.7998
2024-12-28 15:34:03,648 - INFO - Epoch 17/1000, Train Loss: 0.7244, Val Loss: 0.7949
2024-12-28 15:34:04,113 - INFO - Epoch 18/1000, Train Loss: 0.7240, Val Loss: 0.7937
2024-12-28 15:34:04,574 - INFO - Epoch 19/1000, Train Loss: 0.7217, Val Loss: 0.7967
2024-12-28 15:34:05,030 - INFO - Epoch 20/1000, Train Loss: 0.7212, Val Loss: 0.7861
2024-12-28 15:34:05,477 - INFO - Epoch 21/1000, Train Loss: 0.7196, Val Loss: 0.7928
2024-12-28 15:34:05,953 - INFO - Epoch 22/1000, Train Loss: 0.7206, Val Loss: 0.7915
2024-12-28 15:34:06,428 - INFO - Epoch 23/1000, Train Loss: 0.7193, Val Loss: 0.7836
2024-12-28 15:34:06,865 - INFO - Epoch 24/1000, Train Loss: 0.7193, Val Loss: 0.7917
2024-12-28 15:34:07,305 - INFO - Epoch 25/1000, Train Loss: 0.7178, Val Loss: 0.7901
2024-12-28 15:34:07,759 - INFO - Epoch 26/1000, Train Loss: 0.7175, Val Loss: 0.7980
2024-12-28 15:34:08,186 - INFO - Epoch 27/1000, Train Loss: 0.7166, Val Loss: 0.7814
2024-12-28 15:34:08,643 - INFO - Epoch 28/1000, Train Loss: 0.7152, Val Loss: 0.7838
2024-12-28 15:34:09,089 - INFO - Epoch 29/1000, Train Loss: 0.7178, Val Loss: 0.7884
2024-12-28 15:34:09,613 - INFO - Epoch 30/1000, Train Loss: 0.7162, Val Loss: 0.7816
2024-12-28 15:34:10,057 - INFO - Epoch 31/1000, Train Loss: 0.7171, Val Loss: 0.7990
2024-12-28 15:34:10,517 - INFO - Epoch 32/1000, Train Loss: 0.7152, Val Loss: 0.7894
2024-12-28 15:34:10,518 - INFO - Early stopping triggered at epoch 32
2024-12-28 15:34:10,518 - INFO - Training completed in 14.71s
2024-12-28 15:34:10,519 - INFO - Final memory usage: CPU 1906.1 MB, GPU 104.5 MB
2024-12-28 15:34:10,520 - INFO - Model training completed in 14.71s
2024-12-28 15:34:10,606 - INFO - Prediction completed in 0.09s
2024-12-28 15:34:10,618 - INFO - Poison rate 0.0 completed in 21.56s
2024-12-28 15:34:10,618 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:34:10,623 - INFO - Total number of labels flipped: 197
2024-12-28 15:34:10,623 - INFO - Label flipping completed in 0.01s
2024-12-28 15:34:10,623 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:34:10,623 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:34:11,868 - INFO - Feature scaling completed in 1.24s
2024-12-28 15:34:11,868 - INFO - Starting feature selection (k=50)
2024-12-28 15:34:11,895 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:34:11,896 - INFO - Starting anomaly detection
2024-12-28 15:34:20,090 - INFO - Anomaly detection completed in 8.19s
2024-12-28 15:34:20,090 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:34:20,090 - INFO - Total fit_transform time: 9.47s
2024-12-28 15:34:20,091 - INFO - Training set processing completed in 9.47s
2024-12-28 15:34:20,091 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:34:20,092 - INFO - Memory usage at start_fit: CPU 1886.7 MB, GPU 104.3 MB
2024-12-28 15:34:20,092 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:34:20,099 - INFO - Number of unique classes: 43
2024-12-28 15:34:20,245 - INFO - Fitted scaler and transformed data
2024-12-28 15:34:20,245 - INFO - Scaling time: 0.14s
2024-12-28 15:34:20,694 - INFO - Epoch 1/1000, Train Loss: 1.9264, Val Loss: 1.4265
2024-12-28 15:34:21,130 - INFO - Epoch 2/1000, Train Loss: 1.2545, Val Loss: 1.1819
2024-12-28 15:34:21,622 - INFO - Epoch 3/1000, Train Loss: 1.0742, Val Loss: 1.0759
2024-12-28 15:34:22,102 - INFO - Epoch 4/1000, Train Loss: 0.9835, Val Loss: 1.0226
2024-12-28 15:34:22,578 - INFO - Epoch 5/1000, Train Loss: 0.9267, Val Loss: 0.9808
2024-12-28 15:34:23,012 - INFO - Epoch 6/1000, Train Loss: 0.8915, Val Loss: 0.9502
2024-12-28 15:34:23,450 - INFO - Epoch 7/1000, Train Loss: 0.8646, Val Loss: 0.9489
2024-12-28 15:34:23,939 - INFO - Epoch 8/1000, Train Loss: 0.8458, Val Loss: 0.9237
2024-12-28 15:34:24,461 - INFO - Epoch 9/1000, Train Loss: 0.8320, Val Loss: 0.9219
2024-12-28 15:34:24,950 - INFO - Epoch 10/1000, Train Loss: 0.8216, Val Loss: 0.9162
2024-12-28 15:34:25,409 - INFO - Epoch 11/1000, Train Loss: 0.8154, Val Loss: 0.9023
2024-12-28 15:34:25,824 - INFO - Epoch 12/1000, Train Loss: 0.8108, Val Loss: 0.8977
2024-12-28 15:34:26,264 - INFO - Epoch 13/1000, Train Loss: 0.8027, Val Loss: 0.8955
2024-12-28 15:34:26,728 - INFO - Epoch 14/1000, Train Loss: 0.7996, Val Loss: 0.8910
2024-12-28 15:34:27,202 - INFO - Epoch 15/1000, Train Loss: 0.7964, Val Loss: 0.8924
2024-12-28 15:34:27,683 - INFO - Epoch 16/1000, Train Loss: 0.7940, Val Loss: 0.8917
2024-12-28 15:34:28,138 - INFO - Epoch 17/1000, Train Loss: 0.7919, Val Loss: 0.8845
2024-12-28 15:34:28,625 - INFO - Epoch 18/1000, Train Loss: 0.7888, Val Loss: 0.8846
2024-12-28 15:34:29,142 - INFO - Epoch 19/1000, Train Loss: 0.7898, Val Loss: 0.8874
2024-12-28 15:34:29,622 - INFO - Epoch 20/1000, Train Loss: 0.7891, Val Loss: 0.8953
2024-12-28 15:34:30,145 - INFO - Epoch 21/1000, Train Loss: 0.7849, Val Loss: 0.8822
2024-12-28 15:34:30,609 - INFO - Epoch 22/1000, Train Loss: 0.7840, Val Loss: 0.8844
2024-12-28 15:34:31,070 - INFO - Epoch 23/1000, Train Loss: 0.7840, Val Loss: 0.8775
2024-12-28 15:34:31,524 - INFO - Epoch 24/1000, Train Loss: 0.7848, Val Loss: 0.8843
2024-12-28 15:34:31,975 - INFO - Epoch 25/1000, Train Loss: 0.7850, Val Loss: 0.8859
2024-12-28 15:34:32,434 - INFO - Epoch 26/1000, Train Loss: 0.7835, Val Loss: 0.8837
2024-12-28 15:34:32,877 - INFO - Epoch 27/1000, Train Loss: 0.7810, Val Loss: 0.8908
2024-12-28 15:34:33,317 - INFO - Epoch 28/1000, Train Loss: 0.7829, Val Loss: 0.8789
2024-12-28 15:34:33,317 - INFO - Early stopping triggered at epoch 28
2024-12-28 15:34:33,317 - INFO - Training completed in 13.23s
2024-12-28 15:34:33,318 - INFO - Final memory usage: CPU 1925.3 MB, GPU 104.5 MB
2024-12-28 15:34:33,319 - INFO - Model training completed in 13.23s
2024-12-28 15:34:33,409 - INFO - Prediction completed in 0.09s
2024-12-28 15:34:33,420 - INFO - Poison rate 0.01 completed in 22.80s
2024-12-28 15:34:33,420 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:34:33,431 - INFO - Total number of labels flipped: 592
2024-12-28 15:34:33,432 - INFO - Label flipping completed in 0.01s
2024-12-28 15:34:33,432 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:34:33,432 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:34:34,747 - INFO - Feature scaling completed in 1.32s
2024-12-28 15:34:34,747 - INFO - Starting feature selection (k=50)
2024-12-28 15:34:34,776 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:34:34,776 - INFO - Starting anomaly detection
2024-12-28 15:34:42,082 - INFO - Anomaly detection completed in 7.31s
2024-12-28 15:34:42,082 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:34:42,083 - INFO - Total fit_transform time: 8.65s
2024-12-28 15:34:42,083 - INFO - Training set processing completed in 8.65s
2024-12-28 15:34:42,083 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:34:42,084 - INFO - Memory usage at start_fit: CPU 1886.7 MB, GPU 104.3 MB
2024-12-28 15:34:42,084 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:34:42,090 - INFO - Number of unique classes: 43
2024-12-28 15:34:42,241 - INFO - Fitted scaler and transformed data
2024-12-28 15:34:42,241 - INFO - Scaling time: 0.15s
2024-12-28 15:34:42,642 - INFO - Epoch 1/1000, Train Loss: 2.0309, Val Loss: 1.6099
2024-12-28 15:34:43,030 - INFO - Epoch 2/1000, Train Loss: 1.3817, Val Loss: 1.3971
2024-12-28 15:34:43,443 - INFO - Epoch 3/1000, Train Loss: 1.2099, Val Loss: 1.3074
2024-12-28 15:34:43,867 - INFO - Epoch 4/1000, Train Loss: 1.1223, Val Loss: 1.2425
2024-12-28 15:34:44,345 - INFO - Epoch 5/1000, Train Loss: 1.0671, Val Loss: 1.2098
2024-12-28 15:34:44,795 - INFO - Epoch 6/1000, Train Loss: 1.0288, Val Loss: 1.1799
2024-12-28 15:34:45,254 - INFO - Epoch 7/1000, Train Loss: 1.0053, Val Loss: 1.1719
2024-12-28 15:34:45,682 - INFO - Epoch 8/1000, Train Loss: 0.9850, Val Loss: 1.1557
2024-12-28 15:34:46,132 - INFO - Epoch 9/1000, Train Loss: 0.9720, Val Loss: 1.1410
2024-12-28 15:34:46,597 - INFO - Epoch 10/1000, Train Loss: 0.9611, Val Loss: 1.1437
2024-12-28 15:34:47,063 - INFO - Epoch 11/1000, Train Loss: 0.9518, Val Loss: 1.1258
2024-12-28 15:34:47,538 - INFO - Epoch 12/1000, Train Loss: 0.9465, Val Loss: 1.1201
2024-12-28 15:34:48,014 - INFO - Epoch 13/1000, Train Loss: 0.9414, Val Loss: 1.1130
2024-12-28 15:34:48,532 - INFO - Epoch 14/1000, Train Loss: 0.9390, Val Loss: 1.1184
2024-12-28 15:34:49,024 - INFO - Epoch 15/1000, Train Loss: 0.9345, Val Loss: 1.1231
2024-12-28 15:34:49,472 - INFO - Epoch 16/1000, Train Loss: 0.9314, Val Loss: 1.1089
2024-12-28 15:34:49,931 - INFO - Epoch 17/1000, Train Loss: 0.9298, Val Loss: 1.1137
2024-12-28 15:34:50,398 - INFO - Epoch 18/1000, Train Loss: 0.9279, Val Loss: 1.1183
2024-12-28 15:34:50,873 - INFO - Epoch 19/1000, Train Loss: 0.9260, Val Loss: 1.1024
2024-12-28 15:34:51,440 - INFO - Epoch 20/1000, Train Loss: 0.9244, Val Loss: 1.1011
2024-12-28 15:34:51,906 - INFO - Epoch 21/1000, Train Loss: 0.9256, Val Loss: 1.1138
2024-12-28 15:34:52,324 - INFO - Epoch 22/1000, Train Loss: 0.9236, Val Loss: 1.1107
2024-12-28 15:34:52,800 - INFO - Epoch 23/1000, Train Loss: 0.9230, Val Loss: 1.1083
2024-12-28 15:34:53,232 - INFO - Epoch 24/1000, Train Loss: 0.9223, Val Loss: 1.1151
2024-12-28 15:34:53,724 - INFO - Epoch 25/1000, Train Loss: 0.9215, Val Loss: 1.0977
2024-12-28 15:34:54,214 - INFO - Epoch 26/1000, Train Loss: 0.9206, Val Loss: 1.1029
2024-12-28 15:34:54,713 - INFO - Epoch 27/1000, Train Loss: 0.9215, Val Loss: 1.1046
2024-12-28 15:34:55,224 - INFO - Epoch 28/1000, Train Loss: 0.9202, Val Loss: 1.1068
2024-12-28 15:34:55,733 - INFO - Epoch 29/1000, Train Loss: 0.9197, Val Loss: 1.1052
2024-12-28 15:34:56,194 - INFO - Epoch 30/1000, Train Loss: 0.9217, Val Loss: 1.1082
2024-12-28 15:34:56,194 - INFO - Early stopping triggered at epoch 30
2024-12-28 15:34:56,194 - INFO - Training completed in 14.11s
2024-12-28 15:34:56,194 - INFO - Final memory usage: CPU 1925.3 MB, GPU 104.5 MB
2024-12-28 15:34:56,195 - INFO - Model training completed in 14.11s
2024-12-28 15:34:56,256 - INFO - Prediction completed in 0.06s
2024-12-28 15:34:56,267 - INFO - Poison rate 0.03 completed in 22.85s
2024-12-28 15:34:56,268 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:34:56,287 - INFO - Total number of labels flipped: 987
2024-12-28 15:34:56,287 - INFO - Label flipping completed in 0.02s
2024-12-28 15:34:56,287 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:34:56,287 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:34:57,563 - INFO - Feature scaling completed in 1.28s
2024-12-28 15:34:57,563 - INFO - Starting feature selection (k=50)
2024-12-28 15:34:57,592 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:34:57,592 - INFO - Starting anomaly detection
2024-12-28 15:35:04,195 - INFO - Anomaly detection completed in 6.60s
2024-12-28 15:35:04,195 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:35:04,195 - INFO - Total fit_transform time: 7.91s
2024-12-28 15:35:04,195 - INFO - Training set processing completed in 7.91s
2024-12-28 15:35:04,195 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:35:04,196 - INFO - Memory usage at start_fit: CPU 1886.7 MB, GPU 104.3 MB
2024-12-28 15:35:04,196 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:35:04,203 - INFO - Number of unique classes: 43
2024-12-28 15:35:04,346 - INFO - Fitted scaler and transformed data
2024-12-28 15:35:04,346 - INFO - Scaling time: 0.14s
2024-12-28 15:35:04,760 - INFO - Epoch 1/1000, Train Loss: 2.1266, Val Loss: 1.6248
2024-12-28 15:35:05,190 - INFO - Epoch 2/1000, Train Loss: 1.5006, Val Loss: 1.4155
2024-12-28 15:35:05,714 - INFO - Epoch 3/1000, Train Loss: 1.3365, Val Loss: 1.3157
2024-12-28 15:35:06,163 - INFO - Epoch 4/1000, Train Loss: 1.2470, Val Loss: 1.2553
2024-12-28 15:35:06,686 - INFO - Epoch 5/1000, Train Loss: 1.1944, Val Loss: 1.2357
2024-12-28 15:35:07,170 - INFO - Epoch 6/1000, Train Loss: 1.1558, Val Loss: 1.2086
2024-12-28 15:35:07,634 - INFO - Epoch 7/1000, Train Loss: 1.1296, Val Loss: 1.1919
2024-12-28 15:35:08,061 - INFO - Epoch 8/1000, Train Loss: 1.1102, Val Loss: 1.1810
2024-12-28 15:35:08,500 - INFO - Epoch 9/1000, Train Loss: 1.0970, Val Loss: 1.1568
2024-12-28 15:35:08,960 - INFO - Epoch 10/1000, Train Loss: 1.0856, Val Loss: 1.1587
2024-12-28 15:35:09,438 - INFO - Epoch 11/1000, Train Loss: 1.0747, Val Loss: 1.1565
2024-12-28 15:35:09,904 - INFO - Epoch 12/1000, Train Loss: 1.0696, Val Loss: 1.1469
2024-12-28 15:35:10,387 - INFO - Epoch 13/1000, Train Loss: 1.0648, Val Loss: 1.1463
2024-12-28 15:35:10,846 - INFO - Epoch 14/1000, Train Loss: 1.0619, Val Loss: 1.1357
2024-12-28 15:35:11,331 - INFO - Epoch 15/1000, Train Loss: 1.0557, Val Loss: 1.1352
2024-12-28 15:35:11,802 - INFO - Epoch 16/1000, Train Loss: 1.0535, Val Loss: 1.1410
2024-12-28 15:35:12,278 - INFO - Epoch 17/1000, Train Loss: 1.0508, Val Loss: 1.1288
2024-12-28 15:35:12,747 - INFO - Epoch 18/1000, Train Loss: 1.0494, Val Loss: 1.1323
2024-12-28 15:35:13,211 - INFO - Epoch 19/1000, Train Loss: 1.0484, Val Loss: 1.1394
2024-12-28 15:35:13,671 - INFO - Epoch 20/1000, Train Loss: 1.0455, Val Loss: 1.1386
2024-12-28 15:35:14,222 - INFO - Epoch 21/1000, Train Loss: 1.0453, Val Loss: 1.1399
2024-12-28 15:35:14,748 - INFO - Epoch 22/1000, Train Loss: 1.0439, Val Loss: 1.1364
2024-12-28 15:35:14,749 - INFO - Early stopping triggered at epoch 22
2024-12-28 15:35:14,749 - INFO - Training completed in 10.55s
2024-12-28 15:35:14,749 - INFO - Final memory usage: CPU 1925.3 MB, GPU 104.5 MB
2024-12-28 15:35:14,750 - INFO - Model training completed in 10.55s
2024-12-28 15:35:14,836 - INFO - Prediction completed in 0.09s
2024-12-28 15:35:14,853 - INFO - Poison rate 0.05 completed in 18.59s
2024-12-28 15:35:14,853 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:35:14,878 - INFO - Total number of labels flipped: 1382
2024-12-28 15:35:14,879 - INFO - Label flipping completed in 0.03s
2024-12-28 15:35:14,879 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:35:14,879 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:35:16,128 - INFO - Feature scaling completed in 1.25s
2024-12-28 15:35:16,129 - INFO - Starting feature selection (k=50)
2024-12-28 15:35:16,165 - INFO - Feature selection completed in 0.04s. Output shape: (19755, 50)
2024-12-28 15:35:16,166 - INFO - Starting anomaly detection
2024-12-28 15:35:24,184 - INFO - Anomaly detection completed in 8.02s
2024-12-28 15:35:24,184 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:35:24,184 - INFO - Total fit_transform time: 9.31s
2024-12-28 15:35:24,184 - INFO - Training set processing completed in 9.31s
2024-12-28 15:35:24,184 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:35:24,185 - INFO - Memory usage at start_fit: CPU 1886.7 MB, GPU 104.3 MB
2024-12-28 15:35:24,186 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:35:24,193 - INFO - Number of unique classes: 43
2024-12-28 15:35:24,337 - INFO - Fitted scaler and transformed data
2024-12-28 15:35:24,337 - INFO - Scaling time: 0.14s
2024-12-28 15:35:24,804 - INFO - Epoch 1/1000, Train Loss: 2.1922, Val Loss: 1.7677
2024-12-28 15:35:25,235 - INFO - Epoch 2/1000, Train Loss: 1.6016, Val Loss: 1.5511
2024-12-28 15:35:25,672 - INFO - Epoch 3/1000, Train Loss: 1.4415, Val Loss: 1.4533
2024-12-28 15:35:26,069 - INFO - Epoch 4/1000, Train Loss: 1.3556, Val Loss: 1.4003
2024-12-28 15:35:26,493 - INFO - Epoch 5/1000, Train Loss: 1.3022, Val Loss: 1.3725
2024-12-28 15:35:26,928 - INFO - Epoch 6/1000, Train Loss: 1.2660, Val Loss: 1.3505
2024-12-28 15:35:27,364 - INFO - Epoch 7/1000, Train Loss: 1.2423, Val Loss: 1.3345
2024-12-28 15:35:27,873 - INFO - Epoch 8/1000, Train Loss: 1.2206, Val Loss: 1.3169
2024-12-28 15:35:28,306 - INFO - Epoch 9/1000, Train Loss: 1.2036, Val Loss: 1.3174
2024-12-28 15:35:28,743 - INFO - Epoch 10/1000, Train Loss: 1.1949, Val Loss: 1.3083
2024-12-28 15:35:29,221 - INFO - Epoch 11/1000, Train Loss: 1.1872, Val Loss: 1.2952
2024-12-28 15:35:29,663 - INFO - Epoch 12/1000, Train Loss: 1.1775, Val Loss: 1.2988
2024-12-28 15:35:30,160 - INFO - Epoch 13/1000, Train Loss: 1.1749, Val Loss: 1.2875
2024-12-28 15:35:30,705 - INFO - Epoch 14/1000, Train Loss: 1.1688, Val Loss: 1.2871
2024-12-28 15:35:31,352 - INFO - Epoch 15/1000, Train Loss: 1.1642, Val Loss: 1.2855
2024-12-28 15:35:31,954 - INFO - Epoch 16/1000, Train Loss: 1.1623, Val Loss: 1.2757
2024-12-28 15:35:32,476 - INFO - Epoch 17/1000, Train Loss: 1.1600, Val Loss: 1.2744
2024-12-28 15:35:32,988 - INFO - Epoch 18/1000, Train Loss: 1.1579, Val Loss: 1.2770
2024-12-28 15:35:33,539 - INFO - Epoch 19/1000, Train Loss: 1.1576, Val Loss: 1.2761
2024-12-28 15:35:34,118 - INFO - Epoch 20/1000, Train Loss: 1.1542, Val Loss: 1.2800
2024-12-28 15:35:34,679 - INFO - Epoch 21/1000, Train Loss: 1.1556, Val Loss: 1.2808
2024-12-28 15:35:35,203 - INFO - Epoch 22/1000, Train Loss: 1.1534, Val Loss: 1.2714
2024-12-28 15:35:35,820 - INFO - Epoch 23/1000, Train Loss: 1.1516, Val Loss: 1.2774
2024-12-28 15:35:36,470 - INFO - Epoch 24/1000, Train Loss: 1.1512, Val Loss: 1.2765
2024-12-28 15:35:37,027 - INFO - Epoch 25/1000, Train Loss: 1.1498, Val Loss: 1.2736
2024-12-28 15:35:37,783 - INFO - Epoch 26/1000, Train Loss: 1.1521, Val Loss: 1.2671
2024-12-28 15:35:38,525 - INFO - Epoch 27/1000, Train Loss: 1.1498, Val Loss: 1.2749
2024-12-28 15:35:39,097 - INFO - Epoch 28/1000, Train Loss: 1.1505, Val Loss: 1.2749
2024-12-28 15:35:39,781 - INFO - Epoch 29/1000, Train Loss: 1.1486, Val Loss: 1.2680
2024-12-28 15:35:40,535 - INFO - Epoch 30/1000, Train Loss: 1.1493, Val Loss: 1.2627
2024-12-28 15:35:41,316 - INFO - Epoch 31/1000, Train Loss: 1.1486, Val Loss: 1.2727
2024-12-28 15:35:42,126 - INFO - Epoch 32/1000, Train Loss: 1.1499, Val Loss: 1.2722
2024-12-28 15:35:42,937 - INFO - Epoch 33/1000, Train Loss: 1.1462, Val Loss: 1.2789
2024-12-28 15:35:43,586 - INFO - Epoch 34/1000, Train Loss: 1.1501, Val Loss: 1.2796
2024-12-28 15:35:44,189 - INFO - Epoch 35/1000, Train Loss: 1.1494, Val Loss: 1.2725
2024-12-28 15:35:44,189 - INFO - Early stopping triggered at epoch 35
2024-12-28 15:35:44,189 - INFO - Training completed in 20.00s
2024-12-28 15:35:44,190 - INFO - Final memory usage: CPU 1925.3 MB, GPU 104.5 MB
2024-12-28 15:35:44,191 - INFO - Model training completed in 20.01s
2024-12-28 15:35:44,271 - INFO - Prediction completed in 0.08s
2024-12-28 15:35:44,288 - INFO - Poison rate 0.07 completed in 29.44s
2024-12-28 15:35:44,289 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:35:44,324 - INFO - Total number of labels flipped: 1975
2024-12-28 15:35:44,325 - INFO - Label flipping completed in 0.04s
2024-12-28 15:35:44,325 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:35:44,325 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:35:45,559 - INFO - Feature scaling completed in 1.23s
2024-12-28 15:35:45,559 - INFO - Starting feature selection (k=50)
2024-12-28 15:35:45,590 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:35:45,590 - INFO - Starting anomaly detection
2024-12-28 15:35:53,271 - INFO - Anomaly detection completed in 7.68s
2024-12-28 15:35:53,271 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:35:53,271 - INFO - Total fit_transform time: 8.95s
2024-12-28 15:35:53,271 - INFO - Training set processing completed in 8.95s
2024-12-28 15:35:53,272 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:35:53,273 - INFO - Memory usage at start_fit: CPU 1886.7 MB, GPU 104.3 MB
2024-12-28 15:35:53,273 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:35:53,280 - INFO - Number of unique classes: 43
2024-12-28 15:35:53,440 - INFO - Fitted scaler and transformed data
2024-12-28 15:35:53,440 - INFO - Scaling time: 0.16s
2024-12-28 15:35:53,903 - INFO - Epoch 1/1000, Train Loss: 2.3051, Val Loss: 1.9152
2024-12-28 15:35:54,346 - INFO - Epoch 2/1000, Train Loss: 1.7504, Val Loss: 1.7420
2024-12-28 15:35:54,783 - INFO - Epoch 3/1000, Train Loss: 1.6013, Val Loss: 1.6525
2024-12-28 15:35:55,251 - INFO - Epoch 4/1000, Train Loss: 1.5108, Val Loss: 1.6093
2024-12-28 15:35:55,750 - INFO - Epoch 5/1000, Train Loss: 1.4612, Val Loss: 1.5797
2024-12-28 15:35:56,164 - INFO - Epoch 6/1000, Train Loss: 1.4241, Val Loss: 1.5583
2024-12-28 15:35:56,601 - INFO - Epoch 7/1000, Train Loss: 1.3999, Val Loss: 1.5284
2024-12-28 15:35:57,015 - INFO - Epoch 8/1000, Train Loss: 1.3789, Val Loss: 1.5219
2024-12-28 15:35:57,492 - INFO - Epoch 9/1000, Train Loss: 1.3644, Val Loss: 1.5098
2024-12-28 15:35:57,958 - INFO - Epoch 10/1000, Train Loss: 1.3523, Val Loss: 1.5086
2024-12-28 15:35:58,433 - INFO - Epoch 11/1000, Train Loss: 1.3428, Val Loss: 1.4973
2024-12-28 15:35:58,962 - INFO - Epoch 12/1000, Train Loss: 1.3343, Val Loss: 1.4868
2024-12-28 15:35:59,384 - INFO - Epoch 13/1000, Train Loss: 1.3281, Val Loss: 1.5008
2024-12-28 15:35:59,819 - INFO - Epoch 14/1000, Train Loss: 1.3257, Val Loss: 1.5007
2024-12-28 15:36:00,253 - INFO - Epoch 15/1000, Train Loss: 1.3224, Val Loss: 1.4845
2024-12-28 15:36:00,673 - INFO - Epoch 16/1000, Train Loss: 1.3200, Val Loss: 1.4864
2024-12-28 15:36:01,111 - INFO - Epoch 17/1000, Train Loss: 1.3162, Val Loss: 1.4801
2024-12-28 15:36:01,576 - INFO - Epoch 18/1000, Train Loss: 1.3134, Val Loss: 1.4876
2024-12-28 15:36:02,024 - INFO - Epoch 19/1000, Train Loss: 1.3118, Val Loss: 1.4849
2024-12-28 15:36:02,483 - INFO - Epoch 20/1000, Train Loss: 1.3121, Val Loss: 1.4847
2024-12-28 15:36:02,904 - INFO - Epoch 21/1000, Train Loss: 1.3114, Val Loss: 1.4830
2024-12-28 15:36:03,383 - INFO - Epoch 22/1000, Train Loss: 1.3082, Val Loss: 1.4862
2024-12-28 15:36:03,383 - INFO - Early stopping triggered at epoch 22
2024-12-28 15:36:03,383 - INFO - Training completed in 10.11s
2024-12-28 15:36:03,384 - INFO - Final memory usage: CPU 1925.3 MB, GPU 104.5 MB
2024-12-28 15:36:03,385 - INFO - Model training completed in 10.11s
2024-12-28 15:36:03,454 - INFO - Prediction completed in 0.07s
2024-12-28 15:36:03,477 - INFO - Poison rate 0.1 completed in 19.19s
2024-12-28 15:36:03,477 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:36:03,548 - INFO - Total number of labels flipped: 3951
2024-12-28 15:36:03,548 - INFO - Label flipping completed in 0.07s
2024-12-28 15:36:03,548 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:36:03,548 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:36:04,839 - INFO - Feature scaling completed in 1.29s
2024-12-28 15:36:04,839 - INFO - Starting feature selection (k=50)
2024-12-28 15:36:04,868 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:36:04,868 - INFO - Starting anomaly detection
2024-12-28 15:36:11,524 - INFO - Anomaly detection completed in 6.66s
2024-12-28 15:36:11,524 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:36:11,524 - INFO - Total fit_transform time: 7.98s
2024-12-28 15:36:11,524 - INFO - Training set processing completed in 7.98s
2024-12-28 15:36:11,525 - INFO - Fitting LogisticRegressionWrapper model with data shape: (19755, 512)
2024-12-28 15:36:11,525 - INFO - Memory usage at start_fit: CPU 1886.7 MB, GPU 104.3 MB
2024-12-28 15:36:11,526 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:36:11,531 - INFO - Number of unique classes: 43
2024-12-28 15:36:11,677 - INFO - Fitted scaler and transformed data
2024-12-28 15:36:11,678 - INFO - Scaling time: 0.15s
2024-12-28 15:36:12,101 - INFO - Epoch 1/1000, Train Loss: 2.6228, Val Loss: 2.3309
2024-12-28 15:36:12,563 - INFO - Epoch 2/1000, Train Loss: 2.1647, Val Loss: 2.1993
2024-12-28 15:36:12,999 - INFO - Epoch 3/1000, Train Loss: 2.0292, Val Loss: 2.1279
2024-12-28 15:36:13,428 - INFO - Epoch 4/1000, Train Loss: 1.9536, Val Loss: 2.0836
2024-12-28 15:36:13,870 - INFO - Epoch 5/1000, Train Loss: 1.9044, Val Loss: 2.0695
2024-12-28 15:36:14,278 - INFO - Epoch 6/1000, Train Loss: 1.8700, Val Loss: 2.0535
2024-12-28 15:36:14,765 - INFO - Epoch 7/1000, Train Loss: 1.8476, Val Loss: 2.0498
2024-12-28 15:36:15,222 - INFO - Epoch 8/1000, Train Loss: 1.8270, Val Loss: 2.0336
2024-12-28 15:36:15,662 - INFO - Epoch 9/1000, Train Loss: 1.8119, Val Loss: 2.0236
2024-12-28 15:36:16,105 - INFO - Epoch 10/1000, Train Loss: 1.7998, Val Loss: 2.0153
2024-12-28 15:36:16,579 - INFO - Epoch 11/1000, Train Loss: 1.7894, Val Loss: 2.0184
2024-12-28 15:36:17,024 - INFO - Epoch 12/1000, Train Loss: 1.7835, Val Loss: 2.0199
2024-12-28 15:36:17,458 - INFO - Epoch 13/1000, Train Loss: 1.7786, Val Loss: 2.0122
2024-12-28 15:36:17,899 - INFO - Epoch 14/1000, Train Loss: 1.7705, Val Loss: 2.0198
2024-12-28 15:36:18,361 - INFO - Epoch 15/1000, Train Loss: 1.7700, Val Loss: 2.0020
2024-12-28 15:36:18,808 - INFO - Epoch 16/1000, Train Loss: 1.7637, Val Loss: 2.0000
2024-12-28 15:36:19,254 - INFO - Epoch 17/1000, Train Loss: 1.7630, Val Loss: 2.0058
2024-12-28 15:36:19,687 - INFO - Epoch 18/1000, Train Loss: 1.7599, Val Loss: 2.0060
2024-12-28 15:36:20,118 - INFO - Epoch 19/1000, Train Loss: 1.7594, Val Loss: 2.0058
2024-12-28 15:36:20,551 - INFO - Epoch 20/1000, Train Loss: 1.7573, Val Loss: 2.0018
2024-12-28 15:36:20,990 - INFO - Epoch 21/1000, Train Loss: 1.7547, Val Loss: 1.9946
2024-12-28 15:36:21,434 - INFO - Epoch 22/1000, Train Loss: 1.7539, Val Loss: 2.0018
2024-12-28 15:36:21,901 - INFO - Epoch 23/1000, Train Loss: 1.7535, Val Loss: 1.9991
2024-12-28 15:36:22,327 - INFO - Epoch 24/1000, Train Loss: 1.7525, Val Loss: 1.9941
2024-12-28 15:36:22,773 - INFO - Epoch 25/1000, Train Loss: 1.7516, Val Loss: 2.0016
2024-12-28 15:36:23,219 - INFO - Epoch 26/1000, Train Loss: 1.7524, Val Loss: 2.0077
2024-12-28 15:36:23,219 - INFO - Early stopping triggered at epoch 26
2024-12-28 15:36:23,219 - INFO - Training completed in 11.69s
2024-12-28 15:36:23,220 - INFO - Final memory usage: CPU 1925.3 MB, GPU 104.5 MB
2024-12-28 15:36:23,221 - INFO - Model training completed in 11.70s
2024-12-28 15:36:23,297 - INFO - Prediction completed in 0.08s
2024-12-28 15:36:23,308 - INFO - Poison rate 0.2 completed in 19.83s
2024-12-28 15:36:23,314 - INFO - Loaded 189 existing results
2024-12-28 15:36:23,314 - INFO - Total results to save: 196
2024-12-28 15:36:23,315 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:36:23,327 - INFO - Saved 196 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:36:23,327 - INFO - Total evaluation time: 177.09s
2024-12-28 15:36:23,334 - INFO - 
Progress: 30.2% - Evaluating GTSRB with RandomForest (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:36:23,537 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:36:23,537 - INFO - Dataset type: image
2024-12-28 15:36:23,537 - INFO - Sample size: 39209
2024-12-28 15:36:23,537 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:36:23,538 - INFO - Loading datasets...
2024-12-28 15:36:42,308 - INFO - Dataset loading completed in 18.77s
2024-12-28 15:36:42,309 - INFO - Extracting validation features...
2024-12-28 15:36:42,309 - INFO - Extracting features from 4435 samples...
2024-12-28 15:36:43,083 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:36:43,086 - INFO - Validation feature extraction completed in 0.78s
2024-12-28 15:36:43,087 - INFO - Extracting training features...
2024-12-28 15:36:43,087 - INFO - Extracting features from 19755 samples...
2024-12-28 15:36:45,870 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:36:45,882 - INFO - Training feature extraction completed in 2.79s
2024-12-28 15:36:45,882 - INFO - Creating model for classifier: RandomForest
2024-12-28 15:36:45,882 - INFO - Using device: cuda
2024-12-28 15:36:45,882 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:36:45,882 - INFO - Training set processing completed in 0.00s
2024-12-28 15:36:45,882 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:36:45,884 - INFO - Memory usage at start_fit: CPU 1928.0 MB, GPU 104.0 MB
2024-12-28 15:36:45,884 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:36:46,489 - INFO - Fitted scaler and transformed data
2024-12-28 15:36:46,489 - INFO - Scaling time: 0.60s
2024-12-28 15:36:46,510 - INFO - Number of unique classes: 43
2024-12-28 15:36:53,335 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:36:59,578 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 15:37:05,593 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7592
2024-12-28 15:37:12,472 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7585
2024-12-28 15:37:12,473 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:37:12,473 - INFO - Training completed in 26.59s
2024-12-28 15:37:12,473 - INFO - Final memory usage: CPU 1886.8 MB, GPU 153.6 MB
2024-12-28 15:37:12,473 - INFO - Model training completed in 26.59s
2024-12-28 15:37:12,660 - INFO - Prediction completed in 0.19s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:37:12,673 - INFO - Poison rate 0.0 completed in 26.79s
2024-12-28 15:37:12,673 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:37:12,679 - INFO - Total number of labels flipped: 197
2024-12-28 15:37:12,679 - INFO - Label flipping completed in 0.01s
2024-12-28 15:37:12,679 - INFO - Training set processing completed in 0.00s
2024-12-28 15:37:12,679 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:37:12,680 - INFO - Memory usage at start_fit: CPU 1886.8 MB, GPU 112.5 MB
2024-12-28 15:37:12,680 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:37:13,259 - INFO - Fitted scaler and transformed data
2024-12-28 15:37:13,259 - INFO - Scaling time: 0.58s
2024-12-28 15:37:13,279 - INFO - Number of unique classes: 43
2024-12-28 15:37:20,459 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:37:27,372 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7599
2024-12-28 15:37:33,202 - INFO - Epoch 3/10, Train Loss: 3.7595, Val Loss: 3.7593
2024-12-28 15:37:39,846 - INFO - Epoch 4/10, Train Loss: 3.7588, Val Loss: 3.7586
2024-12-28 15:37:39,846 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:37:39,846 - INFO - Training completed in 27.17s
2024-12-28 15:37:39,847 - INFO - Final memory usage: CPU 1886.8 MB, GPU 153.6 MB
2024-12-28 15:37:39,847 - INFO - Model training completed in 27.17s
2024-12-28 15:37:40,025 - INFO - Prediction completed in 0.18s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:37:40,037 - INFO - Poison rate 0.01 completed in 27.36s
2024-12-28 15:37:40,037 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:37:40,049 - INFO - Total number of labels flipped: 592
2024-12-28 15:37:40,049 - INFO - Label flipping completed in 0.01s
2024-12-28 15:37:40,050 - INFO - Training set processing completed in 0.00s
2024-12-28 15:37:40,050 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:37:40,050 - INFO - Memory usage at start_fit: CPU 1886.8 MB, GPU 112.5 MB
2024-12-28 15:37:40,051 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:37:40,573 - INFO - Fitted scaler and transformed data
2024-12-28 15:37:40,573 - INFO - Scaling time: 0.52s
2024-12-28 15:37:40,593 - INFO - Number of unique classes: 43
2024-12-28 15:37:47,088 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:37:54,083 - INFO - Epoch 2/10, Train Loss: 3.7602, Val Loss: 3.7600
2024-12-28 15:38:00,885 - INFO - Epoch 3/10, Train Loss: 3.7596, Val Loss: 3.7593
2024-12-28 15:38:07,218 - INFO - Epoch 4/10, Train Loss: 3.7589, Val Loss: 3.7587
2024-12-28 15:38:07,219 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:38:07,219 - INFO - Training completed in 27.17s
2024-12-28 15:38:07,219 - INFO - Final memory usage: CPU 1886.8 MB, GPU 153.6 MB
2024-12-28 15:38:07,219 - INFO - Model training completed in 27.17s
2024-12-28 15:38:07,544 - INFO - Prediction completed in 0.32s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:38:07,556 - INFO - Poison rate 0.03 completed in 27.52s
2024-12-28 15:38:07,556 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:38:07,575 - INFO - Total number of labels flipped: 987
2024-12-28 15:38:07,575 - INFO - Label flipping completed in 0.02s
2024-12-28 15:38:07,575 - INFO - Training set processing completed in 0.00s
2024-12-28 15:38:07,575 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:38:07,576 - INFO - Memory usage at start_fit: CPU 1886.8 MB, GPU 112.5 MB
2024-12-28 15:38:07,576 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:38:08,113 - INFO - Fitted scaler and transformed data
2024-12-28 15:38:08,113 - INFO - Scaling time: 0.54s
2024-12-28 15:38:08,133 - INFO - Number of unique classes: 43
2024-12-28 15:38:14,680 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7606
2024-12-28 15:38:21,304 - INFO - Epoch 2/10, Train Loss: 3.7603, Val Loss: 3.7600
2024-12-28 15:38:28,833 - INFO - Epoch 3/10, Train Loss: 3.7596, Val Loss: 3.7594
2024-12-28 15:38:34,950 - INFO - Epoch 4/10, Train Loss: 3.7589, Val Loss: 3.7587
2024-12-28 15:38:34,951 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:38:34,951 - INFO - Training completed in 27.37s
2024-12-28 15:38:34,951 - INFO - Final memory usage: CPU 1886.8 MB, GPU 153.6 MB
2024-12-28 15:38:34,951 - INFO - Model training completed in 27.38s
2024-12-28 15:38:35,229 - INFO - Prediction completed in 0.28s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:38:35,241 - INFO - Poison rate 0.05 completed in 27.68s
2024-12-28 15:38:35,242 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:38:35,267 - INFO - Total number of labels flipped: 1382
2024-12-28 15:38:35,267 - INFO - Label flipping completed in 0.03s
2024-12-28 15:38:35,268 - INFO - Training set processing completed in 0.00s
2024-12-28 15:38:35,268 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:38:35,268 - INFO - Memory usage at start_fit: CPU 1886.8 MB, GPU 112.5 MB
2024-12-28 15:38:35,268 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:38:35,781 - INFO - Fitted scaler and transformed data
2024-12-28 15:38:35,781 - INFO - Scaling time: 0.51s
2024-12-28 15:38:35,801 - INFO - Number of unique classes: 43
2024-12-28 15:38:41,867 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7607
2024-12-28 15:38:49,181 - INFO - Epoch 2/10, Train Loss: 3.7603, Val Loss: 3.7601
2024-12-28 15:38:55,252 - INFO - Epoch 3/10, Train Loss: 3.7597, Val Loss: 3.7595
2024-12-28 15:39:01,505 - INFO - Epoch 4/10, Train Loss: 3.7590, Val Loss: 3.7589
2024-12-28 15:39:01,505 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:39:01,505 - INFO - Training completed in 26.24s
2024-12-28 15:39:01,505 - INFO - Final memory usage: CPU 1886.8 MB, GPU 153.6 MB
2024-12-28 15:39:01,506 - INFO - Model training completed in 26.24s
2024-12-28 15:39:01,688 - INFO - Prediction completed in 0.18s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:39:01,700 - INFO - Poison rate 0.07 completed in 26.46s
2024-12-28 15:39:01,700 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:39:01,737 - INFO - Total number of labels flipped: 1975
2024-12-28 15:39:01,737 - INFO - Label flipping completed in 0.04s
2024-12-28 15:39:01,737 - INFO - Training set processing completed in 0.00s
2024-12-28 15:39:01,737 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:39:01,738 - INFO - Memory usage at start_fit: CPU 1886.8 MB, GPU 112.5 MB
2024-12-28 15:39:01,738 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:39:02,264 - INFO - Fitted scaler and transformed data
2024-12-28 15:39:02,264 - INFO - Scaling time: 0.53s
2024-12-28 15:39:02,285 - INFO - Number of unique classes: 43
2024-12-28 15:39:08,504 - INFO - Epoch 1/10, Train Loss: 3.7609, Val Loss: 3.7607
2024-12-28 15:39:15,566 - INFO - Epoch 2/10, Train Loss: 3.7604, Val Loss: 3.7601
2024-12-28 15:39:21,619 - INFO - Epoch 3/10, Train Loss: 3.7598, Val Loss: 3.7596
2024-12-28 15:39:28,105 - INFO - Epoch 4/10, Train Loss: 3.7591, Val Loss: 3.7590
2024-12-28 15:39:28,105 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:39:28,105 - INFO - Training completed in 26.37s
2024-12-28 15:39:28,105 - INFO - Final memory usage: CPU 1886.8 MB, GPU 153.6 MB
2024-12-28 15:39:28,106 - INFO - Model training completed in 26.37s
2024-12-28 15:39:28,303 - INFO - Prediction completed in 0.20s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:39:28,315 - INFO - Poison rate 0.1 completed in 26.62s
2024-12-28 15:39:28,316 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:39:28,387 - INFO - Total number of labels flipped: 3951
2024-12-28 15:39:28,388 - INFO - Label flipping completed in 0.07s
2024-12-28 15:39:28,388 - INFO - Training set processing completed in 0.00s
2024-12-28 15:39:28,388 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:39:28,389 - INFO - Memory usage at start_fit: CPU 1886.8 MB, GPU 112.5 MB
2024-12-28 15:39:28,389 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:39:28,895 - INFO - Fitted scaler and transformed data
2024-12-28 15:39:28,896 - INFO - Scaling time: 0.51s
2024-12-28 15:39:28,917 - INFO - Number of unique classes: 43
2024-12-28 15:39:34,453 - INFO - Epoch 1/10, Train Loss: 3.7610, Val Loss: 3.7608
2024-12-28 15:39:41,887 - INFO - Epoch 2/10, Train Loss: 3.7605, Val Loss: 3.7603
2024-12-28 15:39:48,254 - INFO - Epoch 3/10, Train Loss: 3.7600, Val Loss: 3.7598
2024-12-28 15:39:55,496 - INFO - Epoch 4/10, Train Loss: 3.7595, Val Loss: 3.7594
2024-12-28 15:39:55,496 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:39:55,497 - INFO - Training completed in 27.11s
2024-12-28 15:39:55,497 - INFO - Final memory usage: CPU 1886.8 MB, GPU 153.6 MB
2024-12-28 15:39:55,497 - INFO - Model training completed in 27.11s
2024-12-28 15:39:55,668 - INFO - Prediction completed in 0.17s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:39:55,680 - INFO - Poison rate 0.2 completed in 27.36s
2024-12-28 15:39:55,686 - INFO - Loaded 196 existing results
2024-12-28 15:39:55,686 - INFO - Total results to save: 203
2024-12-28 15:39:55,687 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:39:55,699 - INFO - Saved 203 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:39:55,699 - INFO - Total evaluation time: 212.16s
2024-12-28 15:39:55,705 - INFO - 
Progress: 31.2% - Evaluating GTSRB with RandomForest (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:39:55,885 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:39:55,885 - INFO - Dataset type: image
2024-12-28 15:39:55,885 - INFO - Sample size: 39209
2024-12-28 15:39:55,885 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:39:55,886 - INFO - Loading datasets...
2024-12-28 15:40:15,194 - INFO - Dataset loading completed in 19.31s
2024-12-28 15:40:15,194 - INFO - Extracting validation features...
2024-12-28 15:40:15,194 - INFO - Extracting features from 4435 samples...
2024-12-28 15:40:15,952 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:40:15,956 - INFO - Validation feature extraction completed in 0.76s
2024-12-28 15:40:15,956 - INFO - Extracting training features...
2024-12-28 15:40:15,956 - INFO - Extracting features from 19755 samples...
2024-12-28 15:40:18,694 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:40:18,706 - INFO - Training feature extraction completed in 2.75s
2024-12-28 15:40:18,707 - INFO - Creating model for classifier: RandomForest
2024-12-28 15:40:18,707 - INFO - Using device: cuda
2024-12-28 15:40:18,707 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:40:18,707 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:40:18,708 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:40:20,052 - INFO - Feature scaling completed in 1.34s
2024-12-28 15:40:20,052 - INFO - Starting feature selection (k=50)
2024-12-28 15:40:20,091 - INFO - Feature selection completed in 0.04s. Output shape: (19755, 50)
2024-12-28 15:40:20,092 - INFO - Starting anomaly detection
2024-12-28 15:40:26,435 - INFO - Anomaly detection completed in 6.34s
2024-12-28 15:40:26,436 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:40:26,436 - INFO - Total fit_transform time: 7.73s
2024-12-28 15:40:26,436 - INFO - Training set processing completed in 7.73s
2024-12-28 15:40:26,436 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:40:26,437 - INFO - Memory usage at start_fit: CPU 1886.9 MB, GPU 104.0 MB
2024-12-28 15:40:26,438 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:40:26,983 - INFO - Fitted scaler and transformed data
2024-12-28 15:40:26,983 - INFO - Scaling time: 0.55s
2024-12-28 15:40:26,997 - INFO - Number of unique classes: 43
2024-12-28 15:40:33,308 - INFO - Epoch 1/10, Train Loss: 3.5734, Val Loss: 3.7606
2024-12-28 15:40:39,359 - INFO - Epoch 2/10, Train Loss: 3.5727, Val Loss: 3.7599
2024-12-28 15:40:46,644 - INFO - Epoch 3/10, Train Loss: 3.5721, Val Loss: 3.7592
2024-12-28 15:40:53,857 - INFO - Epoch 4/10, Train Loss: 3.5714, Val Loss: 3.7585
2024-12-28 15:40:53,857 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:40:53,857 - INFO - Training completed in 27.42s
2024-12-28 15:40:53,857 - INFO - Final memory usage: CPU 1886.9 MB, GPU 153.6 MB
2024-12-28 15:40:53,858 - INFO - Model training completed in 27.42s
2024-12-28 15:40:54,058 - INFO - Prediction completed in 0.20s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:40:54,070 - INFO - Poison rate 0.0 completed in 35.36s
2024-12-28 15:40:54,070 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:40:54,075 - INFO - Total number of labels flipped: 197
2024-12-28 15:40:54,076 - INFO - Label flipping completed in 0.01s
2024-12-28 15:40:54,076 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:40:54,076 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:40:55,234 - INFO - Feature scaling completed in 1.16s
2024-12-28 15:40:55,235 - INFO - Starting feature selection (k=50)
2024-12-28 15:40:55,260 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:40:55,260 - INFO - Starting anomaly detection
2024-12-28 15:41:02,620 - INFO - Anomaly detection completed in 7.36s
2024-12-28 15:41:02,621 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:41:02,621 - INFO - Total fit_transform time: 8.54s
2024-12-28 15:41:02,621 - INFO - Training set processing completed in 8.55s
2024-12-28 15:41:02,621 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:41:02,622 - INFO - Memory usage at start_fit: CPU 1886.9 MB, GPU 112.5 MB
2024-12-28 15:41:02,622 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:41:03,171 - INFO - Fitted scaler and transformed data
2024-12-28 15:41:03,171 - INFO - Scaling time: 0.55s
2024-12-28 15:41:03,189 - INFO - Number of unique classes: 43
2024-12-28 15:41:10,344 - INFO - Epoch 1/10, Train Loss: 3.5733, Val Loss: 3.7606
2024-12-28 15:41:16,719 - INFO - Epoch 2/10, Train Loss: 3.5726, Val Loss: 3.7599
2024-12-28 15:41:23,000 - INFO - Epoch 3/10, Train Loss: 3.5719, Val Loss: 3.7593
2024-12-28 15:41:28,774 - INFO - Epoch 4/10, Train Loss: 3.5712, Val Loss: 3.7586
2024-12-28 15:41:28,775 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:41:28,775 - INFO - Training completed in 26.15s
2024-12-28 15:41:28,775 - INFO - Final memory usage: CPU 1886.9 MB, GPU 153.6 MB
2024-12-28 15:41:28,775 - INFO - Model training completed in 26.15s
2024-12-28 15:41:28,950 - INFO - Prediction completed in 0.17s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:41:28,961 - INFO - Poison rate 0.01 completed in 34.89s
2024-12-28 15:41:28,962 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:41:28,973 - INFO - Total number of labels flipped: 592
2024-12-28 15:41:28,973 - INFO - Label flipping completed in 0.01s
2024-12-28 15:41:28,973 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:41:28,973 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:41:30,235 - INFO - Feature scaling completed in 1.26s
2024-12-28 15:41:30,235 - INFO - Starting feature selection (k=50)
2024-12-28 15:41:30,259 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 15:41:30,259 - INFO - Starting anomaly detection
2024-12-28 15:41:38,055 - INFO - Anomaly detection completed in 7.80s
2024-12-28 15:41:38,056 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:41:38,056 - INFO - Total fit_transform time: 9.08s
2024-12-28 15:41:38,056 - INFO - Training set processing completed in 9.08s
2024-12-28 15:41:38,056 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:41:38,057 - INFO - Memory usage at start_fit: CPU 1886.9 MB, GPU 112.5 MB
2024-12-28 15:41:38,057 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:41:38,649 - INFO - Fitted scaler and transformed data
2024-12-28 15:41:38,649 - INFO - Scaling time: 0.59s
2024-12-28 15:41:38,670 - INFO - Number of unique classes: 43
2024-12-28 15:41:46,373 - INFO - Epoch 1/10, Train Loss: 3.5715, Val Loss: 3.7606
2024-12-28 15:41:52,841 - INFO - Epoch 2/10, Train Loss: 3.5709, Val Loss: 3.7600
2024-12-28 15:41:59,720 - INFO - Epoch 3/10, Train Loss: 3.5702, Val Loss: 3.7593
2024-12-28 15:42:06,042 - INFO - Epoch 4/10, Train Loss: 3.5696, Val Loss: 3.7586
2024-12-28 15:42:06,042 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:42:06,042 - INFO - Training completed in 27.99s
2024-12-28 15:42:06,043 - INFO - Final memory usage: CPU 1886.9 MB, GPU 153.6 MB
2024-12-28 15:42:06,043 - INFO - Model training completed in 27.99s
2024-12-28 15:42:06,376 - INFO - Prediction completed in 0.33s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:42:06,392 - INFO - Poison rate 0.03 completed in 37.43s
2024-12-28 15:42:06,392 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:42:06,431 - INFO - Total number of labels flipped: 987
2024-12-28 15:42:06,431 - INFO - Label flipping completed in 0.04s
2024-12-28 15:42:06,431 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:42:06,431 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:42:07,755 - INFO - Feature scaling completed in 1.32s
2024-12-28 15:42:07,755 - INFO - Starting feature selection (k=50)
2024-12-28 15:42:07,781 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:42:07,781 - INFO - Starting anomaly detection
2024-12-28 15:42:15,805 - INFO - Anomaly detection completed in 8.02s
2024-12-28 15:42:15,805 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:42:15,805 - INFO - Total fit_transform time: 9.37s
2024-12-28 15:42:15,805 - INFO - Training set processing completed in 9.37s
2024-12-28 15:42:15,805 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:42:15,806 - INFO - Memory usage at start_fit: CPU 1886.9 MB, GPU 112.5 MB
2024-12-28 15:42:15,806 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:42:16,373 - INFO - Fitted scaler and transformed data
2024-12-28 15:42:16,374 - INFO - Scaling time: 0.57s
2024-12-28 15:42:16,390 - INFO - Number of unique classes: 43
2024-12-28 15:42:23,617 - INFO - Epoch 1/10, Train Loss: 3.5741, Val Loss: 3.7606
2024-12-28 15:42:31,881 - INFO - Epoch 2/10, Train Loss: 3.5735, Val Loss: 3.7600
2024-12-28 15:42:38,320 - INFO - Epoch 3/10, Train Loss: 3.5729, Val Loss: 3.7594
2024-12-28 15:42:44,416 - INFO - Epoch 4/10, Train Loss: 3.5722, Val Loss: 3.7587
2024-12-28 15:42:44,416 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:42:44,416 - INFO - Training completed in 28.61s
2024-12-28 15:42:44,416 - INFO - Final memory usage: CPU 1886.9 MB, GPU 153.6 MB
2024-12-28 15:42:44,417 - INFO - Model training completed in 28.61s
2024-12-28 15:42:44,596 - INFO - Prediction completed in 0.18s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:42:44,608 - INFO - Poison rate 0.05 completed in 38.22s
2024-12-28 15:42:44,608 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:42:44,634 - INFO - Total number of labels flipped: 1382
2024-12-28 15:42:44,635 - INFO - Label flipping completed in 0.03s
2024-12-28 15:42:44,635 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:42:44,635 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:42:45,795 - INFO - Feature scaling completed in 1.16s
2024-12-28 15:42:45,795 - INFO - Starting feature selection (k=50)
2024-12-28 15:42:45,820 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:42:45,821 - INFO - Starting anomaly detection
2024-12-28 15:42:53,500 - INFO - Anomaly detection completed in 7.68s
2024-12-28 15:42:53,500 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:42:53,501 - INFO - Total fit_transform time: 8.87s
2024-12-28 15:42:53,501 - INFO - Training set processing completed in 8.87s
2024-12-28 15:42:53,501 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:42:53,502 - INFO - Memory usage at start_fit: CPU 1886.9 MB, GPU 112.5 MB
2024-12-28 15:42:53,502 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:42:54,013 - INFO - Fitted scaler and transformed data
2024-12-28 15:42:54,014 - INFO - Scaling time: 0.51s
2024-12-28 15:42:54,027 - INFO - Number of unique classes: 43
2024-12-28 15:42:59,671 - INFO - Epoch 1/10, Train Loss: 3.5718, Val Loss: 3.7606
2024-12-28 15:43:05,680 - INFO - Epoch 2/10, Train Loss: 3.5712, Val Loss: 3.7601
2024-12-28 15:43:11,423 - INFO - Epoch 3/10, Train Loss: 3.5706, Val Loss: 3.7595
2024-12-28 15:43:17,814 - INFO - Epoch 4/10, Train Loss: 3.5700, Val Loss: 3.7588
2024-12-28 15:43:17,814 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:43:17,814 - INFO - Training completed in 24.31s
2024-12-28 15:43:17,815 - INFO - Final memory usage: CPU 1886.9 MB, GPU 153.6 MB
2024-12-28 15:43:17,815 - INFO - Model training completed in 24.31s
2024-12-28 15:43:18,038 - INFO - Prediction completed in 0.22s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:43:18,050 - INFO - Poison rate 0.07 completed in 33.44s
2024-12-28 15:43:18,050 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:43:18,083 - INFO - Total number of labels flipped: 1975
2024-12-28 15:43:18,083 - INFO - Label flipping completed in 0.03s
2024-12-28 15:43:18,083 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:43:18,084 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:43:19,277 - INFO - Feature scaling completed in 1.19s
2024-12-28 15:43:19,278 - INFO - Starting feature selection (k=50)
2024-12-28 15:43:19,302 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 15:43:19,302 - INFO - Starting anomaly detection
2024-12-28 15:43:25,213 - INFO - Anomaly detection completed in 5.91s
2024-12-28 15:43:25,214 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:43:25,214 - INFO - Total fit_transform time: 7.13s
2024-12-28 15:43:25,214 - INFO - Training set processing completed in 7.13s
2024-12-28 15:43:25,214 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:43:25,216 - INFO - Memory usage at start_fit: CPU 1886.9 MB, GPU 112.5 MB
2024-12-28 15:43:25,216 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:43:25,760 - INFO - Fitted scaler and transformed data
2024-12-28 15:43:25,760 - INFO - Scaling time: 0.54s
2024-12-28 15:43:25,776 - INFO - Number of unique classes: 43
2024-12-28 15:43:33,051 - INFO - Epoch 1/10, Train Loss: 3.5747, Val Loss: 3.7607
2024-12-28 15:43:39,551 - INFO - Epoch 2/10, Train Loss: 3.5742, Val Loss: 3.7602
2024-12-28 15:43:45,971 - INFO - Epoch 3/10, Train Loss: 3.5736, Val Loss: 3.7597
2024-12-28 15:43:51,956 - INFO - Epoch 4/10, Train Loss: 3.5730, Val Loss: 3.7591
2024-12-28 15:43:51,956 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:43:51,956 - INFO - Training completed in 26.74s
2024-12-28 15:43:51,956 - INFO - Final memory usage: CPU 1886.9 MB, GPU 153.6 MB
2024-12-28 15:43:51,957 - INFO - Model training completed in 26.74s
2024-12-28 15:43:52,203 - INFO - Prediction completed in 0.25s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:43:52,215 - INFO - Poison rate 0.1 completed in 34.17s
2024-12-28 15:43:52,215 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:43:52,285 - INFO - Total number of labels flipped: 3951
2024-12-28 15:43:52,286 - INFO - Label flipping completed in 0.07s
2024-12-28 15:43:52,286 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:43:52,286 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:43:53,512 - INFO - Feature scaling completed in 1.23s
2024-12-28 15:43:53,513 - INFO - Starting feature selection (k=50)
2024-12-28 15:43:53,540 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:43:53,540 - INFO - Starting anomaly detection
2024-12-28 15:44:01,017 - INFO - Anomaly detection completed in 7.48s
2024-12-28 15:44:01,017 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:44:01,017 - INFO - Total fit_transform time: 8.73s
2024-12-28 15:44:01,017 - INFO - Training set processing completed in 8.73s
2024-12-28 15:44:01,017 - INFO - Fitting RandomForestWrapper model with data shape: (19755, 512)
2024-12-28 15:44:01,019 - INFO - Memory usage at start_fit: CPU 1886.9 MB, GPU 112.5 MB
2024-12-28 15:44:01,019 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:44:01,624 - INFO - Fitted scaler and transformed data
2024-12-28 15:44:01,624 - INFO - Scaling time: 0.60s
2024-12-28 15:44:01,638 - INFO - Number of unique classes: 43
2024-12-28 15:44:07,732 - INFO - Epoch 1/10, Train Loss: 3.5723, Val Loss: 3.7608
2024-12-28 15:44:13,403 - INFO - Epoch 2/10, Train Loss: 3.5719, Val Loss: 3.7603
2024-12-28 15:44:20,275 - INFO - Epoch 3/10, Train Loss: 3.5714, Val Loss: 3.7599
2024-12-28 15:44:26,442 - INFO - Epoch 4/10, Train Loss: 3.5709, Val Loss: 3.7594
2024-12-28 15:44:26,443 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:44:26,443 - INFO - Training completed in 25.42s
2024-12-28 15:44:26,443 - INFO - Final memory usage: CPU 1886.9 MB, GPU 153.6 MB
2024-12-28 15:44:26,443 - INFO - Model training completed in 25.43s
2024-12-28 15:44:26,654 - INFO - Prediction completed in 0.21s
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
2024-12-28 15:44:26,666 - INFO - Poison rate 0.2 completed in 34.45s
2024-12-28 15:44:26,672 - INFO - Loaded 203 existing results
2024-12-28 15:44:26,673 - INFO - Total results to save: 210
2024-12-28 15:44:26,673 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:44:26,686 - INFO - Saved 210 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:44:26,686 - INFO - Total evaluation time: 270.80s
2024-12-28 15:44:26,692 - INFO - 
Progress: 32.3% - Evaluating GTSRB with KNeighbors (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:44:26,899 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:44:26,899 - INFO - Dataset type: image
2024-12-28 15:44:26,899 - INFO - Sample size: 39209
2024-12-28 15:44:26,899 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:44:26,901 - INFO - Loading datasets...
2024-12-28 15:44:45,734 - INFO - Dataset loading completed in 18.83s
2024-12-28 15:44:45,735 - INFO - Extracting validation features...
2024-12-28 15:44:45,735 - INFO - Extracting features from 4435 samples...
2024-12-28 15:44:46,514 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:44:46,519 - INFO - Validation feature extraction completed in 0.78s
2024-12-28 15:44:46,519 - INFO - Extracting training features...
2024-12-28 15:44:46,519 - INFO - Extracting features from 19755 samples...
2024-12-28 15:44:49,341 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:44:49,352 - INFO - Training feature extraction completed in 2.83s
2024-12-28 15:44:49,352 - INFO - Creating model for classifier: KNeighbors
2024-12-28 15:44:49,353 - INFO - Using device: cuda
2024-12-28 15:44:49,353 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:44:49,353 - INFO - Training set processing completed in 0.00s
2024-12-28 15:44:49,353 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:44:49,354 - INFO - Memory usage at start_fit: CPU 1924.2 MB, GPU 104.0 MB
2024-12-28 15:44:49,354 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:44:49,866 - INFO - Fitted scaler and transformed data
2024-12-28 15:44:49,866 - INFO - Scaling time: 0.51s
2024-12-28 15:44:49,887 - INFO - Training completed in 0.53s
2024-12-28 15:44:49,887 - INFO - Final memory usage: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:49,888 - INFO - Model training completed in 0.53s
2024-12-28 15:44:50,240 - INFO - Prediction completed in 0.35s
2024-12-28 15:44:50,251 - INFO - Poison rate 0.0 completed in 0.90s
2024-12-28 15:44:50,252 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:44:50,257 - INFO - Total number of labels flipped: 197
2024-12-28 15:44:50,257 - INFO - Label flipping completed in 0.01s
2024-12-28 15:44:50,257 - INFO - Training set processing completed in 0.00s
2024-12-28 15:44:50,257 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:44:50,258 - INFO - Memory usage at start_fit: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:50,258 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:44:50,824 - INFO - Fitted scaler and transformed data
2024-12-28 15:44:50,825 - INFO - Scaling time: 0.57s
2024-12-28 15:44:50,837 - INFO - Training completed in 0.58s
2024-12-28 15:44:50,837 - INFO - Final memory usage: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:50,837 - INFO - Model training completed in 0.58s
2024-12-28 15:44:50,927 - INFO - Prediction completed in 0.09s
2024-12-28 15:44:50,938 - INFO - Poison rate 0.01 completed in 0.69s
2024-12-28 15:44:50,939 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:44:50,951 - INFO - Total number of labels flipped: 592
2024-12-28 15:44:50,951 - INFO - Label flipping completed in 0.01s
2024-12-28 15:44:50,951 - INFO - Training set processing completed in 0.00s
2024-12-28 15:44:50,951 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:44:50,952 - INFO - Memory usage at start_fit: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:50,952 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:44:51,488 - INFO - Fitted scaler and transformed data
2024-12-28 15:44:51,488 - INFO - Scaling time: 0.54s
2024-12-28 15:44:51,501 - INFO - Training completed in 0.55s
2024-12-28 15:44:51,501 - INFO - Final memory usage: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:51,501 - INFO - Model training completed in 0.55s
2024-12-28 15:44:51,647 - INFO - Prediction completed in 0.15s
2024-12-28 15:44:51,657 - INFO - Poison rate 0.03 completed in 0.72s
2024-12-28 15:44:51,657 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:44:51,676 - INFO - Total number of labels flipped: 987
2024-12-28 15:44:51,676 - INFO - Label flipping completed in 0.02s
2024-12-28 15:44:51,676 - INFO - Training set processing completed in 0.00s
2024-12-28 15:44:51,676 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:44:51,678 - INFO - Memory usage at start_fit: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:51,678 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:44:52,193 - INFO - Fitted scaler and transformed data
2024-12-28 15:44:52,193 - INFO - Scaling time: 0.51s
2024-12-28 15:44:52,205 - INFO - Training completed in 0.53s
2024-12-28 15:44:52,206 - INFO - Final memory usage: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:52,206 - INFO - Model training completed in 0.53s
2024-12-28 15:44:52,314 - INFO - Prediction completed in 0.11s
2024-12-28 15:44:52,325 - INFO - Poison rate 0.05 completed in 0.67s
2024-12-28 15:44:52,325 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:44:52,352 - INFO - Total number of labels flipped: 1382
2024-12-28 15:44:52,352 - INFO - Label flipping completed in 0.03s
2024-12-28 15:44:52,352 - INFO - Training set processing completed in 0.00s
2024-12-28 15:44:52,352 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:44:52,353 - INFO - Memory usage at start_fit: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:52,353 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:44:52,885 - INFO - Fitted scaler and transformed data
2024-12-28 15:44:52,885 - INFO - Scaling time: 0.53s
2024-12-28 15:44:52,898 - INFO - Training completed in 0.55s
2024-12-28 15:44:52,898 - INFO - Final memory usage: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:52,898 - INFO - Model training completed in 0.55s
2024-12-28 15:44:53,013 - INFO - Prediction completed in 0.11s
2024-12-28 15:44:53,032 - INFO - Poison rate 0.07 completed in 0.71s
2024-12-28 15:44:53,032 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:44:53,076 - INFO - Total number of labels flipped: 1975
2024-12-28 15:44:53,076 - INFO - Label flipping completed in 0.04s
2024-12-28 15:44:53,076 - INFO - Training set processing completed in 0.00s
2024-12-28 15:44:53,076 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:44:53,077 - INFO - Memory usage at start_fit: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:53,077 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:44:53,607 - INFO - Fitted scaler and transformed data
2024-12-28 15:44:53,607 - INFO - Scaling time: 0.53s
2024-12-28 15:44:53,619 - INFO - Training completed in 0.54s
2024-12-28 15:44:53,620 - INFO - Final memory usage: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:53,620 - INFO - Model training completed in 0.54s
2024-12-28 15:44:53,747 - INFO - Prediction completed in 0.13s
2024-12-28 15:44:53,758 - INFO - Poison rate 0.1 completed in 0.73s
2024-12-28 15:44:53,758 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:44:53,829 - INFO - Total number of labels flipped: 3951
2024-12-28 15:44:53,829 - INFO - Label flipping completed in 0.07s
2024-12-28 15:44:53,829 - INFO - Training set processing completed in 0.00s
2024-12-28 15:44:53,829 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:44:53,831 - INFO - Memory usage at start_fit: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:53,831 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:44:54,365 - INFO - Fitted scaler and transformed data
2024-12-28 15:44:54,365 - INFO - Scaling time: 0.53s
2024-12-28 15:44:54,377 - INFO - Training completed in 0.55s
2024-12-28 15:44:54,378 - INFO - Final memory usage: CPU 1924.2 MB, GPU 142.7 MB
2024-12-28 15:44:54,378 - INFO - Model training completed in 0.55s
2024-12-28 15:44:54,513 - INFO - Prediction completed in 0.14s
2024-12-28 15:44:54,524 - INFO - Poison rate 0.2 completed in 0.77s
2024-12-28 15:44:54,531 - INFO - Loaded 210 existing results
2024-12-28 15:44:54,531 - INFO - Total results to save: 217
2024-12-28 15:44:54,532 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:44:54,545 - INFO - Saved 217 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:44:54,546 - INFO - Total evaluation time: 27.65s
2024-12-28 15:44:54,552 - INFO - 
Progress: 33.3% - Evaluating GTSRB with KNeighbors (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:44:54,743 - INFO - Initialized DatasetHandler for GTSRB
2024-12-28 15:44:54,744 - INFO - Dataset type: image
2024-12-28 15:44:54,744 - INFO - Sample size: 39209
2024-12-28 15:44:54,744 - INFO - GTSRB dataset will be downloaded through torchvision if needed
2024-12-28 15:44:54,744 - INFO - Loading datasets...
2024-12-28 15:45:13,495 - INFO - Dataset loading completed in 18.75s
2024-12-28 15:45:13,495 - INFO - Extracting validation features...
2024-12-28 15:45:13,495 - INFO - Extracting features from 4435 samples...
2024-12-28 15:45:14,251 - INFO - Feature extraction completed. Final feature shape: torch.Size([4435, 512])
2024-12-28 15:45:14,256 - INFO - Validation feature extraction completed in 0.76s
2024-12-28 15:45:14,257 - INFO - Extracting training features...
2024-12-28 15:45:14,257 - INFO - Extracting features from 19755 samples...
2024-12-28 15:45:16,984 - INFO - Feature extraction completed. Final feature shape: torch.Size([19755, 512])
2024-12-28 15:45:16,995 - INFO - Training feature extraction completed in 2.74s
2024-12-28 15:45:16,995 - INFO - Creating model for classifier: KNeighbors
2024-12-28 15:45:16,995 - INFO - Using device: cuda
2024-12-28 15:45:16,995 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:45:16,995 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:45:16,995 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:45:18,279 - INFO - Feature scaling completed in 1.28s
2024-12-28 15:45:18,280 - INFO - Starting feature selection (k=50)
2024-12-28 15:45:18,308 - INFO - Feature selection completed in 0.03s. Output shape: (19755, 50)
2024-12-28 15:45:18,308 - INFO - Starting anomaly detection
2024-12-28 15:45:26,353 - INFO - Anomaly detection completed in 8.04s
2024-12-28 15:45:26,353 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:45:26,353 - INFO - Total fit_transform time: 9.36s
2024-12-28 15:45:26,353 - INFO - Training set processing completed in 9.36s
2024-12-28 15:45:26,353 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:45:26,355 - INFO - Memory usage at start_fit: CPU 1926.2 MB, GPU 103.4 MB
2024-12-28 15:45:26,355 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:45:26,869 - INFO - Fitted scaler and transformed data
2024-12-28 15:45:26,869 - INFO - Scaling time: 0.51s
2024-12-28 15:45:26,883 - INFO - Training completed in 0.53s
2024-12-28 15:45:26,883 - INFO - Final memory usage: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:45:26,883 - INFO - Model training completed in 0.53s
2024-12-28 15:45:27,233 - INFO - Prediction completed in 0.35s
2024-12-28 15:45:27,244 - INFO - Poison rate 0.0 completed in 10.25s
2024-12-28 15:45:27,244 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:45:27,250 - INFO - Total number of labels flipped: 197
2024-12-28 15:45:27,250 - INFO - Label flipping completed in 0.01s
2024-12-28 15:45:27,250 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:45:27,250 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:45:28,476 - INFO - Feature scaling completed in 1.23s
2024-12-28 15:45:28,476 - INFO - Starting feature selection (k=50)
2024-12-28 15:45:28,488 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 15:45:28,488 - INFO - Starting anomaly detection
2024-12-28 15:45:36,700 - INFO - Anomaly detection completed in 8.21s
2024-12-28 15:45:36,700 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:45:36,701 - INFO - Total fit_transform time: 9.45s
2024-12-28 15:45:36,701 - INFO - Training set processing completed in 9.45s
2024-12-28 15:45:36,701 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:45:36,702 - INFO - Memory usage at start_fit: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:45:36,702 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:45:37,271 - INFO - Fitted scaler and transformed data
2024-12-28 15:45:37,271 - INFO - Scaling time: 0.57s
2024-12-28 15:45:37,289 - INFO - Training completed in 0.59s
2024-12-28 15:45:37,290 - INFO - Final memory usage: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:45:37,290 - INFO - Model training completed in 0.59s
2024-12-28 15:45:37,630 - INFO - Prediction completed in 0.34s
2024-12-28 15:45:37,641 - INFO - Poison rate 0.01 completed in 10.40s
2024-12-28 15:45:37,641 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:45:37,653 - INFO - Total number of labels flipped: 592
2024-12-28 15:45:37,653 - INFO - Label flipping completed in 0.01s
2024-12-28 15:45:37,653 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:45:37,653 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:45:38,825 - INFO - Feature scaling completed in 1.17s
2024-12-28 15:45:38,825 - INFO - Starting feature selection (k=50)
2024-12-28 15:45:38,837 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 15:45:38,837 - INFO - Starting anomaly detection
2024-12-28 15:45:46,878 - INFO - Anomaly detection completed in 8.04s
2024-12-28 15:45:46,879 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:45:46,879 - INFO - Total fit_transform time: 9.23s
2024-12-28 15:45:46,879 - INFO - Training set processing completed in 9.23s
2024-12-28 15:45:46,879 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:45:46,880 - INFO - Memory usage at start_fit: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:45:46,881 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:45:47,402 - INFO - Fitted scaler and transformed data
2024-12-28 15:45:47,402 - INFO - Scaling time: 0.52s
2024-12-28 15:45:47,420 - INFO - Training completed in 0.54s
2024-12-28 15:45:47,421 - INFO - Final memory usage: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:45:47,421 - INFO - Model training completed in 0.54s
2024-12-28 15:45:47,794 - INFO - Prediction completed in 0.37s
2024-12-28 15:45:47,805 - INFO - Poison rate 0.03 completed in 10.16s
2024-12-28 15:45:47,806 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:45:47,825 - INFO - Total number of labels flipped: 987
2024-12-28 15:45:47,826 - INFO - Label flipping completed in 0.02s
2024-12-28 15:45:47,826 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:45:47,826 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:45:49,014 - INFO - Feature scaling completed in 1.19s
2024-12-28 15:45:49,015 - INFO - Starting feature selection (k=50)
2024-12-28 15:45:49,027 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 15:45:49,027 - INFO - Starting anomaly detection
2024-12-28 15:45:56,155 - INFO - Anomaly detection completed in 7.13s
2024-12-28 15:45:56,155 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:45:56,155 - INFO - Total fit_transform time: 8.33s
2024-12-28 15:45:56,155 - INFO - Training set processing completed in 8.33s
2024-12-28 15:45:56,155 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:45:56,157 - INFO - Memory usage at start_fit: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:45:56,157 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:45:56,715 - INFO - Fitted scaler and transformed data
2024-12-28 15:45:56,715 - INFO - Scaling time: 0.56s
2024-12-28 15:45:56,728 - INFO - Training completed in 0.57s
2024-12-28 15:45:56,728 - INFO - Final memory usage: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:45:56,728 - INFO - Model training completed in 0.57s
2024-12-28 15:45:56,998 - INFO - Prediction completed in 0.27s
2024-12-28 15:45:57,009 - INFO - Poison rate 0.05 completed in 9.20s
2024-12-28 15:45:57,009 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:45:57,034 - INFO - Total number of labels flipped: 1382
2024-12-28 15:45:57,034 - INFO - Label flipping completed in 0.03s
2024-12-28 15:45:57,035 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:45:57,035 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:45:58,249 - INFO - Feature scaling completed in 1.21s
2024-12-28 15:45:58,249 - INFO - Starting feature selection (k=50)
2024-12-28 15:45:58,265 - INFO - Feature selection completed in 0.02s. Output shape: (19755, 50)
2024-12-28 15:45:58,265 - INFO - Starting anomaly detection
2024-12-28 15:46:06,205 - INFO - Anomaly detection completed in 7.94s
2024-12-28 15:46:06,205 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:46:06,205 - INFO - Total fit_transform time: 9.17s
2024-12-28 15:46:06,205 - INFO - Training set processing completed in 9.17s
2024-12-28 15:46:06,206 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:46:06,207 - INFO - Memory usage at start_fit: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:46:06,207 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:46:06,739 - INFO - Fitted scaler and transformed data
2024-12-28 15:46:06,739 - INFO - Scaling time: 0.53s
2024-12-28 15:46:06,758 - INFO - Training completed in 0.55s
2024-12-28 15:46:06,759 - INFO - Final memory usage: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:46:06,759 - INFO - Model training completed in 0.55s
2024-12-28 15:46:07,184 - INFO - Prediction completed in 0.43s
2024-12-28 15:46:07,195 - INFO - Poison rate 0.07 completed in 10.19s
2024-12-28 15:46:07,195 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:46:07,239 - INFO - Total number of labels flipped: 1975
2024-12-28 15:46:07,240 - INFO - Label flipping completed in 0.04s
2024-12-28 15:46:07,240 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:46:07,240 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:46:08,495 - INFO - Feature scaling completed in 1.26s
2024-12-28 15:46:08,495 - INFO - Starting feature selection (k=50)
2024-12-28 15:46:08,507 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 15:46:08,508 - INFO - Starting anomaly detection
2024-12-28 15:46:12,876 - INFO - Anomaly detection completed in 4.37s
2024-12-28 15:46:12,877 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:46:12,877 - INFO - Total fit_transform time: 5.64s
2024-12-28 15:46:12,877 - INFO - Training set processing completed in 5.64s
2024-12-28 15:46:12,877 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:46:12,878 - INFO - Memory usage at start_fit: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:46:12,878 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:46:13,431 - INFO - Fitted scaler and transformed data
2024-12-28 15:46:13,432 - INFO - Scaling time: 0.55s
2024-12-28 15:46:13,444 - INFO - Training completed in 0.57s
2024-12-28 15:46:13,445 - INFO - Final memory usage: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:46:13,445 - INFO - Model training completed in 0.57s
2024-12-28 15:46:13,766 - INFO - Prediction completed in 0.32s
2024-12-28 15:46:13,777 - INFO - Poison rate 0.1 completed in 6.58s
2024-12-28 15:46:13,777 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:46:13,867 - INFO - Total number of labels flipped: 3951
2024-12-28 15:46:13,868 - INFO - Label flipping completed in 0.09s
2024-12-28 15:46:13,868 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:46:13,868 - INFO - Starting feature scaling on shape (19755, 512)
2024-12-28 15:46:15,127 - INFO - Feature scaling completed in 1.26s
2024-12-28 15:46:15,127 - INFO - Starting feature selection (k=50)
2024-12-28 15:46:15,139 - INFO - Feature selection completed in 0.01s. Output shape: (19755, 50)
2024-12-28 15:46:15,139 - INFO - Starting anomaly detection
2024-12-28 15:46:22,299 - INFO - Anomaly detection completed in 7.16s
2024-12-28 15:46:22,299 - INFO - Found 1976 outliers (10.0%)
2024-12-28 15:46:22,299 - INFO - Total fit_transform time: 8.43s
2024-12-28 15:46:22,299 - INFO - Training set processing completed in 8.43s
2024-12-28 15:46:22,299 - INFO - Fitting KNeighborsWrapper model with data shape: (19755, 512)
2024-12-28 15:46:22,300 - INFO - Memory usage at start_fit: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:46:22,300 - INFO - Input data shape: (19755, 512), labels shape: (19755,)
2024-12-28 15:46:22,837 - INFO - Fitted scaler and transformed data
2024-12-28 15:46:22,837 - INFO - Scaling time: 0.54s
2024-12-28 15:46:22,850 - INFO - Training completed in 0.55s
2024-12-28 15:46:22,850 - INFO - Final memory usage: CPU 1926.2 MB, GPU 142.1 MB
2024-12-28 15:46:22,850 - INFO - Model training completed in 0.55s
2024-12-28 15:46:23,185 - INFO - Prediction completed in 0.33s
2024-12-28 15:46:23,196 - INFO - Poison rate 0.2 completed in 9.42s
2024-12-28 15:46:23,211 - INFO - Loaded 217 existing results
2024-12-28 15:46:23,211 - INFO - Total results to save: 224
2024-12-28 15:46:23,213 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:46:23,234 - INFO - Saved 224 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:46:23,235 - INFO - Total evaluation time: 88.49s
2024-12-28 15:46:23,241 - INFO - Completed evaluation for GTSRB
2024-12-28 15:46:23,241 - INFO - 
Processing dataset: ImageNette
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:46:23,403 - INFO - Initialized DatasetHandler for ImageNette
2024-12-28 15:46:23,403 - INFO - Dataset type: image
2024-12-28 15:46:23,403 - INFO - Sample size: 9469
2024-12-28 15:46:23,404 - INFO - 
Progress: 34.4% - Evaluating ImageNette with SVM (standard mode, iteration 1/1)
2024-12-28 15:46:23,566 - INFO - Initialized DatasetHandler for ImageNette
2024-12-28 15:46:23,566 - INFO - Dataset type: image
2024-12-28 15:46:23,566 - INFO - Sample size: 9469
2024-12-28 15:46:23,566 - INFO - Loading datasets...
2024-12-28 15:46:23,590 - INFO - Dataset loading completed in 0.02s
2024-12-28 15:46:23,590 - INFO - Extracting validation features...
2024-12-28 15:46:23,590 - INFO - Extracting features from 1893 samples...
2024-12-28 15:46:28,488 - INFO - Feature extraction completed. Final feature shape: torch.Size([1893, 512])
2024-12-28 15:46:28,491 - INFO - Validation feature extraction completed in 4.90s
2024-12-28 15:46:28,491 - INFO - Extracting training features...
2024-12-28 15:46:28,491 - INFO - Extracting features from 9469 samples...
2024-12-28 15:46:50,048 - INFO - Feature extraction completed. Final feature shape: torch.Size([9469, 512])
2024-12-28 15:46:50,054 - INFO - Training feature extraction completed in 21.56s
2024-12-28 15:46:50,054 - INFO - Creating model for classifier: SVM
2024-12-28 15:46:50,055 - INFO - Using device: cuda
2024-12-28 15:46:50,055 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 15:46:50,055 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:46:50,055 - INFO - Training set processing completed in 0.00s
2024-12-28 15:46:50,055 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:46:50,056 - INFO - Memory usage at start_fit: CPU 2440.7 MB, GPU 104.0 MB
2024-12-28 15:46:50,057 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:46:50,061 - INFO - Number of unique classes: 10
2024-12-28 15:46:50,129 - INFO - Fitted scaler and transformed data
2024-12-28 15:46:50,129 - INFO - Scaling time: 0.07s
2024-12-28 15:46:50,479 - INFO - Epoch 1/500, Train Loss: 0.7737, Val Loss: 0.1079
2024-12-28 15:46:50,795 - INFO - Epoch 2/500, Train Loss: 0.0960, Val Loss: 0.0829
2024-12-28 15:46:51,143 - INFO - Epoch 3/500, Train Loss: 0.0641, Val Loss: 0.0715
2024-12-28 15:46:51,487 - INFO - Epoch 4/500, Train Loss: 0.0458, Val Loss: 0.0653
2024-12-28 15:46:51,814 - INFO - Epoch 5/500, Train Loss: 0.0346, Val Loss: 0.0609
2024-12-28 15:46:52,137 - INFO - Epoch 6/500, Train Loss: 0.0267, Val Loss: 0.0593
2024-12-28 15:46:52,475 - INFO - Epoch 7/500, Train Loss: 0.0215, Val Loss: 0.0566
2024-12-28 15:46:52,795 - INFO - Epoch 8/500, Train Loss: 0.0178, Val Loss: 0.0528
2024-12-28 15:46:53,144 - INFO - Epoch 9/500, Train Loss: 0.0147, Val Loss: 0.0521
2024-12-28 15:46:53,482 - INFO - Epoch 10/500, Train Loss: 0.0124, Val Loss: 0.0516
2024-12-28 15:46:53,808 - INFO - Epoch 11/500, Train Loss: 0.0105, Val Loss: 0.0522
2024-12-28 15:46:54,132 - INFO - Epoch 12/500, Train Loss: 0.0089, Val Loss: 0.0497
2024-12-28 15:46:54,475 - INFO - Epoch 13/500, Train Loss: 0.0078, Val Loss: 0.0493
2024-12-28 15:46:54,797 - INFO - Epoch 14/500, Train Loss: 0.0068, Val Loss: 0.0498
2024-12-28 15:46:55,141 - INFO - Epoch 15/500, Train Loss: 0.0059, Val Loss: 0.0502
2024-12-28 15:46:55,479 - INFO - Epoch 16/500, Train Loss: 0.0055, Val Loss: 0.0490
2024-12-28 15:46:55,857 - INFO - Epoch 17/500, Train Loss: 0.0047, Val Loss: 0.0476
2024-12-28 15:46:56,225 - INFO - Epoch 18/500, Train Loss: 0.0045, Val Loss: 0.0490
2024-12-28 15:46:56,563 - INFO - Epoch 19/500, Train Loss: 0.0042, Val Loss: 0.0459
2024-12-28 15:46:56,884 - INFO - Epoch 20/500, Train Loss: 0.0038, Val Loss: 0.0472
2024-12-28 15:46:57,280 - INFO - Epoch 21/500, Train Loss: 0.0035, Val Loss: 0.0470
2024-12-28 15:46:57,618 - INFO - Epoch 22/500, Train Loss: 0.0036, Val Loss: 0.0491
2024-12-28 15:46:58,056 - INFO - Epoch 23/500, Train Loss: 0.0031, Val Loss: 0.0491
2024-12-28 15:46:58,411 - INFO - Epoch 24/500, Train Loss: 0.0033, Val Loss: 0.0490
2024-12-28 15:46:58,412 - INFO - Early stopping triggered at epoch 24
2024-12-28 15:46:58,412 - INFO - Training completed in 8.36s
2024-12-28 15:46:58,412 - INFO - Final memory usage: CPU 2440.7 MB, GPU 104.2 MB
2024-12-28 15:46:58,412 - INFO - Model training completed in 8.36s
2024-12-28 15:46:58,435 - INFO - Prediction completed in 0.02s
2024-12-28 15:46:58,444 - INFO - Poison rate 0.0 completed in 8.39s
2024-12-28 15:46:58,444 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:46:58,446 - INFO - Total number of labels flipped: 94
2024-12-28 15:46:58,446 - INFO - Label flipping completed in 0.00s
2024-12-28 15:46:58,446 - INFO - Training set processing completed in 0.00s
2024-12-28 15:46:58,446 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:46:58,447 - INFO - Memory usage at start_fit: CPU 2440.7 MB, GPU 104.1 MB
2024-12-28 15:46:58,447 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:46:58,449 - INFO - Number of unique classes: 10
2024-12-28 15:46:58,519 - INFO - Fitted scaler and transformed data
2024-12-28 15:46:58,519 - INFO - Scaling time: 0.07s
2024-12-28 15:46:58,848 - INFO - Epoch 1/500, Train Loss: 0.9052, Val Loss: 0.1863
2024-12-28 15:46:59,202 - INFO - Epoch 2/500, Train Loss: 0.2298, Val Loss: 0.1591
2024-12-28 15:46:59,538 - INFO - Epoch 3/500, Train Loss: 0.1743, Val Loss: 0.1486
2024-12-28 15:46:59,875 - INFO - Epoch 4/500, Train Loss: 0.1394, Val Loss: 0.1478
2024-12-28 15:47:00,269 - INFO - Epoch 5/500, Train Loss: 0.1167, Val Loss: 0.1479
2024-12-28 15:47:00,637 - INFO - Epoch 6/500, Train Loss: 0.0997, Val Loss: 0.1437
2024-12-28 15:47:00,975 - INFO - Epoch 7/500, Train Loss: 0.0877, Val Loss: 0.1438
2024-12-28 15:47:01,355 - INFO - Epoch 8/500, Train Loss: 0.0786, Val Loss: 0.1470
2024-12-28 15:47:01,681 - INFO - Epoch 9/500, Train Loss: 0.0711, Val Loss: 0.1443
2024-12-28 15:47:02,076 - INFO - Epoch 10/500, Train Loss: 0.0663, Val Loss: 0.1485
2024-12-28 15:47:02,406 - INFO - Epoch 11/500, Train Loss: 0.0614, Val Loss: 0.1488
2024-12-28 15:47:02,406 - INFO - Early stopping triggered at epoch 11
2024-12-28 15:47:02,406 - INFO - Training completed in 3.96s
2024-12-28 15:47:02,407 - INFO - Final memory usage: CPU 2440.7 MB, GPU 104.2 MB
2024-12-28 15:47:02,407 - INFO - Model training completed in 3.96s
2024-12-28 15:47:02,447 - INFO - Prediction completed in 0.04s
2024-12-28 15:47:02,455 - INFO - Poison rate 0.01 completed in 4.01s
2024-12-28 15:47:02,455 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:47:02,459 - INFO - Total number of labels flipped: 284
2024-12-28 15:47:02,459 - INFO - Label flipping completed in 0.00s
2024-12-28 15:47:02,459 - INFO - Training set processing completed in 0.00s
2024-12-28 15:47:02,459 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:47:02,461 - INFO - Memory usage at start_fit: CPU 2440.7 MB, GPU 104.1 MB
2024-12-28 15:47:02,461 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:47:02,464 - INFO - Number of unique classes: 10
2024-12-28 15:47:02,535 - INFO - Fitted scaler and transformed data
2024-12-28 15:47:02,535 - INFO - Scaling time: 0.07s
2024-12-28 15:47:02,883 - INFO - Epoch 1/500, Train Loss: 1.2498, Val Loss: 0.3964
2024-12-28 15:47:03,203 - INFO - Epoch 2/500, Train Loss: 0.4620, Val Loss: 0.3559
2024-12-28 15:47:03,561 - INFO - Epoch 3/500, Train Loss: 0.3792, Val Loss: 0.3470
2024-12-28 15:47:03,891 - INFO - Epoch 4/500, Train Loss: 0.3271, Val Loss: 0.3396
2024-12-28 15:47:04,238 - INFO - Epoch 5/500, Train Loss: 0.2927, Val Loss: 0.3410
2024-12-28 15:47:04,556 - INFO - Epoch 6/500, Train Loss: 0.2630, Val Loss: 0.3372
2024-12-28 15:47:04,869 - INFO - Epoch 7/500, Train Loss: 0.2426, Val Loss: 0.3353
2024-12-28 15:47:05,190 - INFO - Epoch 8/500, Train Loss: 0.2249, Val Loss: 0.3396
2024-12-28 15:47:05,506 - INFO - Epoch 9/500, Train Loss: 0.2133, Val Loss: 0.3367
2024-12-28 15:47:05,839 - INFO - Epoch 10/500, Train Loss: 0.2008, Val Loss: 0.3386
2024-12-28 15:47:06,213 - INFO - Epoch 11/500, Train Loss: 0.1920, Val Loss: 0.3445
2024-12-28 15:47:06,565 - INFO - Epoch 12/500, Train Loss: 0.1816, Val Loss: 0.3451
2024-12-28 15:47:06,565 - INFO - Early stopping triggered at epoch 12
2024-12-28 15:47:06,565 - INFO - Training completed in 4.11s
2024-12-28 15:47:06,566 - INFO - Final memory usage: CPU 2440.7 MB, GPU 104.2 MB
2024-12-28 15:47:06,566 - INFO - Model training completed in 4.11s
2024-12-28 15:47:06,591 - INFO - Prediction completed in 0.02s
2024-12-28 15:47:06,599 - INFO - Poison rate 0.03 completed in 4.14s
2024-12-28 15:47:06,600 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:47:06,606 - INFO - Total number of labels flipped: 473
2024-12-28 15:47:06,606 - INFO - Label flipping completed in 0.01s
2024-12-28 15:47:06,606 - INFO - Training set processing completed in 0.00s
2024-12-28 15:47:06,606 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:47:06,607 - INFO - Memory usage at start_fit: CPU 2440.7 MB, GPU 104.1 MB
2024-12-28 15:47:06,607 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:47:06,609 - INFO - Number of unique classes: 10
2024-12-28 15:47:06,670 - INFO - Fitted scaler and transformed data
2024-12-28 15:47:06,670 - INFO - Scaling time: 0.06s
2024-12-28 15:47:07,013 - INFO - Epoch 1/500, Train Loss: 1.4257, Val Loss: 0.8485
2024-12-28 15:47:07,361 - INFO - Epoch 2/500, Train Loss: 0.6762, Val Loss: 0.8287
2024-12-28 15:47:07,736 - INFO - Epoch 3/500, Train Loss: 0.5883, Val Loss: 0.8139
2024-12-28 15:47:08,073 - INFO - Epoch 4/500, Train Loss: 0.5214, Val Loss: 0.7995
2024-12-28 15:47:08,486 - INFO - Epoch 5/500, Train Loss: 0.4695, Val Loss: 0.7973
2024-12-28 15:47:08,881 - INFO - Epoch 6/500, Train Loss: 0.4310, Val Loss: 0.8216
2024-12-28 15:47:09,226 - INFO - Epoch 7/500, Train Loss: 0.4035, Val Loss: 0.8216
2024-12-28 15:47:09,633 - INFO - Epoch 8/500, Train Loss: 0.3773, Val Loss: 0.8485
2024-12-28 15:47:09,993 - INFO - Epoch 9/500, Train Loss: 0.3564, Val Loss: 0.8446
2024-12-28 15:47:10,330 - INFO - Epoch 10/500, Train Loss: 0.3401, Val Loss: 0.8487
2024-12-28 15:47:10,330 - INFO - Early stopping triggered at epoch 10
2024-12-28 15:47:10,330 - INFO - Training completed in 3.72s
2024-12-28 15:47:10,331 - INFO - Final memory usage: CPU 2440.7 MB, GPU 104.2 MB
2024-12-28 15:47:10,331 - INFO - Model training completed in 3.73s
2024-12-28 15:47:10,354 - INFO - Prediction completed in 0.02s
2024-12-28 15:47:10,376 - INFO - Poison rate 0.05 completed in 3.78s
2024-12-28 15:47:10,376 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:47:10,395 - INFO - Total number of labels flipped: 662
2024-12-28 15:47:10,395 - INFO - Label flipping completed in 0.02s
2024-12-28 15:47:10,395 - INFO - Training set processing completed in 0.00s
2024-12-28 15:47:10,395 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:47:10,396 - INFO - Memory usage at start_fit: CPU 2440.7 MB, GPU 104.1 MB
2024-12-28 15:47:10,396 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:47:10,399 - INFO - Number of unique classes: 10
2024-12-28 15:47:10,465 - INFO - Fitted scaler and transformed data
2024-12-28 15:47:10,465 - INFO - Scaling time: 0.06s
2024-12-28 15:47:10,864 - INFO - Epoch 1/500, Train Loss: 1.6312, Val Loss: 0.8721
2024-12-28 15:47:11,257 - INFO - Epoch 2/500, Train Loss: 0.9020, Val Loss: 0.8714
2024-12-28 15:47:11,611 - INFO - Epoch 3/500, Train Loss: 0.7761, Val Loss: 0.8595
2024-12-28 15:47:11,951 - INFO - Epoch 4/500, Train Loss: 0.6915, Val Loss: 0.8424
2024-12-28 15:47:12,310 - INFO - Epoch 5/500, Train Loss: 0.6379, Val Loss: 0.8593
2024-12-28 15:47:12,635 - INFO - Epoch 6/500, Train Loss: 0.5967, Val Loss: 0.8750
2024-12-28 15:47:12,956 - INFO - Epoch 7/500, Train Loss: 0.5646, Val Loss: 0.8672
2024-12-28 15:47:13,306 - INFO - Epoch 8/500, Train Loss: 0.5402, Val Loss: 0.8700
2024-12-28 15:47:13,624 - INFO - Epoch 9/500, Train Loss: 0.5138, Val Loss: 0.8726
2024-12-28 15:47:13,624 - INFO - Early stopping triggered at epoch 9
2024-12-28 15:47:13,624 - INFO - Training completed in 3.23s
2024-12-28 15:47:13,625 - INFO - Final memory usage: CPU 2440.7 MB, GPU 104.2 MB
2024-12-28 15:47:13,625 - INFO - Model training completed in 3.23s
2024-12-28 15:47:13,661 - INFO - Prediction completed in 0.04s
2024-12-28 15:47:13,669 - INFO - Poison rate 0.07 completed in 3.29s
2024-12-28 15:47:13,670 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:47:13,681 - INFO - Total number of labels flipped: 946
2024-12-28 15:47:13,682 - INFO - Label flipping completed in 0.01s
2024-12-28 15:47:13,682 - INFO - Training set processing completed in 0.00s
2024-12-28 15:47:13,682 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:47:13,683 - INFO - Memory usage at start_fit: CPU 2440.7 MB, GPU 104.1 MB
2024-12-28 15:47:13,683 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:47:13,685 - INFO - Number of unique classes: 10
2024-12-28 15:47:13,750 - INFO - Fitted scaler and transformed data
2024-12-28 15:47:13,750 - INFO - Scaling time: 0.06s
2024-12-28 15:47:14,094 - INFO - Epoch 1/500, Train Loss: 2.0909, Val Loss: 1.5041
2024-12-28 15:47:14,444 - INFO - Epoch 2/500, Train Loss: 1.2715, Val Loss: 1.4995
2024-12-28 15:47:14,805 - INFO - Epoch 3/500, Train Loss: 1.1270, Val Loss: 1.5107
2024-12-28 15:47:15,149 - INFO - Epoch 4/500, Train Loss: 1.0266, Val Loss: 1.5397
2024-12-28 15:47:15,488 - INFO - Epoch 5/500, Train Loss: 0.9555, Val Loss: 1.5590
2024-12-28 15:47:15,884 - INFO - Epoch 6/500, Train Loss: 0.9039, Val Loss: 1.5709
2024-12-28 15:47:16,235 - INFO - Epoch 7/500, Train Loss: 0.8541, Val Loss: 1.5895
2024-12-28 15:47:16,236 - INFO - Early stopping triggered at epoch 7
2024-12-28 15:47:16,237 - INFO - Training completed in 2.55s
2024-12-28 15:47:16,237 - INFO - Final memory usage: CPU 2440.7 MB, GPU 104.2 MB
2024-12-28 15:47:16,238 - INFO - Model training completed in 2.56s
2024-12-28 15:47:16,288 - INFO - Prediction completed in 0.05s
2024-12-28 15:47:16,298 - INFO - Poison rate 0.1 completed in 2.63s
2024-12-28 15:47:16,298 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:47:16,321 - INFO - Total number of labels flipped: 1893
2024-12-28 15:47:16,321 - INFO - Label flipping completed in 0.02s
2024-12-28 15:47:16,321 - INFO - Training set processing completed in 0.00s
2024-12-28 15:47:16,321 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:47:16,322 - INFO - Memory usage at start_fit: CPU 2440.7 MB, GPU 104.1 MB
2024-12-28 15:47:16,323 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:47:16,325 - INFO - Number of unique classes: 10
2024-12-28 15:47:16,390 - INFO - Fitted scaler and transformed data
2024-12-28 15:47:16,390 - INFO - Scaling time: 0.06s
2024-12-28 15:47:16,841 - INFO - Epoch 1/500, Train Loss: 3.1659, Val Loss: 2.6723
2024-12-28 15:47:17,195 - INFO - Epoch 2/500, Train Loss: 2.3730, Val Loss: 2.5983
2024-12-28 15:47:17,538 - INFO - Epoch 3/500, Train Loss: 2.1544, Val Loss: 2.6153
2024-12-28 15:47:17,914 - INFO - Epoch 4/500, Train Loss: 2.0145, Val Loss: 2.6109
2024-12-28 15:47:18,313 - INFO - Epoch 5/500, Train Loss: 1.9245, Val Loss: 2.6297
2024-12-28 15:47:18,663 - INFO - Epoch 6/500, Train Loss: 1.8558, Val Loss: 2.6455
2024-12-28 15:47:19,030 - INFO - Epoch 7/500, Train Loss: 1.8008, Val Loss: 2.6563
2024-12-28 15:47:19,030 - INFO - Early stopping triggered at epoch 7
2024-12-28 15:47:19,030 - INFO - Training completed in 2.71s
2024-12-28 15:47:19,030 - INFO - Final memory usage: CPU 2440.7 MB, GPU 104.2 MB
2024-12-28 15:47:19,031 - INFO - Model training completed in 2.71s
2024-12-28 15:47:19,053 - INFO - Prediction completed in 0.02s
2024-12-28 15:47:19,061 - INFO - Poison rate 0.2 completed in 2.76s
2024-12-28 15:47:19,068 - INFO - Loaded 224 existing results
2024-12-28 15:47:19,068 - INFO - Total results to save: 231
2024-12-28 15:47:19,069 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:47:19,082 - INFO - Saved 231 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:47:19,082 - INFO - Total evaluation time: 55.52s
2024-12-28 15:47:19,084 - INFO - 
Progress: 35.4% - Evaluating ImageNette with SVM (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:47:19,271 - INFO - Initialized DatasetHandler for ImageNette
2024-12-28 15:47:19,271 - INFO - Dataset type: image
2024-12-28 15:47:19,271 - INFO - Sample size: 9469
2024-12-28 15:47:19,272 - INFO - Loading datasets...
2024-12-28 15:47:19,295 - INFO - Dataset loading completed in 0.02s
2024-12-28 15:47:19,295 - INFO - Extracting validation features...
2024-12-28 15:47:19,295 - INFO - Extracting features from 1893 samples...
2024-12-28 15:47:23,937 - INFO - Feature extraction completed. Final feature shape: torch.Size([1893, 512])
2024-12-28 15:47:23,944 - INFO - Validation feature extraction completed in 4.65s
2024-12-28 15:47:23,944 - INFO - Extracting training features...
2024-12-28 15:47:23,944 - INFO - Extracting features from 9469 samples...
2024-12-28 15:47:45,480 - INFO - Feature extraction completed. Final feature shape: torch.Size([9469, 512])
2024-12-28 15:47:45,487 - INFO - Training feature extraction completed in 21.54s
2024-12-28 15:47:45,487 - INFO - Creating model for classifier: SVM
2024-12-28 15:47:45,488 - INFO - Using device: cuda
2024-12-28 15:47:45,488 - INFO - Created SVMWrapper instance: SVMWrapper
2024-12-28 15:47:45,488 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:47:45,488 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:47:45,488 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:47:46,131 - INFO - Feature scaling completed in 0.64s
2024-12-28 15:47:46,131 - INFO - Starting feature selection (k=50)
2024-12-28 15:47:46,139 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:47:46,140 - INFO - Starting anomaly detection
2024-12-28 15:47:49,993 - INFO - Anomaly detection completed in 3.85s
2024-12-28 15:47:49,993 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:47:49,993 - INFO - Total fit_transform time: 4.51s
2024-12-28 15:47:49,993 - INFO - Training set processing completed in 4.51s
2024-12-28 15:47:49,993 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:47:49,995 - INFO - Memory usage at start_fit: CPU 2577.8 MB, GPU 103.4 MB
2024-12-28 15:47:49,995 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:47:49,999 - INFO - Number of unique classes: 10
2024-12-28 15:47:50,068 - INFO - Fitted scaler and transformed data
2024-12-28 15:47:50,068 - INFO - Scaling time: 0.07s
2024-12-28 15:47:50,414 - INFO - Epoch 1/500, Train Loss: 0.7939, Val Loss: 0.1642
2024-12-28 15:47:50,773 - INFO - Epoch 2/500, Train Loss: 0.0952, Val Loss: 0.1241
2024-12-28 15:47:51,111 - INFO - Epoch 3/500, Train Loss: 0.0612, Val Loss: 0.1103
2024-12-28 15:47:51,431 - INFO - Epoch 4/500, Train Loss: 0.0438, Val Loss: 0.0997
2024-12-28 15:47:51,753 - INFO - Epoch 5/500, Train Loss: 0.0322, Val Loss: 0.0925
2024-12-28 15:47:52,194 - INFO - Epoch 6/500, Train Loss: 0.0252, Val Loss: 0.0869
2024-12-28 15:47:52,600 - INFO - Epoch 7/500, Train Loss: 0.0204, Val Loss: 0.0859
2024-12-28 15:47:52,976 - INFO - Epoch 8/500, Train Loss: 0.0173, Val Loss: 0.0838
2024-12-28 15:47:53,310 - INFO - Epoch 9/500, Train Loss: 0.0142, Val Loss: 0.0821
2024-12-28 15:47:53,677 - INFO - Epoch 10/500, Train Loss: 0.0122, Val Loss: 0.0786
2024-12-28 15:47:54,041 - INFO - Epoch 11/500, Train Loss: 0.0107, Val Loss: 0.0790
2024-12-28 15:47:54,426 - INFO - Epoch 12/500, Train Loss: 0.0094, Val Loss: 0.0777
2024-12-28 15:47:54,745 - INFO - Epoch 13/500, Train Loss: 0.0080, Val Loss: 0.0782
2024-12-28 15:47:55,079 - INFO - Epoch 14/500, Train Loss: 0.0076, Val Loss: 0.0763
2024-12-28 15:47:55,398 - INFO - Epoch 15/500, Train Loss: 0.0065, Val Loss: 0.0743
2024-12-28 15:47:55,719 - INFO - Epoch 16/500, Train Loss: 0.0057, Val Loss: 0.0755
2024-12-28 15:47:56,057 - INFO - Epoch 17/500, Train Loss: 0.0051, Val Loss: 0.0735
2024-12-28 15:47:56,372 - INFO - Epoch 18/500, Train Loss: 0.0045, Val Loss: 0.0744
2024-12-28 15:47:56,737 - INFO - Epoch 19/500, Train Loss: 0.0042, Val Loss: 0.0763
2024-12-28 15:47:57,060 - INFO - Epoch 20/500, Train Loss: 0.0038, Val Loss: 0.0764
2024-12-28 15:47:57,060 - INFO - Early stopping triggered at epoch 20
2024-12-28 15:47:57,060 - INFO - Training completed in 7.07s
2024-12-28 15:47:57,061 - INFO - Final memory usage: CPU 2587.0 MB, GPU 103.6 MB
2024-12-28 15:47:57,061 - INFO - Model training completed in 7.07s
2024-12-28 15:47:57,096 - INFO - Prediction completed in 0.03s
2024-12-28 15:47:57,110 - INFO - Poison rate 0.0 completed in 11.62s
2024-12-28 15:47:57,110 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:47:57,112 - INFO - Total number of labels flipped: 94
2024-12-28 15:47:57,112 - INFO - Label flipping completed in 0.00s
2024-12-28 15:47:57,112 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:47:57,112 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:47:57,656 - INFO - Feature scaling completed in 0.54s
2024-12-28 15:47:57,656 - INFO - Starting feature selection (k=50)
2024-12-28 15:47:57,670 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:47:57,671 - INFO - Starting anomaly detection
2024-12-28 15:48:01,411 - INFO - Anomaly detection completed in 3.74s
2024-12-28 15:48:01,411 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:48:01,411 - INFO - Total fit_transform time: 4.30s
2024-12-28 15:48:01,411 - INFO - Training set processing completed in 4.30s
2024-12-28 15:48:01,411 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:48:01,412 - INFO - Memory usage at start_fit: CPU 2587.0 MB, GPU 103.5 MB
2024-12-28 15:48:01,412 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:48:01,415 - INFO - Number of unique classes: 10
2024-12-28 15:48:01,477 - INFO - Fitted scaler and transformed data
2024-12-28 15:48:01,477 - INFO - Scaling time: 0.06s
2024-12-28 15:48:01,837 - INFO - Epoch 1/500, Train Loss: 0.8896, Val Loss: 0.3040
2024-12-28 15:48:02,182 - INFO - Epoch 2/500, Train Loss: 0.1937, Val Loss: 0.2722
2024-12-28 15:48:02,531 - INFO - Epoch 3/500, Train Loss: 0.1473, Val Loss: 0.2525
2024-12-28 15:48:02,874 - INFO - Epoch 4/500, Train Loss: 0.1187, Val Loss: 0.2493
2024-12-28 15:48:03,230 - INFO - Epoch 5/500, Train Loss: 0.0982, Val Loss: 0.2422
2024-12-28 15:48:03,616 - INFO - Epoch 6/500, Train Loss: 0.0852, Val Loss: 0.2425
2024-12-28 15:48:03,940 - INFO - Epoch 7/500, Train Loss: 0.0756, Val Loss: 0.2402
2024-12-28 15:48:04,264 - INFO - Epoch 8/500, Train Loss: 0.0675, Val Loss: 0.2403
2024-12-28 15:48:04,633 - INFO - Epoch 9/500, Train Loss: 0.0615, Val Loss: 0.2414
2024-12-28 15:48:05,004 - INFO - Epoch 10/500, Train Loss: 0.0574, Val Loss: 0.2433
2024-12-28 15:48:05,376 - INFO - Epoch 11/500, Train Loss: 0.0541, Val Loss: 0.2452
2024-12-28 15:48:05,783 - INFO - Epoch 12/500, Train Loss: 0.0503, Val Loss: 0.2523
2024-12-28 15:48:05,783 - INFO - Early stopping triggered at epoch 12
2024-12-28 15:48:05,783 - INFO - Training completed in 4.37s
2024-12-28 15:48:05,784 - INFO - Final memory usage: CPU 2587.0 MB, GPU 103.6 MB
2024-12-28 15:48:05,784 - INFO - Model training completed in 4.37s
2024-12-28 15:48:05,812 - INFO - Prediction completed in 0.03s
2024-12-28 15:48:05,822 - INFO - Poison rate 0.01 completed in 8.71s
2024-12-28 15:48:05,822 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:48:05,826 - INFO - Total number of labels flipped: 284
2024-12-28 15:48:05,826 - INFO - Label flipping completed in 0.00s
2024-12-28 15:48:05,826 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:48:05,826 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:48:06,361 - INFO - Feature scaling completed in 0.54s
2024-12-28 15:48:06,362 - INFO - Starting feature selection (k=50)
2024-12-28 15:48:06,375 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:48:06,375 - INFO - Starting anomaly detection
2024-12-28 15:48:09,380 - INFO - Anomaly detection completed in 3.01s
2024-12-28 15:48:09,381 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:48:09,381 - INFO - Total fit_transform time: 3.55s
2024-12-28 15:48:09,381 - INFO - Training set processing completed in 3.56s
2024-12-28 15:48:09,381 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:48:09,382 - INFO - Memory usage at start_fit: CPU 2587.0 MB, GPU 103.5 MB
2024-12-28 15:48:09,383 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:48:09,386 - INFO - Number of unique classes: 10
2024-12-28 15:48:09,453 - INFO - Fitted scaler and transformed data
2024-12-28 15:48:09,453 - INFO - Scaling time: 0.07s
2024-12-28 15:48:09,809 - INFO - Epoch 1/500, Train Loss: 1.1151, Val Loss: 0.4024
2024-12-28 15:48:10,116 - INFO - Epoch 2/500, Train Loss: 0.4372, Val Loss: 0.3773
2024-12-28 15:48:10,478 - INFO - Epoch 3/500, Train Loss: 0.3641, Val Loss: 0.3637
2024-12-28 15:48:10,842 - INFO - Epoch 4/500, Train Loss: 0.3182, Val Loss: 0.3675
2024-12-28 15:48:11,168 - INFO - Epoch 5/500, Train Loss: 0.2803, Val Loss: 0.3664
2024-12-28 15:48:11,505 - INFO - Epoch 6/500, Train Loss: 0.2514, Val Loss: 0.3621
2024-12-28 15:48:11,916 - INFO - Epoch 7/500, Train Loss: 0.2285, Val Loss: 0.3776
2024-12-28 15:48:12,311 - INFO - Epoch 8/500, Train Loss: 0.2114, Val Loss: 0.3846
2024-12-28 15:48:12,747 - INFO - Epoch 9/500, Train Loss: 0.1962, Val Loss: 0.3812
2024-12-28 15:48:13,152 - INFO - Epoch 10/500, Train Loss: 0.1853, Val Loss: 0.3774
2024-12-28 15:48:13,515 - INFO - Epoch 11/500, Train Loss: 0.1766, Val Loss: 0.3928
2024-12-28 15:48:13,515 - INFO - Early stopping triggered at epoch 11
2024-12-28 15:48:13,515 - INFO - Training completed in 4.13s
2024-12-28 15:48:13,515 - INFO - Final memory usage: CPU 2587.0 MB, GPU 103.6 MB
2024-12-28 15:48:13,516 - INFO - Model training completed in 4.13s
2024-12-28 15:48:13,566 - INFO - Prediction completed in 0.05s
2024-12-28 15:48:13,584 - INFO - Poison rate 0.03 completed in 7.76s
2024-12-28 15:48:13,584 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:48:13,591 - INFO - Total number of labels flipped: 473
2024-12-28 15:48:13,591 - INFO - Label flipping completed in 0.01s
2024-12-28 15:48:13,591 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:48:13,591 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:48:14,115 - INFO - Feature scaling completed in 0.52s
2024-12-28 15:48:14,115 - INFO - Starting feature selection (k=50)
2024-12-28 15:48:14,128 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:48:14,128 - INFO - Starting anomaly detection
2024-12-28 15:48:17,104 - INFO - Anomaly detection completed in 2.98s
2024-12-28 15:48:17,104 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:48:17,104 - INFO - Total fit_transform time: 3.51s
2024-12-28 15:48:17,104 - INFO - Training set processing completed in 3.51s
2024-12-28 15:48:17,104 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:48:17,106 - INFO - Memory usage at start_fit: CPU 2587.0 MB, GPU 103.5 MB
2024-12-28 15:48:17,106 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:48:17,109 - INFO - Number of unique classes: 10
2024-12-28 15:48:17,173 - INFO - Fitted scaler and transformed data
2024-12-28 15:48:17,173 - INFO - Scaling time: 0.06s
2024-12-28 15:48:17,545 - INFO - Epoch 1/500, Train Loss: 1.2956, Val Loss: 0.8295
2024-12-28 15:48:17,881 - INFO - Epoch 2/500, Train Loss: 0.6519, Val Loss: 0.8260
2024-12-28 15:48:18,254 - INFO - Epoch 3/500, Train Loss: 0.5623, Val Loss: 0.8247
2024-12-28 15:48:18,660 - INFO - Epoch 4/500, Train Loss: 0.4953, Val Loss: 0.8216
2024-12-28 15:48:19,020 - INFO - Epoch 5/500, Train Loss: 0.4475, Val Loss: 0.8171
2024-12-28 15:48:19,461 - INFO - Epoch 6/500, Train Loss: 0.4114, Val Loss: 0.8194
2024-12-28 15:48:19,889 - INFO - Epoch 7/500, Train Loss: 0.3863, Val Loss: 0.8191
2024-12-28 15:48:20,278 - INFO - Epoch 8/500, Train Loss: 0.3640, Val Loss: 0.8186
2024-12-28 15:48:20,666 - INFO - Epoch 9/500, Train Loss: 0.3418, Val Loss: 0.8293
2024-12-28 15:48:21,068 - INFO - Epoch 10/500, Train Loss: 0.3303, Val Loss: 0.8267
2024-12-28 15:48:21,068 - INFO - Early stopping triggered at epoch 10
2024-12-28 15:48:21,068 - INFO - Training completed in 3.96s
2024-12-28 15:48:21,068 - INFO - Final memory usage: CPU 2587.0 MB, GPU 103.6 MB
2024-12-28 15:48:21,069 - INFO - Model training completed in 3.96s
2024-12-28 15:48:21,090 - INFO - Prediction completed in 0.02s
2024-12-28 15:48:21,099 - INFO - Poison rate 0.05 completed in 7.51s
2024-12-28 15:48:21,099 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:48:21,107 - INFO - Total number of labels flipped: 662
2024-12-28 15:48:21,107 - INFO - Label flipping completed in 0.01s
2024-12-28 15:48:21,107 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:48:21,107 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:48:21,659 - INFO - Feature scaling completed in 0.55s
2024-12-28 15:48:21,659 - INFO - Starting feature selection (k=50)
2024-12-28 15:48:21,673 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:48:21,674 - INFO - Starting anomaly detection
2024-12-28 15:48:25,792 - INFO - Anomaly detection completed in 4.12s
2024-12-28 15:48:25,792 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:48:25,792 - INFO - Total fit_transform time: 4.68s
2024-12-28 15:48:25,792 - INFO - Training set processing completed in 4.69s
2024-12-28 15:48:25,793 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:48:25,794 - INFO - Memory usage at start_fit: CPU 2587.0 MB, GPU 103.5 MB
2024-12-28 15:48:25,794 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:48:25,797 - INFO - Number of unique classes: 10
2024-12-28 15:48:25,861 - INFO - Fitted scaler and transformed data
2024-12-28 15:48:25,861 - INFO - Scaling time: 0.06s
2024-12-28 15:48:26,226 - INFO - Epoch 1/500, Train Loss: 1.5696, Val Loss: 1.2011
2024-12-28 15:48:26,572 - INFO - Epoch 2/500, Train Loss: 0.8632, Val Loss: 1.1547
2024-12-28 15:48:26,911 - INFO - Epoch 3/500, Train Loss: 0.7488, Val Loss: 1.1333
2024-12-28 15:48:27,221 - INFO - Epoch 4/500, Train Loss: 0.6699, Val Loss: 1.1437
2024-12-28 15:48:27,655 - INFO - Epoch 5/500, Train Loss: 0.6105, Val Loss: 1.1283
2024-12-28 15:48:27,999 - INFO - Epoch 6/500, Train Loss: 0.5659, Val Loss: 1.1220
2024-12-28 15:48:28,384 - INFO - Epoch 7/500, Train Loss: 0.5386, Val Loss: 1.1557
2024-12-28 15:48:28,778 - INFO - Epoch 8/500, Train Loss: 0.5061, Val Loss: 1.1534
2024-12-28 15:48:29,208 - INFO - Epoch 9/500, Train Loss: 0.4879, Val Loss: 1.1527
2024-12-28 15:48:29,568 - INFO - Epoch 10/500, Train Loss: 0.4698, Val Loss: 1.1554
2024-12-28 15:48:29,894 - INFO - Epoch 11/500, Train Loss: 0.4500, Val Loss: 1.1621
2024-12-28 15:48:29,894 - INFO - Early stopping triggered at epoch 11
2024-12-28 15:48:29,895 - INFO - Training completed in 4.10s
2024-12-28 15:48:29,895 - INFO - Final memory usage: CPU 2587.0 MB, GPU 103.6 MB
2024-12-28 15:48:29,895 - INFO - Model training completed in 4.10s
2024-12-28 15:48:29,917 - INFO - Prediction completed in 0.02s
2024-12-28 15:48:29,925 - INFO - Poison rate 0.07 completed in 8.83s
2024-12-28 15:48:29,925 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:48:29,936 - INFO - Total number of labels flipped: 946
2024-12-28 15:48:29,937 - INFO - Label flipping completed in 0.01s
2024-12-28 15:48:29,937 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:48:29,937 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:48:30,489 - INFO - Feature scaling completed in 0.55s
2024-12-28 15:48:30,489 - INFO - Starting feature selection (k=50)
2024-12-28 15:48:30,502 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:48:30,503 - INFO - Starting anomaly detection
2024-12-28 15:48:34,819 - INFO - Anomaly detection completed in 4.32s
2024-12-28 15:48:34,819 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:48:34,819 - INFO - Total fit_transform time: 4.88s
2024-12-28 15:48:34,819 - INFO - Training set processing completed in 4.88s
2024-12-28 15:48:34,819 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:48:34,820 - INFO - Memory usage at start_fit: CPU 2587.0 MB, GPU 103.5 MB
2024-12-28 15:48:34,820 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:48:34,823 - INFO - Number of unique classes: 10
2024-12-28 15:48:34,885 - INFO - Fitted scaler and transformed data
2024-12-28 15:48:34,885 - INFO - Scaling time: 0.06s
2024-12-28 15:48:35,271 - INFO - Epoch 1/500, Train Loss: 1.8338, Val Loss: 1.5192
2024-12-28 15:48:35,635 - INFO - Epoch 2/500, Train Loss: 1.1608, Val Loss: 1.5448
2024-12-28 15:48:36,088 - INFO - Epoch 3/500, Train Loss: 1.0308, Val Loss: 1.5467
2024-12-28 15:48:36,442 - INFO - Epoch 4/500, Train Loss: 0.9381, Val Loss: 1.5598
2024-12-28 15:48:36,772 - INFO - Epoch 5/500, Train Loss: 0.8663, Val Loss: 1.5436
2024-12-28 15:48:37,081 - INFO - Epoch 6/500, Train Loss: 0.8214, Val Loss: 1.5897
2024-12-28 15:48:37,081 - INFO - Early stopping triggered at epoch 6
2024-12-28 15:48:37,081 - INFO - Training completed in 2.26s
2024-12-28 15:48:37,081 - INFO - Final memory usage: CPU 2587.0 MB, GPU 103.6 MB
2024-12-28 15:48:37,082 - INFO - Model training completed in 2.26s
2024-12-28 15:48:37,116 - INFO - Prediction completed in 0.03s
2024-12-28 15:48:37,126 - INFO - Poison rate 0.1 completed in 7.20s
2024-12-28 15:48:37,126 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:48:37,148 - INFO - Total number of labels flipped: 1893
2024-12-28 15:48:37,149 - INFO - Label flipping completed in 0.02s
2024-12-28 15:48:37,149 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:48:37,149 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:48:37,717 - INFO - Feature scaling completed in 0.57s
2024-12-28 15:48:37,718 - INFO - Starting feature selection (k=50)
2024-12-28 15:48:37,731 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:48:37,731 - INFO - Starting anomaly detection
2024-12-28 15:48:40,856 - INFO - Anomaly detection completed in 3.12s
2024-12-28 15:48:40,856 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:48:40,856 - INFO - Total fit_transform time: 3.71s
2024-12-28 15:48:40,856 - INFO - Training set processing completed in 3.71s
2024-12-28 15:48:40,856 - INFO - Fitting SVMWrapper model with data shape: (9469, 512)
2024-12-28 15:48:40,857 - INFO - Memory usage at start_fit: CPU 2587.0 MB, GPU 103.5 MB
2024-12-28 15:48:40,858 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:48:40,860 - INFO - Number of unique classes: 10
2024-12-28 15:48:40,925 - INFO - Fitted scaler and transformed data
2024-12-28 15:48:40,926 - INFO - Scaling time: 0.06s
2024-12-28 15:48:41,261 - INFO - Epoch 1/500, Train Loss: 3.0414, Val Loss: 2.8956
2024-12-28 15:48:41,598 - INFO - Epoch 2/500, Train Loss: 2.2155, Val Loss: 2.8183
2024-12-28 15:48:41,934 - INFO - Epoch 3/500, Train Loss: 2.0154, Val Loss: 2.7309
2024-12-28 15:48:42,260 - INFO - Epoch 4/500, Train Loss: 1.8915, Val Loss: 2.7449
2024-12-28 15:48:42,610 - INFO - Epoch 5/500, Train Loss: 1.7999, Val Loss: 2.7497
2024-12-28 15:48:42,932 - INFO - Epoch 6/500, Train Loss: 1.7306, Val Loss: 2.7568
2024-12-28 15:48:43,357 - INFO - Epoch 7/500, Train Loss: 1.6801, Val Loss: 2.7879
2024-12-28 15:48:43,712 - INFO - Epoch 8/500, Train Loss: 1.6323, Val Loss: 2.7576
2024-12-28 15:48:43,712 - INFO - Early stopping triggered at epoch 8
2024-12-28 15:48:43,713 - INFO - Training completed in 2.86s
2024-12-28 15:48:43,713 - INFO - Final memory usage: CPU 2587.0 MB, GPU 103.6 MB
2024-12-28 15:48:43,714 - INFO - Model training completed in 2.86s
2024-12-28 15:48:43,742 - INFO - Prediction completed in 0.03s
2024-12-28 15:48:43,750 - INFO - Poison rate 0.2 completed in 6.62s
2024-12-28 15:48:43,758 - INFO - Loaded 231 existing results
2024-12-28 15:48:43,758 - INFO - Total results to save: 238
2024-12-28 15:48:43,759 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:48:43,772 - INFO - Saved 238 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:48:43,773 - INFO - Total evaluation time: 84.50s
2024-12-28 15:48:43,775 - INFO - 
Progress: 36.5% - Evaluating ImageNette with LogisticRegression (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:48:43,967 - INFO - Initialized DatasetHandler for ImageNette
2024-12-28 15:48:43,967 - INFO - Dataset type: image
2024-12-28 15:48:43,968 - INFO - Sample size: 9469
2024-12-28 15:48:43,968 - INFO - Loading datasets...
2024-12-28 15:48:43,991 - INFO - Dataset loading completed in 0.02s
2024-12-28 15:48:43,991 - INFO - Extracting validation features...
2024-12-28 15:48:43,991 - INFO - Extracting features from 1893 samples...
2024-12-28 15:48:48,933 - INFO - Feature extraction completed. Final feature shape: torch.Size([1893, 512])
2024-12-28 15:48:48,937 - INFO - Validation feature extraction completed in 4.95s
2024-12-28 15:48:48,937 - INFO - Extracting training features...
2024-12-28 15:48:48,937 - INFO - Extracting features from 9469 samples...
2024-12-28 15:49:10,568 - INFO - Feature extraction completed. Final feature shape: torch.Size([9469, 512])
2024-12-28 15:49:10,577 - INFO - Training feature extraction completed in 21.64s
2024-12-28 15:49:10,577 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 15:49:10,577 - INFO - Using device: cuda
2024-12-28 15:49:10,577 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:49:10,578 - INFO - Training set processing completed in 0.00s
2024-12-28 15:49:10,578 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:49:10,579 - INFO - Memory usage at start_fit: CPU 2715.0 MB, GPU 104.0 MB
2024-12-28 15:49:10,580 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:49:10,584 - INFO - Number of unique classes: 10
2024-12-28 15:49:10,656 - INFO - Fitted scaler and transformed data
2024-12-28 15:49:10,656 - INFO - Scaling time: 0.07s
2024-12-28 15:49:10,943 - INFO - Epoch 1/1000, Train Loss: 0.4994, Val Loss: 0.1301
2024-12-28 15:49:11,201 - INFO - Epoch 2/1000, Train Loss: 0.0955, Val Loss: 0.0906
2024-12-28 15:49:11,506 - INFO - Epoch 3/1000, Train Loss: 0.0685, Val Loss: 0.0760
2024-12-28 15:49:11,768 - INFO - Epoch 4/1000, Train Loss: 0.0551, Val Loss: 0.0682
2024-12-28 15:49:12,025 - INFO - Epoch 5/1000, Train Loss: 0.0475, Val Loss: 0.0640
2024-12-28 15:49:12,277 - INFO - Epoch 6/1000, Train Loss: 0.0425, Val Loss: 0.0612
2024-12-28 15:49:12,582 - INFO - Epoch 7/1000, Train Loss: 0.0391, Val Loss: 0.0587
2024-12-28 15:49:12,849 - INFO - Epoch 8/1000, Train Loss: 0.0363, Val Loss: 0.0576
2024-12-28 15:49:13,043 - INFO - Epoch 9/1000, Train Loss: 0.0345, Val Loss: 0.0569
2024-12-28 15:49:13,245 - INFO - Epoch 10/1000, Train Loss: 0.0331, Val Loss: 0.0559
2024-12-28 15:49:13,431 - INFO - Epoch 11/1000, Train Loss: 0.0321, Val Loss: 0.0557
2024-12-28 15:49:13,739 - INFO - Epoch 12/1000, Train Loss: 0.0314, Val Loss: 0.0551
2024-12-28 15:49:14,060 - INFO - Epoch 13/1000, Train Loss: 0.0308, Val Loss: 0.0546
2024-12-28 15:49:14,367 - INFO - Epoch 14/1000, Train Loss: 0.0301, Val Loss: 0.0541
2024-12-28 15:49:14,649 - INFO - Epoch 15/1000, Train Loss: 0.0301, Val Loss: 0.0541
2024-12-28 15:49:15,000 - INFO - Epoch 16/1000, Train Loss: 0.0293, Val Loss: 0.0534
2024-12-28 15:49:15,300 - INFO - Epoch 17/1000, Train Loss: 0.0291, Val Loss: 0.0535
2024-12-28 15:49:15,639 - INFO - Epoch 18/1000, Train Loss: 0.0286, Val Loss: 0.0539
2024-12-28 15:49:15,921 - INFO - Epoch 19/1000, Train Loss: 0.0285, Val Loss: 0.0533
2024-12-28 15:49:16,176 - INFO - Epoch 20/1000, Train Loss: 0.0286, Val Loss: 0.0530
2024-12-28 15:49:16,474 - INFO - Epoch 21/1000, Train Loss: 0.0280, Val Loss: 0.0532
2024-12-28 15:49:16,475 - INFO - Early stopping triggered at epoch 21
2024-12-28 15:49:16,475 - INFO - Training completed in 5.90s
2024-12-28 15:49:16,476 - INFO - Final memory usage: CPU 2715.4 MB, GPU 104.2 MB
2024-12-28 15:49:16,476 - INFO - Model training completed in 5.90s
2024-12-28 15:49:16,536 - INFO - Prediction completed in 0.06s
2024-12-28 15:49:16,546 - INFO - Poison rate 0.0 completed in 5.97s
2024-12-28 15:49:16,546 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:49:16,548 - INFO - Total number of labels flipped: 94
2024-12-28 15:49:16,549 - INFO - Label flipping completed in 0.00s
2024-12-28 15:49:16,549 - INFO - Training set processing completed in 0.00s
2024-12-28 15:49:16,549 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:49:16,550 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 104.1 MB
2024-12-28 15:49:16,550 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:49:16,553 - INFO - Number of unique classes: 10
2024-12-28 15:49:16,628 - INFO - Fitted scaler and transformed data
2024-12-28 15:49:16,628 - INFO - Scaling time: 0.07s
2024-12-28 15:49:16,882 - INFO - Epoch 1/1000, Train Loss: 0.5493, Val Loss: 0.1825
2024-12-28 15:49:17,198 - INFO - Epoch 2/1000, Train Loss: 0.1732, Val Loss: 0.1527
2024-12-28 15:49:17,437 - INFO - Epoch 3/1000, Train Loss: 0.1476, Val Loss: 0.1420
2024-12-28 15:49:17,654 - INFO - Epoch 4/1000, Train Loss: 0.1347, Val Loss: 0.1383
2024-12-28 15:49:17,831 - INFO - Epoch 5/1000, Train Loss: 0.1259, Val Loss: 0.1365
2024-12-28 15:49:18,024 - INFO - Epoch 6/1000, Train Loss: 0.1194, Val Loss: 0.1357
2024-12-28 15:49:18,235 - INFO - Epoch 7/1000, Train Loss: 0.1143, Val Loss: 0.1339
2024-12-28 15:49:18,466 - INFO - Epoch 8/1000, Train Loss: 0.1104, Val Loss: 0.1348
2024-12-28 15:49:18,672 - INFO - Epoch 9/1000, Train Loss: 0.1062, Val Loss: 0.1338
2024-12-28 15:49:18,850 - INFO - Epoch 10/1000, Train Loss: 0.1039, Val Loss: 0.1331
2024-12-28 15:49:19,092 - INFO - Epoch 11/1000, Train Loss: 0.1017, Val Loss: 0.1329
2024-12-28 15:49:19,300 - INFO - Epoch 12/1000, Train Loss: 0.1000, Val Loss: 0.1327
2024-12-28 15:49:19,510 - INFO - Epoch 13/1000, Train Loss: 0.0994, Val Loss: 0.1336
2024-12-28 15:49:19,750 - INFO - Epoch 14/1000, Train Loss: 0.0969, Val Loss: 0.1323
2024-12-28 15:49:19,958 - INFO - Epoch 15/1000, Train Loss: 0.0949, Val Loss: 0.1321
2024-12-28 15:49:20,162 - INFO - Epoch 16/1000, Train Loss: 0.0941, Val Loss: 0.1318
2024-12-28 15:49:20,385 - INFO - Epoch 17/1000, Train Loss: 0.0941, Val Loss: 0.1321
2024-12-28 15:49:20,386 - INFO - Early stopping triggered at epoch 17
2024-12-28 15:49:20,386 - INFO - Training completed in 3.84s
2024-12-28 15:49:20,387 - INFO - Final memory usage: CPU 2715.4 MB, GPU 104.2 MB
2024-12-28 15:49:20,387 - INFO - Model training completed in 3.84s
2024-12-28 15:49:20,443 - INFO - Prediction completed in 0.05s
2024-12-28 15:49:20,455 - INFO - Poison rate 0.01 completed in 3.91s
2024-12-28 15:49:20,455 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:49:20,459 - INFO - Total number of labels flipped: 284
2024-12-28 15:49:20,459 - INFO - Label flipping completed in 0.00s
2024-12-28 15:49:20,459 - INFO - Training set processing completed in 0.00s
2024-12-28 15:49:20,460 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:49:20,460 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 104.1 MB
2024-12-28 15:49:20,461 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:49:20,463 - INFO - Number of unique classes: 10
2024-12-28 15:49:20,530 - INFO - Fitted scaler and transformed data
2024-12-28 15:49:20,530 - INFO - Scaling time: 0.07s
2024-12-28 15:49:20,741 - INFO - Epoch 1/1000, Train Loss: 0.6294, Val Loss: 0.3041
2024-12-28 15:49:20,946 - INFO - Epoch 2/1000, Train Loss: 0.2958, Val Loss: 0.2868
2024-12-28 15:49:21,159 - INFO - Epoch 3/1000, Train Loss: 0.2746, Val Loss: 0.2819
2024-12-28 15:49:21,362 - INFO - Epoch 4/1000, Train Loss: 0.2622, Val Loss: 0.2826
2024-12-28 15:49:21,580 - INFO - Epoch 5/1000, Train Loss: 0.2501, Val Loss: 0.2811
2024-12-28 15:49:21,779 - INFO - Epoch 6/1000, Train Loss: 0.2421, Val Loss: 0.2808
2024-12-28 15:49:21,954 - INFO - Epoch 7/1000, Train Loss: 0.2370, Val Loss: 0.2815
2024-12-28 15:49:22,153 - INFO - Epoch 8/1000, Train Loss: 0.2309, Val Loss: 0.2826
2024-12-28 15:49:22,346 - INFO - Epoch 9/1000, Train Loss: 0.2270, Val Loss: 0.2827
2024-12-28 15:49:22,558 - INFO - Epoch 10/1000, Train Loss: 0.2228, Val Loss: 0.2836
2024-12-28 15:49:22,756 - INFO - Epoch 11/1000, Train Loss: 0.2194, Val Loss: 0.2851
2024-12-28 15:49:22,756 - INFO - Early stopping triggered at epoch 11
2024-12-28 15:49:22,756 - INFO - Training completed in 2.30s
2024-12-28 15:49:22,756 - INFO - Final memory usage: CPU 2715.4 MB, GPU 104.2 MB
2024-12-28 15:49:22,757 - INFO - Model training completed in 2.30s
2024-12-28 15:49:22,812 - INFO - Prediction completed in 0.05s
2024-12-28 15:49:22,820 - INFO - Poison rate 0.03 completed in 2.36s
2024-12-28 15:49:22,820 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:49:22,826 - INFO - Total number of labels flipped: 473
2024-12-28 15:49:22,827 - INFO - Label flipping completed in 0.01s
2024-12-28 15:49:22,827 - INFO - Training set processing completed in 0.00s
2024-12-28 15:49:22,827 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:49:22,827 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 104.1 MB
2024-12-28 15:49:22,828 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:49:22,830 - INFO - Number of unique classes: 10
2024-12-28 15:49:22,892 - INFO - Fitted scaler and transformed data
2024-12-28 15:49:22,892 - INFO - Scaling time: 0.06s
2024-12-28 15:49:23,109 - INFO - Epoch 1/1000, Train Loss: 0.7281, Val Loss: 0.3821
2024-12-28 15:49:23,311 - INFO - Epoch 2/1000, Train Loss: 0.4098, Val Loss: 0.3688
2024-12-28 15:49:23,514 - INFO - Epoch 3/1000, Train Loss: 0.3879, Val Loss: 0.3624
2024-12-28 15:49:23,713 - INFO - Epoch 4/1000, Train Loss: 0.3743, Val Loss: 0.3634
2024-12-28 15:49:23,908 - INFO - Epoch 5/1000, Train Loss: 0.3615, Val Loss: 0.3616
2024-12-28 15:49:24,100 - INFO - Epoch 6/1000, Train Loss: 0.3519, Val Loss: 0.3619
2024-12-28 15:49:24,303 - INFO - Epoch 7/1000, Train Loss: 0.3466, Val Loss: 0.3649
2024-12-28 15:49:24,515 - INFO - Epoch 8/1000, Train Loss: 0.3405, Val Loss: 0.3653
2024-12-28 15:49:24,515 - INFO - Early stopping triggered at epoch 8
2024-12-28 15:49:24,515 - INFO - Training completed in 1.69s
2024-12-28 15:49:24,516 - INFO - Final memory usage: CPU 2715.4 MB, GPU 104.2 MB
2024-12-28 15:49:24,517 - INFO - Model training completed in 1.69s
2024-12-28 15:49:24,563 - INFO - Prediction completed in 0.05s
2024-12-28 15:49:24,571 - INFO - Poison rate 0.05 completed in 1.75s
2024-12-28 15:49:24,571 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:49:24,579 - INFO - Total number of labels flipped: 662
2024-12-28 15:49:24,580 - INFO - Label flipping completed in 0.01s
2024-12-28 15:49:24,580 - INFO - Training set processing completed in 0.00s
2024-12-28 15:49:24,580 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:49:24,581 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 104.1 MB
2024-12-28 15:49:24,581 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:49:24,583 - INFO - Number of unique classes: 10
2024-12-28 15:49:24,648 - INFO - Fitted scaler and transformed data
2024-12-28 15:49:24,648 - INFO - Scaling time: 0.06s
2024-12-28 15:49:24,870 - INFO - Epoch 1/1000, Train Loss: 0.8136, Val Loss: 0.6101
2024-12-28 15:49:25,070 - INFO - Epoch 2/1000, Train Loss: 0.5155, Val Loss: 0.5971
2024-12-28 15:49:25,292 - INFO - Epoch 3/1000, Train Loss: 0.4900, Val Loss: 0.5956
2024-12-28 15:49:25,502 - INFO - Epoch 4/1000, Train Loss: 0.4710, Val Loss: 0.5945
2024-12-28 15:49:25,731 - INFO - Epoch 5/1000, Train Loss: 0.4592, Val Loss: 0.5944
2024-12-28 15:49:25,951 - INFO - Epoch 6/1000, Train Loss: 0.4486, Val Loss: 0.5971
2024-12-28 15:49:26,150 - INFO - Epoch 7/1000, Train Loss: 0.4405, Val Loss: 0.5959
2024-12-28 15:49:26,356 - INFO - Epoch 8/1000, Train Loss: 0.4318, Val Loss: 0.6023
2024-12-28 15:49:26,565 - INFO - Epoch 9/1000, Train Loss: 0.4275, Val Loss: 0.6015
2024-12-28 15:49:26,566 - INFO - Early stopping triggered at epoch 9
2024-12-28 15:49:26,566 - INFO - Training completed in 1.99s
2024-12-28 15:49:26,567 - INFO - Final memory usage: CPU 2715.4 MB, GPU 104.2 MB
2024-12-28 15:49:26,567 - INFO - Model training completed in 1.99s
2024-12-28 15:49:26,606 - INFO - Prediction completed in 0.04s
2024-12-28 15:49:26,615 - INFO - Poison rate 0.07 completed in 2.04s
2024-12-28 15:49:26,615 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:49:26,635 - INFO - Total number of labels flipped: 946
2024-12-28 15:49:26,635 - INFO - Label flipping completed in 0.02s
2024-12-28 15:49:26,635 - INFO - Training set processing completed in 0.00s
2024-12-28 15:49:26,636 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:49:26,637 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 104.1 MB
2024-12-28 15:49:26,637 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:49:26,640 - INFO - Number of unique classes: 10
2024-12-28 15:49:26,714 - INFO - Fitted scaler and transformed data
2024-12-28 15:49:26,715 - INFO - Scaling time: 0.07s
2024-12-28 15:49:26,937 - INFO - Epoch 1/1000, Train Loss: 0.9239, Val Loss: 0.6527
2024-12-28 15:49:27,127 - INFO - Epoch 2/1000, Train Loss: 0.6650, Val Loss: 0.6434
2024-12-28 15:49:27,331 - INFO - Epoch 3/1000, Train Loss: 0.6343, Val Loss: 0.6442
2024-12-28 15:49:27,534 - INFO - Epoch 4/1000, Train Loss: 0.6162, Val Loss: 0.6409
2024-12-28 15:49:27,739 - INFO - Epoch 5/1000, Train Loss: 0.6003, Val Loss: 0.6429
2024-12-28 15:49:27,979 - INFO - Epoch 6/1000, Train Loss: 0.5907, Val Loss: 0.6447
2024-12-28 15:49:28,323 - INFO - Epoch 7/1000, Train Loss: 0.5799, Val Loss: 0.6480
2024-12-28 15:49:28,722 - INFO - Epoch 8/1000, Train Loss: 0.5721, Val Loss: 0.6468
2024-12-28 15:49:29,130 - INFO - Epoch 9/1000, Train Loss: 0.5671, Val Loss: 0.6479
2024-12-28 15:49:29,131 - INFO - Early stopping triggered at epoch 9
2024-12-28 15:49:29,131 - INFO - Training completed in 2.49s
2024-12-28 15:49:29,131 - INFO - Final memory usage: CPU 2715.4 MB, GPU 104.2 MB
2024-12-28 15:49:29,131 - INFO - Model training completed in 2.50s
2024-12-28 15:49:29,154 - INFO - Prediction completed in 0.02s
2024-12-28 15:49:29,162 - INFO - Poison rate 0.1 completed in 2.55s
2024-12-28 15:49:29,162 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:49:29,185 - INFO - Total number of labels flipped: 1893
2024-12-28 15:49:29,185 - INFO - Label flipping completed in 0.02s
2024-12-28 15:49:29,185 - INFO - Training set processing completed in 0.00s
2024-12-28 15:49:29,185 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:49:29,186 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 104.1 MB
2024-12-28 15:49:29,186 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:49:29,189 - INFO - Number of unique classes: 10
2024-12-28 15:49:29,266 - INFO - Fitted scaler and transformed data
2024-12-28 15:49:29,266 - INFO - Scaling time: 0.08s
2024-12-28 15:49:29,653 - INFO - Epoch 1/1000, Train Loss: 1.2593, Val Loss: 1.1231
2024-12-28 15:49:30,065 - INFO - Epoch 2/1000, Train Loss: 1.0466, Val Loss: 1.1122
2024-12-28 15:49:30,482 - INFO - Epoch 3/1000, Train Loss: 1.0147, Val Loss: 1.1054
2024-12-28 15:49:30,893 - INFO - Epoch 4/1000, Train Loss: 0.9909, Val Loss: 1.1159
2024-12-28 15:49:31,309 - INFO - Epoch 5/1000, Train Loss: 0.9741, Val Loss: 1.1177
2024-12-28 15:49:31,726 - INFO - Epoch 6/1000, Train Loss: 0.9614, Val Loss: 1.1218
2024-12-28 15:49:32,125 - INFO - Epoch 7/1000, Train Loss: 0.9524, Val Loss: 1.1243
2024-12-28 15:49:32,547 - INFO - Epoch 8/1000, Train Loss: 0.9434, Val Loss: 1.1327
2024-12-28 15:49:32,547 - INFO - Early stopping triggered at epoch 8
2024-12-28 15:49:32,547 - INFO - Training completed in 3.36s
2024-12-28 15:49:32,548 - INFO - Final memory usage: CPU 2715.4 MB, GPU 104.2 MB
2024-12-28 15:49:32,548 - INFO - Model training completed in 3.36s
2024-12-28 15:49:32,580 - INFO - Prediction completed in 0.03s
2024-12-28 15:49:32,590 - INFO - Poison rate 0.2 completed in 3.43s
2024-12-28 15:49:32,599 - INFO - Loaded 238 existing results
2024-12-28 15:49:32,599 - INFO - Total results to save: 245
2024-12-28 15:49:32,600 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:49:32,614 - INFO - Saved 245 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:49:32,614 - INFO - Total evaluation time: 48.65s
2024-12-28 15:49:32,616 - INFO - 
Progress: 37.5% - Evaluating ImageNette with LogisticRegression (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:49:32,793 - INFO - Initialized DatasetHandler for ImageNette
2024-12-28 15:49:32,793 - INFO - Dataset type: image
2024-12-28 15:49:32,793 - INFO - Sample size: 9469
2024-12-28 15:49:32,794 - INFO - Loading datasets...
2024-12-28 15:49:32,817 - INFO - Dataset loading completed in 0.02s
2024-12-28 15:49:32,817 - INFO - Extracting validation features...
2024-12-28 15:49:32,817 - INFO - Extracting features from 1893 samples...
2024-12-28 15:49:37,398 - INFO - Feature extraction completed. Final feature shape: torch.Size([1893, 512])
2024-12-28 15:49:37,401 - INFO - Validation feature extraction completed in 4.58s
2024-12-28 15:49:37,402 - INFO - Extracting training features...
2024-12-28 15:49:37,402 - INFO - Extracting features from 9469 samples...
2024-12-28 15:49:58,340 - INFO - Feature extraction completed. Final feature shape: torch.Size([9469, 512])
2024-12-28 15:49:58,347 - INFO - Training feature extraction completed in 20.95s
2024-12-28 15:49:58,347 - INFO - Creating model for classifier: LogisticRegression
2024-12-28 15:49:58,347 - INFO - Using device: cuda
2024-12-28 15:49:58,347 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:49:58,347 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:49:58,348 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:49:58,894 - INFO - Feature scaling completed in 0.55s
2024-12-28 15:49:58,895 - INFO - Starting feature selection (k=50)
2024-12-28 15:49:58,903 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:49:58,903 - INFO - Starting anomaly detection
2024-12-28 15:50:02,124 - INFO - Anomaly detection completed in 3.22s
2024-12-28 15:50:02,124 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:50:02,124 - INFO - Total fit_transform time: 3.78s
2024-12-28 15:50:02,124 - INFO - Training set processing completed in 3.78s
2024-12-28 15:50:02,124 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:50:02,125 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 103.4 MB
2024-12-28 15:50:02,125 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:50:02,127 - INFO - Number of unique classes: 10
2024-12-28 15:50:02,190 - INFO - Fitted scaler and transformed data
2024-12-28 15:50:02,191 - INFO - Scaling time: 0.06s
2024-12-28 15:50:02,394 - INFO - Epoch 1/1000, Train Loss: 0.5147, Val Loss: 0.1298
2024-12-28 15:50:02,587 - INFO - Epoch 2/1000, Train Loss: 0.0961, Val Loss: 0.0917
2024-12-28 15:50:02,771 - INFO - Epoch 3/1000, Train Loss: 0.0685, Val Loss: 0.0777
2024-12-28 15:50:02,978 - INFO - Epoch 4/1000, Train Loss: 0.0554, Val Loss: 0.0714
2024-12-28 15:50:03,224 - INFO - Epoch 5/1000, Train Loss: 0.0475, Val Loss: 0.0670
2024-12-28 15:50:03,423 - INFO - Epoch 6/1000, Train Loss: 0.0421, Val Loss: 0.0646
2024-12-28 15:50:03,614 - INFO - Epoch 7/1000, Train Loss: 0.0387, Val Loss: 0.0630
2024-12-28 15:50:03,801 - INFO - Epoch 8/1000, Train Loss: 0.0363, Val Loss: 0.0629
2024-12-28 15:50:04,007 - INFO - Epoch 9/1000, Train Loss: 0.0344, Val Loss: 0.0614
2024-12-28 15:50:04,249 - INFO - Epoch 10/1000, Train Loss: 0.0330, Val Loss: 0.0610
2024-12-28 15:50:04,458 - INFO - Epoch 11/1000, Train Loss: 0.0318, Val Loss: 0.0602
2024-12-28 15:50:04,684 - INFO - Epoch 12/1000, Train Loss: 0.0310, Val Loss: 0.0599
2024-12-28 15:50:04,885 - INFO - Epoch 13/1000, Train Loss: 0.0305, Val Loss: 0.0594
2024-12-28 15:50:05,135 - INFO - Epoch 14/1000, Train Loss: 0.0300, Val Loss: 0.0594
2024-12-28 15:50:05,340 - INFO - Epoch 15/1000, Train Loss: 0.0293, Val Loss: 0.0594
2024-12-28 15:50:05,549 - INFO - Epoch 16/1000, Train Loss: 0.0292, Val Loss: 0.0596
2024-12-28 15:50:05,550 - INFO - Early stopping triggered at epoch 16
2024-12-28 15:50:05,550 - INFO - Training completed in 3.43s
2024-12-28 15:50:05,551 - INFO - Final memory usage: CPU 2715.4 MB, GPU 103.6 MB
2024-12-28 15:50:05,551 - INFO - Model training completed in 3.43s
2024-12-28 15:50:05,602 - INFO - Prediction completed in 0.05s
2024-12-28 15:50:05,610 - INFO - Poison rate 0.0 completed in 7.26s
2024-12-28 15:50:05,610 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:50:05,612 - INFO - Total number of labels flipped: 94
2024-12-28 15:50:05,613 - INFO - Label flipping completed in 0.00s
2024-12-28 15:50:05,613 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:50:05,613 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:50:06,191 - INFO - Feature scaling completed in 0.58s
2024-12-28 15:50:06,191 - INFO - Starting feature selection (k=50)
2024-12-28 15:50:06,205 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:50:06,205 - INFO - Starting anomaly detection
2024-12-28 15:50:09,522 - INFO - Anomaly detection completed in 3.32s
2024-12-28 15:50:09,522 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:50:09,522 - INFO - Total fit_transform time: 3.91s
2024-12-28 15:50:09,522 - INFO - Training set processing completed in 3.91s
2024-12-28 15:50:09,522 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:50:09,523 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 103.5 MB
2024-12-28 15:50:09,523 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:50:09,525 - INFO - Number of unique classes: 10
2024-12-28 15:50:09,589 - INFO - Fitted scaler and transformed data
2024-12-28 15:50:09,589 - INFO - Scaling time: 0.06s
2024-12-28 15:50:09,795 - INFO - Epoch 1/1000, Train Loss: 0.5355, Val Loss: 0.1534
2024-12-28 15:50:09,994 - INFO - Epoch 2/1000, Train Loss: 0.1690, Val Loss: 0.1222
2024-12-28 15:50:10,245 - INFO - Epoch 3/1000, Train Loss: 0.1455, Val Loss: 0.1110
2024-12-28 15:50:10,432 - INFO - Epoch 4/1000, Train Loss: 0.1333, Val Loss: 0.1077
2024-12-28 15:50:10,625 - INFO - Epoch 5/1000, Train Loss: 0.1262, Val Loss: 0.1053
2024-12-28 15:50:10,829 - INFO - Epoch 6/1000, Train Loss: 0.1193, Val Loss: 0.1047
2024-12-28 15:50:11,011 - INFO - Epoch 7/1000, Train Loss: 0.1140, Val Loss: 0.1044
2024-12-28 15:50:11,243 - INFO - Epoch 8/1000, Train Loss: 0.1107, Val Loss: 0.1057
2024-12-28 15:50:11,436 - INFO - Epoch 9/1000, Train Loss: 0.1075, Val Loss: 0.1043
2024-12-28 15:50:11,621 - INFO - Epoch 10/1000, Train Loss: 0.1049, Val Loss: 0.1054
2024-12-28 15:50:11,621 - INFO - Early stopping triggered at epoch 10
2024-12-28 15:50:11,621 - INFO - Training completed in 2.10s
2024-12-28 15:50:11,621 - INFO - Final memory usage: CPU 2715.4 MB, GPU 103.6 MB
2024-12-28 15:50:11,622 - INFO - Model training completed in 2.10s
2024-12-28 15:50:11,675 - INFO - Prediction completed in 0.05s
2024-12-28 15:50:11,684 - INFO - Poison rate 0.01 completed in 6.07s
2024-12-28 15:50:11,684 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:50:11,688 - INFO - Total number of labels flipped: 284
2024-12-28 15:50:11,689 - INFO - Label flipping completed in 0.00s
2024-12-28 15:50:11,689 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:50:11,689 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:50:12,291 - INFO - Feature scaling completed in 0.60s
2024-12-28 15:50:12,291 - INFO - Starting feature selection (k=50)
2024-12-28 15:50:12,304 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:50:12,304 - INFO - Starting anomaly detection
2024-12-28 15:50:14,730 - INFO - Anomaly detection completed in 2.43s
2024-12-28 15:50:14,730 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:50:14,731 - INFO - Total fit_transform time: 3.04s
2024-12-28 15:50:14,731 - INFO - Training set processing completed in 3.04s
2024-12-28 15:50:14,731 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:50:14,732 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 103.5 MB
2024-12-28 15:50:14,733 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:50:14,735 - INFO - Number of unique classes: 10
2024-12-28 15:50:14,801 - INFO - Fitted scaler and transformed data
2024-12-28 15:50:14,801 - INFO - Scaling time: 0.06s
2024-12-28 15:50:15,019 - INFO - Epoch 1/1000, Train Loss: 0.6126, Val Loss: 0.3478
2024-12-28 15:50:15,209 - INFO - Epoch 2/1000, Train Loss: 0.2960, Val Loss: 0.3322
2024-12-28 15:50:15,400 - INFO - Epoch 3/1000, Train Loss: 0.2750, Val Loss: 0.3280
2024-12-28 15:50:15,600 - INFO - Epoch 4/1000, Train Loss: 0.2618, Val Loss: 0.3254
2024-12-28 15:50:15,783 - INFO - Epoch 5/1000, Train Loss: 0.2502, Val Loss: 0.3231
2024-12-28 15:50:15,992 - INFO - Epoch 6/1000, Train Loss: 0.2417, Val Loss: 0.3268
2024-12-28 15:50:16,192 - INFO - Epoch 7/1000, Train Loss: 0.2348, Val Loss: 0.3280
2024-12-28 15:50:16,402 - INFO - Epoch 8/1000, Train Loss: 0.2294, Val Loss: 0.3277
2024-12-28 15:50:16,619 - INFO - Epoch 9/1000, Train Loss: 0.2251, Val Loss: 0.3288
2024-12-28 15:50:16,814 - INFO - Epoch 10/1000, Train Loss: 0.2218, Val Loss: 0.3283
2024-12-28 15:50:16,814 - INFO - Early stopping triggered at epoch 10
2024-12-28 15:50:16,814 - INFO - Training completed in 2.08s
2024-12-28 15:50:16,815 - INFO - Final memory usage: CPU 2715.4 MB, GPU 103.6 MB
2024-12-28 15:50:16,816 - INFO - Model training completed in 2.08s
2024-12-28 15:50:16,860 - INFO - Prediction completed in 0.04s
2024-12-28 15:50:16,876 - INFO - Poison rate 0.03 completed in 5.19s
2024-12-28 15:50:16,876 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:50:16,883 - INFO - Total number of labels flipped: 473
2024-12-28 15:50:16,883 - INFO - Label flipping completed in 0.01s
2024-12-28 15:50:16,883 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:50:16,883 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:50:17,460 - INFO - Feature scaling completed in 0.58s
2024-12-28 15:50:17,460 - INFO - Starting feature selection (k=50)
2024-12-28 15:50:17,468 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:50:17,469 - INFO - Starting anomaly detection
2024-12-28 15:50:21,101 - INFO - Anomaly detection completed in 3.63s
2024-12-28 15:50:21,102 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:50:21,102 - INFO - Total fit_transform time: 4.22s
2024-12-28 15:50:21,102 - INFO - Training set processing completed in 4.22s
2024-12-28 15:50:21,102 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:50:21,103 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 103.5 MB
2024-12-28 15:50:21,103 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:50:21,106 - INFO - Number of unique classes: 10
2024-12-28 15:50:21,170 - INFO - Fitted scaler and transformed data
2024-12-28 15:50:21,170 - INFO - Scaling time: 0.06s
2024-12-28 15:50:21,386 - INFO - Epoch 1/1000, Train Loss: 0.7070, Val Loss: 0.4885
2024-12-28 15:50:21,622 - INFO - Epoch 2/1000, Train Loss: 0.4072, Val Loss: 0.4822
2024-12-28 15:50:21,818 - INFO - Epoch 3/1000, Train Loss: 0.3828, Val Loss: 0.4805
2024-12-28 15:50:22,044 - INFO - Epoch 4/1000, Train Loss: 0.3665, Val Loss: 0.4816
2024-12-28 15:50:22,244 - INFO - Epoch 5/1000, Train Loss: 0.3563, Val Loss: 0.4861
2024-12-28 15:50:22,442 - INFO - Epoch 6/1000, Train Loss: 0.3497, Val Loss: 0.4851
2024-12-28 15:50:22,653 - INFO - Epoch 7/1000, Train Loss: 0.3417, Val Loss: 0.4896
2024-12-28 15:50:22,863 - INFO - Epoch 8/1000, Train Loss: 0.3343, Val Loss: 0.4921
2024-12-28 15:50:22,864 - INFO - Early stopping triggered at epoch 8
2024-12-28 15:50:22,864 - INFO - Training completed in 1.76s
2024-12-28 15:50:22,865 - INFO - Final memory usage: CPU 2715.4 MB, GPU 103.6 MB
2024-12-28 15:50:22,865 - INFO - Model training completed in 1.76s
2024-12-28 15:50:22,907 - INFO - Prediction completed in 0.04s
2024-12-28 15:50:22,916 - INFO - Poison rate 0.05 completed in 6.04s
2024-12-28 15:50:22,916 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:50:22,924 - INFO - Total number of labels flipped: 662
2024-12-28 15:50:22,924 - INFO - Label flipping completed in 0.01s
2024-12-28 15:50:22,925 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:50:22,925 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:50:23,475 - INFO - Feature scaling completed in 0.55s
2024-12-28 15:50:23,475 - INFO - Starting feature selection (k=50)
2024-12-28 15:50:23,483 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:50:23,484 - INFO - Starting anomaly detection
2024-12-28 15:50:25,907 - INFO - Anomaly detection completed in 2.42s
2024-12-28 15:50:25,907 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:50:25,907 - INFO - Total fit_transform time: 2.98s
2024-12-28 15:50:25,907 - INFO - Training set processing completed in 2.98s
2024-12-28 15:50:25,907 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:50:25,908 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 103.5 MB
2024-12-28 15:50:25,909 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:50:25,911 - INFO - Number of unique classes: 10
2024-12-28 15:50:25,990 - INFO - Fitted scaler and transformed data
2024-12-28 15:50:25,990 - INFO - Scaling time: 0.08s
2024-12-28 15:50:26,189 - INFO - Epoch 1/1000, Train Loss: 0.8089, Val Loss: 0.5615
2024-12-28 15:50:26,386 - INFO - Epoch 2/1000, Train Loss: 0.5144, Val Loss: 0.5486
2024-12-28 15:50:26,593 - INFO - Epoch 3/1000, Train Loss: 0.4874, Val Loss: 0.5467
2024-12-28 15:50:26,813 - INFO - Epoch 4/1000, Train Loss: 0.4717, Val Loss: 0.5484
2024-12-28 15:50:27,019 - INFO - Epoch 5/1000, Train Loss: 0.4569, Val Loss: 0.5530
2024-12-28 15:50:27,227 - INFO - Epoch 6/1000, Train Loss: 0.4477, Val Loss: 0.5587
2024-12-28 15:50:27,426 - INFO - Epoch 7/1000, Train Loss: 0.4376, Val Loss: 0.5598
2024-12-28 15:50:27,635 - INFO - Epoch 8/1000, Train Loss: 0.4325, Val Loss: 0.5665
2024-12-28 15:50:27,636 - INFO - Early stopping triggered at epoch 8
2024-12-28 15:50:27,636 - INFO - Training completed in 1.73s
2024-12-28 15:50:27,636 - INFO - Final memory usage: CPU 2715.4 MB, GPU 103.6 MB
2024-12-28 15:50:27,636 - INFO - Model training completed in 1.73s
2024-12-28 15:50:27,663 - INFO - Prediction completed in 0.03s
2024-12-28 15:50:27,671 - INFO - Poison rate 0.07 completed in 4.76s
2024-12-28 15:50:27,671 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:50:27,683 - INFO - Total number of labels flipped: 946
2024-12-28 15:50:27,683 - INFO - Label flipping completed in 0.01s
2024-12-28 15:50:27,683 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:50:27,683 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:50:28,249 - INFO - Feature scaling completed in 0.57s
2024-12-28 15:50:28,249 - INFO - Starting feature selection (k=50)
2024-12-28 15:50:28,260 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:50:28,261 - INFO - Starting anomaly detection
2024-12-28 15:50:31,853 - INFO - Anomaly detection completed in 3.59s
2024-12-28 15:50:31,853 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:50:31,853 - INFO - Total fit_transform time: 4.17s
2024-12-28 15:50:31,853 - INFO - Training set processing completed in 4.17s
2024-12-28 15:50:31,853 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:50:31,854 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 103.5 MB
2024-12-28 15:50:31,855 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:50:31,858 - INFO - Number of unique classes: 10
2024-12-28 15:50:31,922 - INFO - Fitted scaler and transformed data
2024-12-28 15:50:31,922 - INFO - Scaling time: 0.06s
2024-12-28 15:50:32,126 - INFO - Epoch 1/1000, Train Loss: 0.9168, Val Loss: 0.6668
2024-12-28 15:50:32,316 - INFO - Epoch 2/1000, Train Loss: 0.6558, Val Loss: 0.6512
2024-12-28 15:50:32,518 - INFO - Epoch 3/1000, Train Loss: 0.6302, Val Loss: 0.6479
2024-12-28 15:50:32,734 - INFO - Epoch 4/1000, Train Loss: 0.6123, Val Loss: 0.6467
2024-12-28 15:50:32,928 - INFO - Epoch 5/1000, Train Loss: 0.6010, Val Loss: 0.6482
2024-12-28 15:50:33,126 - INFO - Epoch 6/1000, Train Loss: 0.5875, Val Loss: 0.6507
2024-12-28 15:50:33,340 - INFO - Epoch 7/1000, Train Loss: 0.5779, Val Loss: 0.6549
2024-12-28 15:50:33,533 - INFO - Epoch 8/1000, Train Loss: 0.5715, Val Loss: 0.6502
2024-12-28 15:50:33,726 - INFO - Epoch 9/1000, Train Loss: 0.5650, Val Loss: 0.6520
2024-12-28 15:50:33,726 - INFO - Early stopping triggered at epoch 9
2024-12-28 15:50:33,726 - INFO - Training completed in 1.87s
2024-12-28 15:50:33,727 - INFO - Final memory usage: CPU 2715.4 MB, GPU 103.6 MB
2024-12-28 15:50:33,727 - INFO - Model training completed in 1.87s
2024-12-28 15:50:33,784 - INFO - Prediction completed in 0.06s
2024-12-28 15:50:33,793 - INFO - Poison rate 0.1 completed in 6.12s
2024-12-28 15:50:33,793 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:50:33,816 - INFO - Total number of labels flipped: 1893
2024-12-28 15:50:33,816 - INFO - Label flipping completed in 0.02s
2024-12-28 15:50:33,816 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:50:33,816 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:50:34,368 - INFO - Feature scaling completed in 0.55s
2024-12-28 15:50:34,368 - INFO - Starting feature selection (k=50)
2024-12-28 15:50:34,381 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:50:34,382 - INFO - Starting anomaly detection
2024-12-28 15:50:38,648 - INFO - Anomaly detection completed in 4.27s
2024-12-28 15:50:38,648 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:50:38,648 - INFO - Total fit_transform time: 4.83s
2024-12-28 15:50:38,648 - INFO - Training set processing completed in 4.83s
2024-12-28 15:50:38,648 - INFO - Fitting LogisticRegressionWrapper model with data shape: (9469, 512)
2024-12-28 15:50:38,649 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 103.5 MB
2024-12-28 15:50:38,650 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:50:38,652 - INFO - Number of unique classes: 10
2024-12-28 15:50:38,713 - INFO - Fitted scaler and transformed data
2024-12-28 15:50:38,713 - INFO - Scaling time: 0.06s
2024-12-28 15:50:38,963 - INFO - Epoch 1/1000, Train Loss: 1.2450, Val Loss: 0.9999
2024-12-28 15:50:39,184 - INFO - Epoch 2/1000, Train Loss: 1.0579, Val Loss: 0.9982
2024-12-28 15:50:39,411 - INFO - Epoch 3/1000, Train Loss: 1.0231, Val Loss: 1.0049
2024-12-28 15:50:39,623 - INFO - Epoch 4/1000, Train Loss: 1.0040, Val Loss: 1.0002
2024-12-28 15:50:39,823 - INFO - Epoch 5/1000, Train Loss: 0.9876, Val Loss: 1.0049
2024-12-28 15:50:40,023 - INFO - Epoch 6/1000, Train Loss: 0.9735, Val Loss: 1.0159
2024-12-28 15:50:40,224 - INFO - Epoch 7/1000, Train Loss: 0.9631, Val Loss: 1.0210
2024-12-28 15:50:40,224 - INFO - Early stopping triggered at epoch 7
2024-12-28 15:50:40,224 - INFO - Training completed in 1.58s
2024-12-28 15:50:40,225 - INFO - Final memory usage: CPU 2715.4 MB, GPU 103.6 MB
2024-12-28 15:50:40,226 - INFO - Model training completed in 1.58s
2024-12-28 15:50:40,256 - INFO - Prediction completed in 0.03s
2024-12-28 15:50:40,264 - INFO - Poison rate 0.2 completed in 6.47s
2024-12-28 15:50:40,272 - INFO - Loaded 245 existing results
2024-12-28 15:50:40,272 - INFO - Total results to save: 252
2024-12-28 15:50:40,273 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:50:40,307 - INFO - Saved 252 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:50:40,308 - INFO - Total evaluation time: 67.51s
2024-12-28 15:50:40,312 - INFO - 
Progress: 38.5% - Evaluating ImageNette with RandomForest (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:50:40,521 - INFO - Initialized DatasetHandler for ImageNette
2024-12-28 15:50:40,521 - INFO - Dataset type: image
2024-12-28 15:50:40,521 - INFO - Sample size: 9469
2024-12-28 15:50:40,522 - INFO - Loading datasets...
2024-12-28 15:50:40,544 - INFO - Dataset loading completed in 0.02s
2024-12-28 15:50:40,544 - INFO - Extracting validation features...
2024-12-28 15:50:40,544 - INFO - Extracting features from 1893 samples...
2024-12-28 15:50:45,328 - INFO - Feature extraction completed. Final feature shape: torch.Size([1893, 512])
2024-12-28 15:50:45,336 - INFO - Validation feature extraction completed in 4.79s
2024-12-28 15:50:45,336 - INFO - Extracting training features...
2024-12-28 15:50:45,336 - INFO - Extracting features from 9469 samples...
2024-12-28 15:51:06,872 - INFO - Feature extraction completed. Final feature shape: torch.Size([9469, 512])
2024-12-28 15:51:06,882 - INFO - Training feature extraction completed in 21.55s
2024-12-28 15:51:06,882 - INFO - Creating model for classifier: RandomForest
2024-12-28 15:51:06,882 - INFO - Using device: cuda
2024-12-28 15:51:06,882 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:51:06,882 - INFO - Training set processing completed in 0.00s
2024-12-28 15:51:06,883 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:51:06,884 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 104.0 MB
2024-12-28 15:51:06,884 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:51:07,109 - INFO - Fitted scaler and transformed data
2024-12-28 15:51:07,109 - INFO - Scaling time: 0.22s
2024-12-28 15:51:07,116 - INFO - Number of unique classes: 10
2024-12-28 15:51:10,531 - INFO - Epoch 1/10, Train Loss: 2.3020, Val Loss: 2.3013
2024-12-28 15:51:14,039 - INFO - Epoch 2/10, Train Loss: 2.3005, Val Loss: 2.3000
2024-12-28 15:51:17,717 - INFO - Epoch 3/10, Train Loss: 2.2991, Val Loss: 2.2987
2024-12-28 15:51:21,489 - INFO - Epoch 4/10, Train Loss: 2.2977, Val Loss: 2.2973
2024-12-28 15:51:21,489 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:51:21,489 - INFO - Training completed in 14.61s
2024-12-28 15:51:21,490 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.9 MB
2024-12-28 15:51:21,490 - INFO - Model training completed in 14.61s
2024-12-28 15:51:21,743 - INFO - Prediction completed in 0.25s
2024-12-28 15:51:21,752 - INFO - Poison rate 0.0 completed in 14.87s
2024-12-28 15:51:21,752 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:51:21,754 - INFO - Total number of labels flipped: 94
2024-12-28 15:51:21,754 - INFO - Label flipping completed in 0.00s
2024-12-28 15:51:21,754 - INFO - Training set processing completed in 0.00s
2024-12-28 15:51:21,754 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:51:21,755 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 106.0 MB
2024-12-28 15:51:21,755 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:51:21,923 - INFO - Fitted scaler and transformed data
2024-12-28 15:51:21,923 - INFO - Scaling time: 0.17s
2024-12-28 15:51:21,933 - INFO - Number of unique classes: 10
2024-12-28 15:51:24,963 - INFO - Epoch 1/10, Train Loss: 2.3020, Val Loss: 2.3014
2024-12-28 15:51:28,524 - INFO - Epoch 2/10, Train Loss: 2.3006, Val Loss: 2.3001
2024-12-28 15:51:31,947 - INFO - Epoch 3/10, Train Loss: 2.2992, Val Loss: 2.2989
2024-12-28 15:51:35,381 - INFO - Epoch 4/10, Train Loss: 2.2978, Val Loss: 2.2977
2024-12-28 15:51:35,381 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:51:35,381 - INFO - Training completed in 13.63s
2024-12-28 15:51:35,382 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.9 MB
2024-12-28 15:51:35,382 - INFO - Model training completed in 13.63s
2024-12-28 15:51:35,562 - INFO - Prediction completed in 0.18s
2024-12-28 15:51:35,571 - INFO - Poison rate 0.01 completed in 13.82s
2024-12-28 15:51:35,571 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:51:35,576 - INFO - Total number of labels flipped: 284
2024-12-28 15:51:35,576 - INFO - Label flipping completed in 0.00s
2024-12-28 15:51:35,576 - INFO - Training set processing completed in 0.00s
2024-12-28 15:51:35,576 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:51:35,577 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 106.0 MB
2024-12-28 15:51:35,577 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:51:35,753 - INFO - Fitted scaler and transformed data
2024-12-28 15:51:35,753 - INFO - Scaling time: 0.18s
2024-12-28 15:51:35,765 - INFO - Number of unique classes: 10
2024-12-28 15:51:39,080 - INFO - Epoch 1/10, Train Loss: 2.3020, Val Loss: 2.3014
2024-12-28 15:51:42,285 - INFO - Epoch 2/10, Train Loss: 2.3006, Val Loss: 2.3001
2024-12-28 15:51:45,217 - INFO - Epoch 3/10, Train Loss: 2.2993, Val Loss: 2.2989
2024-12-28 15:51:48,069 - INFO - Epoch 4/10, Train Loss: 2.2979, Val Loss: 2.2976
2024-12-28 15:51:48,069 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:51:48,069 - INFO - Training completed in 12.49s
2024-12-28 15:51:48,069 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.9 MB
2024-12-28 15:51:48,070 - INFO - Model training completed in 12.49s
2024-12-28 15:51:48,333 - INFO - Prediction completed in 0.26s
2024-12-28 15:51:48,344 - INFO - Poison rate 0.03 completed in 12.77s
2024-12-28 15:51:48,344 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:51:48,350 - INFO - Total number of labels flipped: 473
2024-12-28 15:51:48,350 - INFO - Label flipping completed in 0.01s
2024-12-28 15:51:48,351 - INFO - Training set processing completed in 0.00s
2024-12-28 15:51:48,351 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:51:48,351 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 106.0 MB
2024-12-28 15:51:48,352 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:51:48,559 - INFO - Fitted scaler and transformed data
2024-12-28 15:51:48,559 - INFO - Scaling time: 0.21s
2024-12-28 15:51:48,570 - INFO - Number of unique classes: 10
2024-12-28 15:51:51,835 - INFO - Epoch 1/10, Train Loss: 2.3020, Val Loss: 2.3015
2024-12-28 15:51:55,048 - INFO - Epoch 2/10, Train Loss: 2.3007, Val Loss: 2.3003
2024-12-28 15:51:58,489 - INFO - Epoch 3/10, Train Loss: 2.2994, Val Loss: 2.2991
2024-12-28 15:52:01,596 - INFO - Epoch 4/10, Train Loss: 2.2981, Val Loss: 2.2979
2024-12-28 15:52:01,596 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:52:01,596 - INFO - Training completed in 13.25s
2024-12-28 15:52:01,596 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.9 MB
2024-12-28 15:52:01,597 - INFO - Model training completed in 13.25s
2024-12-28 15:52:01,814 - INFO - Prediction completed in 0.22s
2024-12-28 15:52:01,822 - INFO - Poison rate 0.05 completed in 13.48s
2024-12-28 15:52:01,822 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:52:01,831 - INFO - Total number of labels flipped: 662
2024-12-28 15:52:01,831 - INFO - Label flipping completed in 0.01s
2024-12-28 15:52:01,831 - INFO - Training set processing completed in 0.00s
2024-12-28 15:52:01,831 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:52:01,832 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 106.0 MB
2024-12-28 15:52:01,832 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:52:02,002 - INFO - Fitted scaler and transformed data
2024-12-28 15:52:02,003 - INFO - Scaling time: 0.17s
2024-12-28 15:52:02,013 - INFO - Number of unique classes: 10
2024-12-28 15:52:05,824 - INFO - Epoch 1/10, Train Loss: 2.3020, Val Loss: 2.3014
2024-12-28 15:52:09,508 - INFO - Epoch 2/10, Train Loss: 2.3008, Val Loss: 2.3003
2024-12-28 15:52:12,637 - INFO - Epoch 3/10, Train Loss: 2.2995, Val Loss: 2.2992
2024-12-28 15:52:15,756 - INFO - Epoch 4/10, Train Loss: 2.2982, Val Loss: 2.2980
2024-12-28 15:52:15,756 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:52:15,756 - INFO - Training completed in 13.92s
2024-12-28 15:52:15,756 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.9 MB
2024-12-28 15:52:15,756 - INFO - Model training completed in 13.93s
2024-12-28 15:52:15,880 - INFO - Prediction completed in 0.12s
2024-12-28 15:52:15,888 - INFO - Poison rate 0.07 completed in 14.07s
2024-12-28 15:52:15,888 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:52:15,900 - INFO - Total number of labels flipped: 946
2024-12-28 15:52:15,900 - INFO - Label flipping completed in 0.01s
2024-12-28 15:52:15,900 - INFO - Training set processing completed in 0.00s
2024-12-28 15:52:15,900 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:52:15,901 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 106.0 MB
2024-12-28 15:52:15,901 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:52:16,071 - INFO - Fitted scaler and transformed data
2024-12-28 15:52:16,071 - INFO - Scaling time: 0.17s
2024-12-28 15:52:16,082 - INFO - Number of unique classes: 10
2024-12-28 15:52:19,342 - INFO - Epoch 1/10, Train Loss: 2.3021, Val Loss: 2.3016
2024-12-28 15:52:22,621 - INFO - Epoch 2/10, Train Loss: 2.3009, Val Loss: 2.3005
2024-12-28 15:52:25,674 - INFO - Epoch 3/10, Train Loss: 2.2997, Val Loss: 2.2995
2024-12-28 15:52:28,457 - INFO - Epoch 4/10, Train Loss: 2.2984, Val Loss: 2.2984
2024-12-28 15:52:28,457 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:52:28,457 - INFO - Training completed in 12.56s
2024-12-28 15:52:28,458 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.9 MB
2024-12-28 15:52:28,458 - INFO - Model training completed in 12.56s
2024-12-28 15:52:28,585 - INFO - Prediction completed in 0.13s
2024-12-28 15:52:28,594 - INFO - Poison rate 0.1 completed in 12.71s
2024-12-28 15:52:28,594 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:52:28,618 - INFO - Total number of labels flipped: 1893
2024-12-28 15:52:28,618 - INFO - Label flipping completed in 0.02s
2024-12-28 15:52:28,618 - INFO - Training set processing completed in 0.00s
2024-12-28 15:52:28,618 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:52:28,619 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 106.0 MB
2024-12-28 15:52:28,619 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:52:28,791 - INFO - Fitted scaler and transformed data
2024-12-28 15:52:28,791 - INFO - Scaling time: 0.17s
2024-12-28 15:52:28,802 - INFO - Number of unique classes: 10
2024-12-28 15:52:32,408 - INFO - Epoch 1/10, Train Loss: 2.3022, Val Loss: 2.3017
2024-12-28 15:52:35,570 - INFO - Epoch 2/10, Train Loss: 2.3012, Val Loss: 2.3008
2024-12-28 15:52:38,964 - INFO - Epoch 3/10, Train Loss: 2.3002, Val Loss: 2.2999
2024-12-28 15:52:42,117 - INFO - Epoch 4/10, Train Loss: 2.2992, Val Loss: 2.2991
2024-12-28 15:52:42,118 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:52:42,118 - INFO - Training completed in 13.50s
2024-12-28 15:52:42,118 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.9 MB
2024-12-28 15:52:42,118 - INFO - Model training completed in 13.50s
2024-12-28 15:52:42,363 - INFO - Prediction completed in 0.24s
2024-12-28 15:52:42,371 - INFO - Poison rate 0.2 completed in 13.78s
2024-12-28 15:52:42,379 - INFO - Loaded 252 existing results
2024-12-28 15:52:42,379 - INFO - Total results to save: 259
2024-12-28 15:52:42,380 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:52:42,393 - INFO - Saved 259 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:52:42,395 - INFO - Total evaluation time: 121.87s
2024-12-28 15:52:42,399 - INFO - 
Progress: 39.6% - Evaluating ImageNette with RandomForest (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:52:42,600 - INFO - Initialized DatasetHandler for ImageNette
2024-12-28 15:52:42,601 - INFO - Dataset type: image
2024-12-28 15:52:42,601 - INFO - Sample size: 9469
2024-12-28 15:52:42,602 - INFO - Loading datasets...
2024-12-28 15:52:42,624 - INFO - Dataset loading completed in 0.02s
2024-12-28 15:52:42,624 - INFO - Extracting validation features...
2024-12-28 15:52:42,624 - INFO - Extracting features from 1893 samples...
2024-12-28 15:52:47,424 - INFO - Feature extraction completed. Final feature shape: torch.Size([1893, 512])
2024-12-28 15:52:47,427 - INFO - Validation feature extraction completed in 4.80s
2024-12-28 15:52:47,427 - INFO - Extracting training features...
2024-12-28 15:52:47,428 - INFO - Extracting features from 9469 samples...
2024-12-28 15:53:08,685 - INFO - Feature extraction completed. Final feature shape: torch.Size([9469, 512])
2024-12-28 15:53:08,692 - INFO - Training feature extraction completed in 21.26s
2024-12-28 15:53:08,693 - INFO - Creating model for classifier: RandomForest
2024-12-28 15:53:08,693 - INFO - Using device: cuda
2024-12-28 15:53:08,693 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:53:08,693 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:53:08,693 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:53:09,250 - INFO - Feature scaling completed in 0.56s
2024-12-28 15:53:09,250 - INFO - Starting feature selection (k=50)
2024-12-28 15:53:09,259 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:53:09,259 - INFO - Starting anomaly detection
2024-12-28 15:53:13,155 - INFO - Anomaly detection completed in 3.90s
2024-12-28 15:53:13,155 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:53:13,155 - INFO - Total fit_transform time: 4.46s
2024-12-28 15:53:13,155 - INFO - Training set processing completed in 4.46s
2024-12-28 15:53:13,155 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:53:13,157 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 103.4 MB
2024-12-28 15:53:13,157 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:53:13,337 - INFO - Fitted scaler and transformed data
2024-12-28 15:53:13,337 - INFO - Scaling time: 0.18s
2024-12-28 15:53:13,345 - INFO - Number of unique classes: 10
2024-12-28 15:53:16,566 - INFO - Epoch 1/10, Train Loss: 2.1856, Val Loss: 2.3013
2024-12-28 15:53:19,669 - INFO - Epoch 2/10, Train Loss: 2.1843, Val Loss: 2.3000
2024-12-28 15:53:22,525 - INFO - Epoch 3/10, Train Loss: 2.1829, Val Loss: 2.2987
2024-12-28 15:53:25,413 - INFO - Epoch 4/10, Train Loss: 2.1816, Val Loss: 2.2973
2024-12-28 15:53:25,413 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:53:25,413 - INFO - Training completed in 12.26s
2024-12-28 15:53:25,414 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.3 MB
2024-12-28 15:53:25,414 - INFO - Model training completed in 12.26s
2024-12-28 15:53:25,545 - INFO - Prediction completed in 0.13s
2024-12-28 15:53:25,554 - INFO - Poison rate 0.0 completed in 16.86s
2024-12-28 15:53:25,554 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:53:25,556 - INFO - Total number of labels flipped: 94
2024-12-28 15:53:25,556 - INFO - Label flipping completed in 0.00s
2024-12-28 15:53:25,556 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:53:25,556 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:53:26,122 - INFO - Feature scaling completed in 0.57s
2024-12-28 15:53:26,122 - INFO - Starting feature selection (k=50)
2024-12-28 15:53:26,134 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:53:26,135 - INFO - Starting anomaly detection
2024-12-28 15:53:30,494 - INFO - Anomaly detection completed in 4.36s
2024-12-28 15:53:30,495 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:53:30,495 - INFO - Total fit_transform time: 4.94s
2024-12-28 15:53:30,495 - INFO - Training set processing completed in 4.94s
2024-12-28 15:53:30,495 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:53:30,496 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 105.4 MB
2024-12-28 15:53:30,496 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:53:30,680 - INFO - Fitted scaler and transformed data
2024-12-28 15:53:30,681 - INFO - Scaling time: 0.18s
2024-12-28 15:53:30,689 - INFO - Number of unique classes: 10
2024-12-28 15:53:34,025 - INFO - Epoch 1/10, Train Loss: 2.1854, Val Loss: 2.3013
2024-12-28 15:53:37,394 - INFO - Epoch 2/10, Train Loss: 2.1840, Val Loss: 2.3000
2024-12-28 15:53:40,876 - INFO - Epoch 3/10, Train Loss: 2.1827, Val Loss: 2.2987
2024-12-28 15:53:44,601 - INFO - Epoch 4/10, Train Loss: 2.1814, Val Loss: 2.2974
2024-12-28 15:53:44,601 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:53:44,601 - INFO - Training completed in 14.11s
2024-12-28 15:53:44,602 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.3 MB
2024-12-28 15:53:44,602 - INFO - Model training completed in 14.11s
2024-12-28 15:53:44,738 - INFO - Prediction completed in 0.14s
2024-12-28 15:53:44,746 - INFO - Poison rate 0.01 completed in 19.19s
2024-12-28 15:53:44,746 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:53:44,750 - INFO - Total number of labels flipped: 284
2024-12-28 15:53:44,750 - INFO - Label flipping completed in 0.00s
2024-12-28 15:53:44,750 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:53:44,750 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:53:45,288 - INFO - Feature scaling completed in 0.54s
2024-12-28 15:53:45,288 - INFO - Starting feature selection (k=50)
2024-12-28 15:53:45,301 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:53:45,301 - INFO - Starting anomaly detection
2024-12-28 15:53:48,889 - INFO - Anomaly detection completed in 3.59s
2024-12-28 15:53:48,889 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:53:48,889 - INFO - Total fit_transform time: 4.14s
2024-12-28 15:53:48,889 - INFO - Training set processing completed in 4.14s
2024-12-28 15:53:48,889 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:53:48,891 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 105.4 MB
2024-12-28 15:53:48,891 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:53:49,076 - INFO - Fitted scaler and transformed data
2024-12-28 15:53:49,076 - INFO - Scaling time: 0.19s
2024-12-28 15:53:49,084 - INFO - Number of unique classes: 10
2024-12-28 15:53:52,274 - INFO - Epoch 1/10, Train Loss: 2.1855, Val Loss: 2.3014
2024-12-28 15:53:55,969 - INFO - Epoch 2/10, Train Loss: 2.1842, Val Loss: 2.3002
2024-12-28 15:53:59,961 - INFO - Epoch 3/10, Train Loss: 2.1830, Val Loss: 2.2990
2024-12-28 15:54:02,993 - INFO - Epoch 4/10, Train Loss: 2.1817, Val Loss: 2.2977
2024-12-28 15:54:02,993 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:54:02,993 - INFO - Training completed in 14.10s
2024-12-28 15:54:02,994 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.3 MB
2024-12-28 15:54:02,994 - INFO - Model training completed in 14.10s
2024-12-28 15:54:03,120 - INFO - Prediction completed in 0.13s
2024-12-28 15:54:03,129 - INFO - Poison rate 0.03 completed in 18.38s
2024-12-28 15:54:03,129 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:54:03,135 - INFO - Total number of labels flipped: 473
2024-12-28 15:54:03,136 - INFO - Label flipping completed in 0.01s
2024-12-28 15:54:03,136 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:54:03,136 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:54:03,688 - INFO - Feature scaling completed in 0.55s
2024-12-28 15:54:03,688 - INFO - Starting feature selection (k=50)
2024-12-28 15:54:03,700 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:54:03,701 - INFO - Starting anomaly detection
2024-12-28 15:54:07,985 - INFO - Anomaly detection completed in 4.28s
2024-12-28 15:54:07,986 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:54:07,986 - INFO - Total fit_transform time: 4.85s
2024-12-28 15:54:07,986 - INFO - Training set processing completed in 4.85s
2024-12-28 15:54:07,986 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:54:07,987 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 105.4 MB
2024-12-28 15:54:07,987 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:54:08,182 - INFO - Fitted scaler and transformed data
2024-12-28 15:54:08,182 - INFO - Scaling time: 0.19s
2024-12-28 15:54:08,189 - INFO - Number of unique classes: 10
2024-12-28 15:54:11,542 - INFO - Epoch 1/10, Train Loss: 2.1868, Val Loss: 2.3014
2024-12-28 15:54:14,483 - INFO - Epoch 2/10, Train Loss: 2.1856, Val Loss: 2.3001
2024-12-28 15:54:17,226 - INFO - Epoch 3/10, Train Loss: 2.1843, Val Loss: 2.2989
2024-12-28 15:54:20,140 - INFO - Epoch 4/10, Train Loss: 2.1831, Val Loss: 2.2976
2024-12-28 15:54:20,140 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:54:20,140 - INFO - Training completed in 12.15s
2024-12-28 15:54:20,140 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.3 MB
2024-12-28 15:54:20,141 - INFO - Model training completed in 12.15s
2024-12-28 15:54:20,326 - INFO - Prediction completed in 0.19s
2024-12-28 15:54:20,334 - INFO - Poison rate 0.05 completed in 17.21s
2024-12-28 15:54:20,335 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:54:20,343 - INFO - Total number of labels flipped: 662
2024-12-28 15:54:20,343 - INFO - Label flipping completed in 0.01s
2024-12-28 15:54:20,343 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:54:20,343 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:54:20,937 - INFO - Feature scaling completed in 0.59s
2024-12-28 15:54:20,937 - INFO - Starting feature selection (k=50)
2024-12-28 15:54:20,952 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:54:20,952 - INFO - Starting anomaly detection
2024-12-28 15:54:23,634 - INFO - Anomaly detection completed in 2.68s
2024-12-28 15:54:23,635 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:54:23,635 - INFO - Total fit_transform time: 3.29s
2024-12-28 15:54:23,635 - INFO - Training set processing completed in 3.29s
2024-12-28 15:54:23,635 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:54:23,637 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 105.4 MB
2024-12-28 15:54:23,637 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:54:23,823 - INFO - Fitted scaler and transformed data
2024-12-28 15:54:23,823 - INFO - Scaling time: 0.19s
2024-12-28 15:54:23,830 - INFO - Number of unique classes: 10
2024-12-28 15:54:27,224 - INFO - Epoch 1/10, Train Loss: 2.1867, Val Loss: 2.3015
2024-12-28 15:54:30,466 - INFO - Epoch 2/10, Train Loss: 2.1855, Val Loss: 2.3004
2024-12-28 15:54:33,677 - INFO - Epoch 3/10, Train Loss: 2.1843, Val Loss: 2.2992
2024-12-28 15:54:36,743 - INFO - Epoch 4/10, Train Loss: 2.1831, Val Loss: 2.2981
2024-12-28 15:54:36,743 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:54:36,743 - INFO - Training completed in 13.11s
2024-12-28 15:54:36,744 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.3 MB
2024-12-28 15:54:36,744 - INFO - Model training completed in 13.11s
2024-12-28 15:54:36,892 - INFO - Prediction completed in 0.15s
2024-12-28 15:54:36,901 - INFO - Poison rate 0.07 completed in 16.57s
2024-12-28 15:54:36,901 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:54:36,912 - INFO - Total number of labels flipped: 946
2024-12-28 15:54:36,913 - INFO - Label flipping completed in 0.01s
2024-12-28 15:54:36,913 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:54:36,913 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:54:37,471 - INFO - Feature scaling completed in 0.56s
2024-12-28 15:54:37,471 - INFO - Starting feature selection (k=50)
2024-12-28 15:54:37,483 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:54:37,483 - INFO - Starting anomaly detection
2024-12-28 15:54:40,014 - INFO - Anomaly detection completed in 2.53s
2024-12-28 15:54:40,014 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:54:40,015 - INFO - Total fit_transform time: 3.10s
2024-12-28 15:54:40,015 - INFO - Training set processing completed in 3.10s
2024-12-28 15:54:40,015 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:54:40,016 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 105.4 MB
2024-12-28 15:54:40,016 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:54:40,197 - INFO - Fitted scaler and transformed data
2024-12-28 15:54:40,197 - INFO - Scaling time: 0.18s
2024-12-28 15:54:40,204 - INFO - Number of unique classes: 10
2024-12-28 15:54:43,126 - INFO - Epoch 1/10, Train Loss: 2.1878, Val Loss: 2.3016
2024-12-28 15:54:45,943 - INFO - Epoch 2/10, Train Loss: 2.1867, Val Loss: 2.3005
2024-12-28 15:54:49,274 - INFO - Epoch 3/10, Train Loss: 2.1856, Val Loss: 2.2995
2024-12-28 15:54:52,145 - INFO - Epoch 4/10, Train Loss: 2.1845, Val Loss: 2.2984
2024-12-28 15:54:52,145 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:54:52,145 - INFO - Training completed in 12.13s
2024-12-28 15:54:52,145 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.3 MB
2024-12-28 15:54:52,146 - INFO - Model training completed in 12.13s
2024-12-28 15:54:52,262 - INFO - Prediction completed in 0.12s
2024-12-28 15:54:52,270 - INFO - Poison rate 0.1 completed in 15.37s
2024-12-28 15:54:52,270 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:54:52,291 - INFO - Total number of labels flipped: 1893
2024-12-28 15:54:52,291 - INFO - Label flipping completed in 0.02s
2024-12-28 15:54:52,292 - INFO - Initialized DynaDetect trainer on cuda
2024-12-28 15:54:52,292 - INFO - Starting feature scaling on shape (9469, 512)
2024-12-28 15:54:53,054 - INFO - Feature scaling completed in 0.76s
2024-12-28 15:54:53,054 - INFO - Starting feature selection (k=50)
2024-12-28 15:54:53,066 - INFO - Feature selection completed in 0.01s. Output shape: (9469, 50)
2024-12-28 15:54:53,066 - INFO - Starting anomaly detection
2024-12-28 15:54:57,330 - INFO - Anomaly detection completed in 4.26s
2024-12-28 15:54:57,330 - INFO - Found 947 outliers (10.0%)
2024-12-28 15:54:57,330 - INFO - Total fit_transform time: 5.04s
2024-12-28 15:54:57,330 - INFO - Training set processing completed in 5.04s
2024-12-28 15:54:57,330 - INFO - Fitting RandomForestWrapper model with data shape: (9469, 512)
2024-12-28 15:54:57,331 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 105.4 MB
2024-12-28 15:54:57,331 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:54:57,539 - INFO - Fitted scaler and transformed data
2024-12-28 15:54:57,539 - INFO - Scaling time: 0.21s
2024-12-28 15:54:57,550 - INFO - Number of unique classes: 10
2024-12-28 15:55:00,710 - INFO - Epoch 1/10, Train Loss: 2.1870, Val Loss: 2.3018
2024-12-28 15:55:04,435 - INFO - Epoch 2/10, Train Loss: 2.1860, Val Loss: 2.3009
2024-12-28 15:55:07,309 - INFO - Epoch 3/10, Train Loss: 2.1850, Val Loss: 2.3001
2024-12-28 15:55:10,105 - INFO - Epoch 4/10, Train Loss: 2.1840, Val Loss: 2.2993
2024-12-28 15:55:10,105 - INFO - Early stopping triggered at epoch 4
2024-12-28 15:55:10,105 - INFO - Training completed in 12.77s
2024-12-28 15:55:10,106 - INFO - Final memory usage: CPU 2715.4 MB, GPU 125.3 MB
2024-12-28 15:55:10,106 - INFO - Model training completed in 12.78s
2024-12-28 15:55:10,296 - INFO - Prediction completed in 0.19s
2024-12-28 15:55:10,305 - INFO - Poison rate 0.2 completed in 18.03s
2024-12-28 15:55:10,313 - INFO - Loaded 259 existing results
2024-12-28 15:55:10,313 - INFO - Total results to save: 266
2024-12-28 15:55:10,314 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:55:10,328 - INFO - Saved 266 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:55:10,329 - INFO - Total evaluation time: 147.73s
2024-12-28 15:55:10,331 - INFO - 
Progress: 40.6% - Evaluating ImageNette with KNeighbors (standard mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:55:10,500 - INFO - Initialized DatasetHandler for ImageNette
2024-12-28 15:55:10,500 - INFO - Dataset type: image
2024-12-28 15:55:10,501 - INFO - Sample size: 9469
2024-12-28 15:55:10,502 - INFO - Loading datasets...
2024-12-28 15:55:10,524 - INFO - Dataset loading completed in 0.02s
2024-12-28 15:55:10,524 - INFO - Extracting validation features...
2024-12-28 15:55:10,524 - INFO - Extracting features from 1893 samples...
2024-12-28 15:55:15,412 - INFO - Feature extraction completed. Final feature shape: torch.Size([1893, 512])
2024-12-28 15:55:15,416 - INFO - Validation feature extraction completed in 4.89s
2024-12-28 15:55:15,416 - INFO - Extracting training features...
2024-12-28 15:55:15,416 - INFO - Extracting features from 9469 samples...
2024-12-28 15:55:37,236 - INFO - Feature extraction completed. Final feature shape: torch.Size([9469, 512])
2024-12-28 15:55:37,243 - INFO - Training feature extraction completed in 21.83s
2024-12-28 15:55:37,243 - INFO - Creating model for classifier: KNeighbors
2024-12-28 15:55:37,243 - INFO - Using device: cuda
2024-12-28 15:55:37,244 - INFO - 
Processing poison rate: 0.0
2024-12-28 15:55:37,244 - INFO - Training set processing completed in 0.00s
2024-12-28 15:55:37,244 - INFO - Fitting KNeighborsWrapper model with data shape: (9469, 512)
2024-12-28 15:55:37,245 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 104.0 MB
2024-12-28 15:55:37,246 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:55:37,433 - INFO - Fitted scaler and transformed data
2024-12-28 15:55:37,433 - INFO - Scaling time: 0.19s
2024-12-28 15:55:37,440 - INFO - Training completed in 0.20s
2024-12-28 15:55:37,441 - INFO - Final memory usage: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:37,441 - INFO - Model training completed in 0.20s
2024-12-28 15:55:37,485 - INFO - Prediction completed in 0.04s
2024-12-28 15:55:37,497 - INFO - Poison rate 0.0 completed in 0.25s
2024-12-28 15:55:37,497 - INFO - 
Processing poison rate: 0.01
2024-12-28 15:55:37,499 - INFO - Total number of labels flipped: 94
2024-12-28 15:55:37,499 - INFO - Label flipping completed in 0.00s
2024-12-28 15:55:37,499 - INFO - Training set processing completed in 0.00s
2024-12-28 15:55:37,500 - INFO - Fitting KNeighborsWrapper model with data shape: (9469, 512)
2024-12-28 15:55:37,500 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:37,501 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:55:37,743 - INFO - Fitted scaler and transformed data
2024-12-28 15:55:37,743 - INFO - Scaling time: 0.24s
2024-12-28 15:55:37,750 - INFO - Training completed in 0.25s
2024-12-28 15:55:37,751 - INFO - Final memory usage: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:37,751 - INFO - Model training completed in 0.25s
2024-12-28 15:55:37,792 - INFO - Prediction completed in 0.04s
2024-12-28 15:55:37,812 - INFO - Poison rate 0.01 completed in 0.31s
2024-12-28 15:55:37,812 - INFO - 
Processing poison rate: 0.03
2024-12-28 15:55:37,818 - INFO - Total number of labels flipped: 284
2024-12-28 15:55:37,819 - INFO - Label flipping completed in 0.01s
2024-12-28 15:55:37,819 - INFO - Training set processing completed in 0.00s
2024-12-28 15:55:37,819 - INFO - Fitting KNeighborsWrapper model with data shape: (9469, 512)
2024-12-28 15:55:37,820 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:37,820 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:55:38,003 - INFO - Fitted scaler and transformed data
2024-12-28 15:55:38,003 - INFO - Scaling time: 0.18s
2024-12-28 15:55:38,009 - INFO - Training completed in 0.19s
2024-12-28 15:55:38,010 - INFO - Final memory usage: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:38,010 - INFO - Model training completed in 0.19s
2024-12-28 15:55:38,070 - INFO - Prediction completed in 0.06s
2024-12-28 15:55:38,078 - INFO - Poison rate 0.03 completed in 0.27s
2024-12-28 15:55:38,078 - INFO - 
Processing poison rate: 0.05
2024-12-28 15:55:38,088 - INFO - Total number of labels flipped: 473
2024-12-28 15:55:38,088 - INFO - Label flipping completed in 0.01s
2024-12-28 15:55:38,088 - INFO - Training set processing completed in 0.00s
2024-12-28 15:55:38,088 - INFO - Fitting KNeighborsWrapper model with data shape: (9469, 512)
2024-12-28 15:55:38,089 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:38,089 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:55:38,286 - INFO - Fitted scaler and transformed data
2024-12-28 15:55:38,286 - INFO - Scaling time: 0.20s
2024-12-28 15:55:38,293 - INFO - Training completed in 0.20s
2024-12-28 15:55:38,294 - INFO - Final memory usage: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:38,294 - INFO - Model training completed in 0.21s
2024-12-28 15:55:38,343 - INFO - Prediction completed in 0.05s
2024-12-28 15:55:38,351 - INFO - Poison rate 0.05 completed in 0.27s
2024-12-28 15:55:38,352 - INFO - 
Processing poison rate: 0.07
2024-12-28 15:55:38,360 - INFO - Total number of labels flipped: 662
2024-12-28 15:55:38,360 - INFO - Label flipping completed in 0.01s
2024-12-28 15:55:38,360 - INFO - Training set processing completed in 0.00s
2024-12-28 15:55:38,360 - INFO - Fitting KNeighborsWrapper model with data shape: (9469, 512)
2024-12-28 15:55:38,361 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:38,361 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:55:38,552 - INFO - Fitted scaler and transformed data
2024-12-28 15:55:38,552 - INFO - Scaling time: 0.19s
2024-12-28 15:55:38,558 - INFO - Training completed in 0.20s
2024-12-28 15:55:38,559 - INFO - Final memory usage: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:38,559 - INFO - Model training completed in 0.20s
2024-12-28 15:55:38,595 - INFO - Prediction completed in 0.04s
2024-12-28 15:55:38,603 - INFO - Poison rate 0.07 completed in 0.25s
2024-12-28 15:55:38,604 - INFO - 
Processing poison rate: 0.1
2024-12-28 15:55:38,615 - INFO - Total number of labels flipped: 946
2024-12-28 15:55:38,615 - INFO - Label flipping completed in 0.01s
2024-12-28 15:55:38,615 - INFO - Training set processing completed in 0.00s
2024-12-28 15:55:38,615 - INFO - Fitting KNeighborsWrapper model with data shape: (9469, 512)
2024-12-28 15:55:38,616 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:38,616 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:55:38,795 - INFO - Fitted scaler and transformed data
2024-12-28 15:55:38,795 - INFO - Scaling time: 0.18s
2024-12-28 15:55:38,801 - INFO - Training completed in 0.19s
2024-12-28 15:55:38,802 - INFO - Final memory usage: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:38,802 - INFO - Model training completed in 0.19s
2024-12-28 15:55:38,869 - INFO - Prediction completed in 0.07s
2024-12-28 15:55:38,877 - INFO - Poison rate 0.1 completed in 0.27s
2024-12-28 15:55:38,878 - INFO - 
Processing poison rate: 0.2
2024-12-28 15:55:38,900 - INFO - Total number of labels flipped: 1893
2024-12-28 15:55:38,900 - INFO - Label flipping completed in 0.02s
2024-12-28 15:55:38,900 - INFO - Training set processing completed in 0.00s
2024-12-28 15:55:38,900 - INFO - Fitting KNeighborsWrapper model with data shape: (9469, 512)
2024-12-28 15:55:38,901 - INFO - Memory usage at start_fit: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:38,901 - INFO - Input data shape: (9469, 512), labels shape: (9469,)
2024-12-28 15:55:39,126 - INFO - Fitted scaler and transformed data
2024-12-28 15:55:39,126 - INFO - Scaling time: 0.22s
2024-12-28 15:55:39,132 - INFO - Training completed in 0.23s
2024-12-28 15:55:39,133 - INFO - Final memory usage: CPU 2715.4 MB, GPU 122.6 MB
2024-12-28 15:55:39,133 - INFO - Model training completed in 0.23s
2024-12-28 15:55:39,182 - INFO - Prediction completed in 0.05s
2024-12-28 15:55:39,190 - INFO - Poison rate 0.2 completed in 0.31s
2024-12-28 15:55:39,199 - INFO - Loaded 266 existing results
2024-12-28 15:55:39,199 - INFO - Total results to save: 273
2024-12-28 15:55:39,201 - INFO - Archived existing results to /home/brian/Notebooks/ddv2-ws/results/archive/experiment_results_20241228_140314.csv
2024-12-28 15:55:39,218 - INFO - Saved 273 results to /home/brian/Notebooks/ddv2-ws/results/experiment_results_20241228_140314.csv
2024-12-28 15:55:39,219 - INFO - Total evaluation time: 28.72s
2024-12-28 15:55:39,223 - INFO - 
Progress: 41.7% - Evaluating ImageNette with KNeighbors (dynadetect mode, iteration 1/1)
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brian/miniconda3/envs/jupyterlab/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2024-12-28 15:55:39,419 - INFO - Initialized DatasetHandler for ImageNette
2024-12-28 15:55:39,419 - INFO - Dataset type: image
2024-12-28 15:55:39,419 - INFO - Sample size: 9469
2024-12-28 15:55:39,420 - INFO - Loading datasets...
2024-12-28 15:55:39,442 - INFO - Dataset loading completed in 0.02s
2024-12-28 15:55:39,442 - INFO - Extracting validation features...
2024-12-28 15:55:39,443 - INFO - Extracting features from 1893 samples...
